{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# This is a Kaggle competition to predict the probability of 0 or 1 of Tabular Playground series data\n",
    "## I will be comparing which model to use: XGB, Random Forest or Neural Networks\n",
    "\n",
    "\n",
    "Lets first load required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import NullFormatter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyodbc\n",
    "import matplotlib.ticker as ticker\n",
    "from sklearn import preprocessing\n",
    "%matplotlib inline\n",
    "import keras\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import SimpleRNN\n",
    "from keras.datasets import imdb\n",
    "from keras import initializers\n",
    "from keras.models  import Sequential\n",
    "from keras.layers import Input, Dense, Flatten, Dropout, BatchNormalization\n",
    "from keras.optimizers import Adam, SGD, RMSprop\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_curve, roc_auc_score, roc_curve, accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Lets download the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('C:/Users/taihs/OneDrive/Documents/tps competition/train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Find the number of records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300000, 32)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 300000 entries, 0 to 299999\n",
      "Data columns (total 32 columns):\n",
      " #   Column  Non-Null Count   Dtype  \n",
      "---  ------  --------------   -----  \n",
      " 0   id      300000 non-null  int64  \n",
      " 1   cat0    300000 non-null  object \n",
      " 2   cat1    300000 non-null  object \n",
      " 3   cat2    300000 non-null  object \n",
      " 4   cat3    300000 non-null  object \n",
      " 5   cat4    300000 non-null  object \n",
      " 6   cat5    300000 non-null  object \n",
      " 7   cat6    300000 non-null  object \n",
      " 8   cat7    300000 non-null  object \n",
      " 9   cat8    300000 non-null  object \n",
      " 10  cat9    300000 non-null  object \n",
      " 11  cat10   300000 non-null  object \n",
      " 12  cat11   300000 non-null  object \n",
      " 13  cat12   300000 non-null  object \n",
      " 14  cat13   300000 non-null  object \n",
      " 15  cat14   300000 non-null  object \n",
      " 16  cat15   300000 non-null  object \n",
      " 17  cat16   300000 non-null  object \n",
      " 18  cat17   300000 non-null  object \n",
      " 19  cat18   300000 non-null  object \n",
      " 20  cont0   300000 non-null  float64\n",
      " 21  cont1   300000 non-null  float64\n",
      " 22  cont2   300000 non-null  float64\n",
      " 23  cont3   300000 non-null  float64\n",
      " 24  cont4   300000 non-null  float64\n",
      " 25  cont5   300000 non-null  float64\n",
      " 26  cont6   300000 non-null  float64\n",
      " 27  cont7   300000 non-null  float64\n",
      " 28  cont8   300000 non-null  float64\n",
      " 29  cont9   300000 non-null  float64\n",
      " 30  cont10  300000 non-null  float64\n",
      " 31  target  300000 non-null  int64  \n",
      "dtypes: float64(11), int64(2), object(19)\n",
      "memory usage: 73.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df1.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Lets look at the columns\n",
    "\n",
    "### Attributes: (all numeric-valued)\n",
    "   1. Field goal percentage for home team\n",
    "   2. Free throw percentage for home team\n",
    "   3. 3pt Field goal percentage for home team\n",
    "   4. Assists for home team\n",
    "   5. Rebounds for home team\n",
    "   6. Field goal percentage for away team\n",
    "   7. Assists for away team\n",
    "   8. Rebounds for away team\n",
    "   9. Win or Loss (1 or 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cat0</th>\n",
       "      <th>cat1</th>\n",
       "      <th>cat2</th>\n",
       "      <th>cat3</th>\n",
       "      <th>cat4</th>\n",
       "      <th>cat5</th>\n",
       "      <th>cat6</th>\n",
       "      <th>cat7</th>\n",
       "      <th>cat8</th>\n",
       "      <th>...</th>\n",
       "      <th>cont2</th>\n",
       "      <th>cont3</th>\n",
       "      <th>cont4</th>\n",
       "      <th>cont5</th>\n",
       "      <th>cont6</th>\n",
       "      <th>cont7</th>\n",
       "      <th>cont8</th>\n",
       "      <th>cont9</th>\n",
       "      <th>cont10</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>A</td>\n",
       "      <td>I</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>BI</td>\n",
       "      <td>A</td>\n",
       "      <td>S</td>\n",
       "      <td>Q</td>\n",
       "      <td>...</td>\n",
       "      <td>0.759439</td>\n",
       "      <td>0.795549</td>\n",
       "      <td>0.681917</td>\n",
       "      <td>0.621672</td>\n",
       "      <td>0.592184</td>\n",
       "      <td>0.791921</td>\n",
       "      <td>0.815254</td>\n",
       "      <td>0.965006</td>\n",
       "      <td>0.665915</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>I</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>E</td>\n",
       "      <td>BI</td>\n",
       "      <td>K</td>\n",
       "      <td>W</td>\n",
       "      <td>AD</td>\n",
       "      <td>...</td>\n",
       "      <td>0.386385</td>\n",
       "      <td>0.541366</td>\n",
       "      <td>0.388982</td>\n",
       "      <td>0.357778</td>\n",
       "      <td>0.600044</td>\n",
       "      <td>0.408701</td>\n",
       "      <td>0.399353</td>\n",
       "      <td>0.927406</td>\n",
       "      <td>0.493729</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>A</td>\n",
       "      <td>K</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>E</td>\n",
       "      <td>BI</td>\n",
       "      <td>A</td>\n",
       "      <td>E</td>\n",
       "      <td>BM</td>\n",
       "      <td>...</td>\n",
       "      <td>0.343255</td>\n",
       "      <td>0.616352</td>\n",
       "      <td>0.793687</td>\n",
       "      <td>0.552877</td>\n",
       "      <td>0.352113</td>\n",
       "      <td>0.388835</td>\n",
       "      <td>0.412303</td>\n",
       "      <td>0.292696</td>\n",
       "      <td>0.549452</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>A</td>\n",
       "      <td>K</td>\n",
       "      <td>A</td>\n",
       "      <td>C</td>\n",
       "      <td>E</td>\n",
       "      <td>BI</td>\n",
       "      <td>A</td>\n",
       "      <td>Y</td>\n",
       "      <td>AD</td>\n",
       "      <td>...</td>\n",
       "      <td>0.831147</td>\n",
       "      <td>0.807807</td>\n",
       "      <td>0.800032</td>\n",
       "      <td>0.619147</td>\n",
       "      <td>0.221789</td>\n",
       "      <td>0.897617</td>\n",
       "      <td>0.633669</td>\n",
       "      <td>0.760318</td>\n",
       "      <td>0.934242</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>A</td>\n",
       "      <td>I</td>\n",
       "      <td>G</td>\n",
       "      <td>B</td>\n",
       "      <td>E</td>\n",
       "      <td>BI</td>\n",
       "      <td>C</td>\n",
       "      <td>G</td>\n",
       "      <td>Q</td>\n",
       "      <td>...</td>\n",
       "      <td>0.338818</td>\n",
       "      <td>0.277308</td>\n",
       "      <td>0.610578</td>\n",
       "      <td>0.128291</td>\n",
       "      <td>0.578764</td>\n",
       "      <td>0.279167</td>\n",
       "      <td>0.351103</td>\n",
       "      <td>0.357084</td>\n",
       "      <td>0.328960</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id cat0 cat1 cat2 cat3 cat4 cat5 cat6 cat7 cat8  ...     cont2     cont3  \\\n",
       "0   0    A    I    A    B    B   BI    A    S    Q  ...  0.759439  0.795549   \n",
       "1   1    A    I    A    A    E   BI    K    W   AD  ...  0.386385  0.541366   \n",
       "2   2    A    K    A    A    E   BI    A    E   BM  ...  0.343255  0.616352   \n",
       "3   3    A    K    A    C    E   BI    A    Y   AD  ...  0.831147  0.807807   \n",
       "4   4    A    I    G    B    E   BI    C    G    Q  ...  0.338818  0.277308   \n",
       "\n",
       "      cont4     cont5     cont6     cont7     cont8     cont9    cont10 target  \n",
       "0  0.681917  0.621672  0.592184  0.791921  0.815254  0.965006  0.665915      0  \n",
       "1  0.388982  0.357778  0.600044  0.408701  0.399353  0.927406  0.493729      0  \n",
       "2  0.793687  0.552877  0.352113  0.388835  0.412303  0.292696  0.549452      0  \n",
       "3  0.800032  0.619147  0.221789  0.897617  0.633669  0.760318  0.934242      0  \n",
       "4  0.610578  0.128291  0.578764  0.279167  0.351103  0.357084  0.328960      1  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "icat0=le.fit_transform(df1['cat0'])\n",
    "icat1=le.fit_transform(df1['cat1'])\n",
    "icat2=le.fit_transform(df1['cat2'])\n",
    "icat3=le.fit_transform(df1['cat3'])\n",
    "icat4=le.fit_transform(df1['cat4'])\n",
    "icat5=le.fit_transform(df1['cat5'])\n",
    "icat6=le.fit_transform(df1['cat6'])\n",
    "icat7=le.fit_transform(df1['cat7'])\n",
    "icat8=le.fit_transform(df1['cat8'])\n",
    "icat9=le.fit_transform(df1['cat9'])\n",
    "icat10=le.fit_transform(df1['cat10'])\n",
    "icat11=le.fit_transform(df1['cat11'])\n",
    "icat12=le.fit_transform(df1['cat12'])\n",
    "icat13=le.fit_transform(df1['cat13'])\n",
    "icat14=le.fit_transform(df1['cat14'])\n",
    "icat15=le.fit_transform(df1['cat15'])\n",
    "icat16=le.fit_transform(df1['cat16'])\n",
    "icat17=le.fit_transform(df1['cat17'])\n",
    "icat18=le.fit_transform(df1['cat18'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>icat0</th>\n",
       "      <th>icat1</th>\n",
       "      <th>icat2</th>\n",
       "      <th>icat3</th>\n",
       "      <th>icat4</th>\n",
       "      <th>icat5</th>\n",
       "      <th>icat6</th>\n",
       "      <th>icat7</th>\n",
       "      <th>icat8</th>\n",
       "      <th>icat9</th>\n",
       "      <th>icat10</th>\n",
       "      <th>icat11</th>\n",
       "      <th>icat12</th>\n",
       "      <th>icat13</th>\n",
       "      <th>icat14</th>\n",
       "      <th>icat15</th>\n",
       "      <th>icat16</th>\n",
       "      <th>icat17</th>\n",
       "      <th>icat18</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>44</td>\n",
       "      <td>54</td>\n",
       "      <td>0</td>\n",
       "      <td>258</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>33</td>\n",
       "      <td>8</td>\n",
       "      <td>48</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>162</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>38</td>\n",
       "      <td>9</td>\n",
       "      <td>69</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>241</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>33</td>\n",
       "      <td>2</td>\n",
       "      <td>32</td>\n",
       "      <td>54</td>\n",
       "      <td>0</td>\n",
       "      <td>75</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299995</th>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>48</td>\n",
       "      <td>0</td>\n",
       "      <td>159</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299996</th>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>163</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299997</th>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>33</td>\n",
       "      <td>2</td>\n",
       "      <td>37</td>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "      <td>156</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299998</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299999</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>33</td>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>256</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300000 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        icat0  icat1  icat2  icat3  icat4  icat5  icat6  icat7  icat8  icat9  \\\n",
       "0           0      8      0      1      1     33      0     44     54      0   \n",
       "1           0      8      0      0      4     33      8     48      3      5   \n",
       "2           0     10      0      0      4     33      0     30     38      9   \n",
       "3           0     10      0      2      4     33      0     50      3      5   \n",
       "4           0      8      6      1      4     33      2     32     54      0   \n",
       "...       ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "299995      0     13      5      0      4     45      0     19     48      0   \n",
       "299996      0     10      0      0      6     33      0     36      4      4   \n",
       "299997      0      6     12      0      7     33      2     37     43      0   \n",
       "299998      1      7      0      3      1     33      0      1     23      0   \n",
       "299999      0      5      2      0      4     33      2     22     55      0   \n",
       "\n",
       "        icat10  icat11  icat12  icat13  icat14  icat15  icat16  icat17  icat18  \n",
       "0          258       0       0       0       0       1       3       3       1  \n",
       "1          162       0       1       0       1       3       1       3       1  \n",
       "2           69       0       1       0       0       1       3       3       1  \n",
       "3          241       0       0       0       0       1       3       3       1  \n",
       "4           75       0       0       0       1       1       1       3       1  \n",
       "...        ...     ...     ...     ...     ...     ...     ...     ...     ...  \n",
       "299995     159       0       0       0       1       3       1       3       1  \n",
       "299996     163       0       1       0       1       1       3       3       1  \n",
       "299997     156       1       0       0       1       3       1       3       3  \n",
       "299998      25       0       0       0       0       1       0       3       0  \n",
       "299999     256       0       0       0       0       1       3       3       1  \n",
       "\n",
       "[300000 rows x 19 columns]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.DataFrame ({'icat0':icat0,'icat1':icat1,'icat2':icat2,'icat3':icat3,'icat4':icat4,'icat5':icat5,\\\n",
    "                     'icat6':icat6,'icat7':icat7,'icat8':icat8,'icat9':icat9,'icat10':icat10,'icat11':icat11,\\\n",
    "                     'icat12':icat12,'icat13':icat13,'icat14':icat14,'icat15':icat15,'icat16':icat16,'icat17':icat17,\\\n",
    "                     'icat18':icat18})\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cont0</th>\n",
       "      <th>cont2</th>\n",
       "      <th>cont3</th>\n",
       "      <th>cont4</th>\n",
       "      <th>cont5</th>\n",
       "      <th>cont6</th>\n",
       "      <th>cont7</th>\n",
       "      <th>cont8</th>\n",
       "      <th>cont9</th>\n",
       "      <th>cont10</th>\n",
       "      <th>...</th>\n",
       "      <th>icat9</th>\n",
       "      <th>icat10</th>\n",
       "      <th>icat11</th>\n",
       "      <th>icat12</th>\n",
       "      <th>icat13</th>\n",
       "      <th>icat14</th>\n",
       "      <th>icat15</th>\n",
       "      <th>icat16</th>\n",
       "      <th>icat17</th>\n",
       "      <th>icat18</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.629858</td>\n",
       "      <td>0.759439</td>\n",
       "      <td>0.795549</td>\n",
       "      <td>0.681917</td>\n",
       "      <td>0.621672</td>\n",
       "      <td>0.592184</td>\n",
       "      <td>0.791921</td>\n",
       "      <td>0.815254</td>\n",
       "      <td>0.965006</td>\n",
       "      <td>0.665915</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>258</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.370727</td>\n",
       "      <td>0.386385</td>\n",
       "      <td>0.541366</td>\n",
       "      <td>0.388982</td>\n",
       "      <td>0.357778</td>\n",
       "      <td>0.600044</td>\n",
       "      <td>0.408701</td>\n",
       "      <td>0.399353</td>\n",
       "      <td>0.927406</td>\n",
       "      <td>0.493729</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>162</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.502272</td>\n",
       "      <td>0.343255</td>\n",
       "      <td>0.616352</td>\n",
       "      <td>0.793687</td>\n",
       "      <td>0.552877</td>\n",
       "      <td>0.352113</td>\n",
       "      <td>0.388835</td>\n",
       "      <td>0.412303</td>\n",
       "      <td>0.292696</td>\n",
       "      <td>0.549452</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>69</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.934242</td>\n",
       "      <td>0.831147</td>\n",
       "      <td>0.807807</td>\n",
       "      <td>0.800032</td>\n",
       "      <td>0.619147</td>\n",
       "      <td>0.221789</td>\n",
       "      <td>0.897617</td>\n",
       "      <td>0.633669</td>\n",
       "      <td>0.760318</td>\n",
       "      <td>0.934242</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>241</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.254427</td>\n",
       "      <td>0.338818</td>\n",
       "      <td>0.277308</td>\n",
       "      <td>0.610578</td>\n",
       "      <td>0.128291</td>\n",
       "      <td>0.578764</td>\n",
       "      <td>0.279167</td>\n",
       "      <td>0.351103</td>\n",
       "      <td>0.357084</td>\n",
       "      <td>0.328960</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>75</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299995</th>\n",
       "      <td>0.681700</td>\n",
       "      <td>0.662428</td>\n",
       "      <td>0.671927</td>\n",
       "      <td>0.390566</td>\n",
       "      <td>0.145840</td>\n",
       "      <td>0.262767</td>\n",
       "      <td>0.514248</td>\n",
       "      <td>0.519340</td>\n",
       "      <td>0.617436</td>\n",
       "      <td>0.688007</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>159</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299996</th>\n",
       "      <td>0.489226</td>\n",
       "      <td>0.821657</td>\n",
       "      <td>0.620356</td>\n",
       "      <td>0.384891</td>\n",
       "      <td>0.735879</td>\n",
       "      <td>0.547731</td>\n",
       "      <td>0.726653</td>\n",
       "      <td>0.470575</td>\n",
       "      <td>0.275743</td>\n",
       "      <td>0.638939</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>163</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299997</th>\n",
       "      <td>0.487882</td>\n",
       "      <td>0.407037</td>\n",
       "      <td>0.232436</td>\n",
       "      <td>0.832482</td>\n",
       "      <td>0.810663</td>\n",
       "      <td>0.596939</td>\n",
       "      <td>0.308821</td>\n",
       "      <td>0.373997</td>\n",
       "      <td>0.518024</td>\n",
       "      <td>0.452144</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>156</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299998</th>\n",
       "      <td>0.331900</td>\n",
       "      <td>0.808045</td>\n",
       "      <td>0.630708</td>\n",
       "      <td>0.346898</td>\n",
       "      <td>0.735147</td>\n",
       "      <td>0.563488</td>\n",
       "      <td>0.609836</td>\n",
       "      <td>0.680430</td>\n",
       "      <td>0.318453</td>\n",
       "      <td>0.335822</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299999</th>\n",
       "      <td>0.822600</td>\n",
       "      <td>0.775451</td>\n",
       "      <td>0.848696</td>\n",
       "      <td>0.819377</td>\n",
       "      <td>0.355467</td>\n",
       "      <td>0.218153</td>\n",
       "      <td>0.968856</td>\n",
       "      <td>0.823655</td>\n",
       "      <td>0.330515</td>\n",
       "      <td>0.972569</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>256</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300000 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           cont0     cont2     cont3     cont4     cont5     cont6     cont7  \\\n",
       "0       0.629858  0.759439  0.795549  0.681917  0.621672  0.592184  0.791921   \n",
       "1       0.370727  0.386385  0.541366  0.388982  0.357778  0.600044  0.408701   \n",
       "2       0.502272  0.343255  0.616352  0.793687  0.552877  0.352113  0.388835   \n",
       "3       0.934242  0.831147  0.807807  0.800032  0.619147  0.221789  0.897617   \n",
       "4       0.254427  0.338818  0.277308  0.610578  0.128291  0.578764  0.279167   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "299995  0.681700  0.662428  0.671927  0.390566  0.145840  0.262767  0.514248   \n",
       "299996  0.489226  0.821657  0.620356  0.384891  0.735879  0.547731  0.726653   \n",
       "299997  0.487882  0.407037  0.232436  0.832482  0.810663  0.596939  0.308821   \n",
       "299998  0.331900  0.808045  0.630708  0.346898  0.735147  0.563488  0.609836   \n",
       "299999  0.822600  0.775451  0.848696  0.819377  0.355467  0.218153  0.968856   \n",
       "\n",
       "           cont8     cont9    cont10  ...  icat9  icat10  icat11  icat12  \\\n",
       "0       0.815254  0.965006  0.665915  ...      0     258       0       0   \n",
       "1       0.399353  0.927406  0.493729  ...      5     162       0       1   \n",
       "2       0.412303  0.292696  0.549452  ...      9      69       0       1   \n",
       "3       0.633669  0.760318  0.934242  ...      5     241       0       0   \n",
       "4       0.351103  0.357084  0.328960  ...      0      75       0       0   \n",
       "...          ...       ...       ...  ...    ...     ...     ...     ...   \n",
       "299995  0.519340  0.617436  0.688007  ...      0     159       0       0   \n",
       "299996  0.470575  0.275743  0.638939  ...      4     163       0       1   \n",
       "299997  0.373997  0.518024  0.452144  ...      0     156       1       0   \n",
       "299998  0.680430  0.318453  0.335822  ...      0      25       0       0   \n",
       "299999  0.823655  0.330515  0.972569  ...      0     256       0       0   \n",
       "\n",
       "        icat13  icat14  icat15  icat16  icat17  icat18  \n",
       "0            0       0       1       3       3       1  \n",
       "1            0       1       3       1       3       1  \n",
       "2            0       0       1       3       3       1  \n",
       "3            0       0       1       3       3       1  \n",
       "4            0       1       1       1       3       1  \n",
       "...        ...     ...     ...     ...     ...     ...  \n",
       "299995       0       1       3       1       3       1  \n",
       "299996       0       1       1       3       3       1  \n",
       "299997       0       1       3       1       3       3  \n",
       "299998       0       0       1       0       3       0  \n",
       "299999       0       0       1       3       3       1  \n",
       "\n",
       "[300000 rows x 29 columns]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = pd.concat([df1[['cont0','cont2','cont3','cont4','cont5','cont6','cont7','cont8','cont9','cont10']],df2],axis=1)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the features and split the data into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.60501457,  1.13012057,  1.48320358,  0.77841806,  0.49499738,\n",
       "         0.49190174,  1.42598731,  1.82732733,  2.54740193,  0.77527564,\n",
       "        -0.5849203 , -0.03985428, -0.63495693,  0.32442796, -2.50936389,\n",
       "         0.26095686, -0.59667435,  1.47628735,  1.08344194, -0.58202622,\n",
       "         1.59106764, -0.39825287, -0.40826972, -0.15779161, -0.93437503,\n",
       "        -0.56025241,  0.65968443,  0.4308457 , -0.38312652],\n",
       "       [-0.6481137 , -0.60621963,  0.30988215, -0.50935767, -0.59889466,\n",
       "         0.52909034, -0.45719667, -0.49551903,  2.35409893, -0.07129282,\n",
       "        -0.5849203 , -0.03985428, -0.63495693, -0.5200993 , -0.53162154,\n",
       "         0.26095686,  3.18564049,  1.76448086, -1.60044309,  1.22990528,\n",
       "         0.28167178, -0.39825287,  2.44936115, -0.15779161,  1.07023408,\n",
       "         1.5744491 , -1.49142537,  0.4308457 , -0.38312652],\n",
       "       [-0.01197887, -0.80696417,  0.65601813,  1.2697701 ,  0.20982845,\n",
       "        -0.64407648, -0.55482142, -0.42318901, -0.90892674,  0.20267551,\n",
       "        -0.5849203 ,  0.61135943, -0.63495693, -0.5200993 , -0.53162154,\n",
       "         0.26095686, -0.59667435,  0.46761006,  0.24143879,  2.67945048,\n",
       "        -0.98680546, -0.39825287,  2.44936115, -0.15779161, -0.93437503,\n",
       "        -0.56025241,  0.65968443,  0.4308457 , -0.38312652],\n",
       "       [ 2.07698067,  1.46388124,  1.53978564,  1.29766629,  0.48453226,\n",
       "        -1.26075134,  1.94539096,  0.81316129,  1.49510667,  2.09453177,\n",
       "        -0.5849203 ,  0.61135943, -0.63495693,  1.16895522, -0.53162154,\n",
       "         0.26095686, -0.59667435,  1.90857761, -1.60044309,  1.22990528,\n",
       "         1.35919545, -0.39825287, -0.40826972, -0.15779161, -0.93437503,\n",
       "        -0.56025241,  0.65968443,  0.4308457 , -0.38312652],\n",
       "       [-1.21052602, -0.82761484, -0.90902528,  0.46480302, -1.55016554,\n",
       "         0.4283999 , -1.09374083, -0.76499927, -0.57790977, -0.88139728,\n",
       "        -0.5849203 , -0.03985428,  0.81560878,  0.32442796, -0.53162154,\n",
       "         0.26095686,  0.34890436,  0.61170682,  1.08344194, -0.58202622,\n",
       "        -0.90496821, -0.39825287, -0.40826972, -0.15779161,  1.07023408,\n",
       "        -0.56025241, -1.49142537,  0.4308457 , -0.38312652]])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X= preprocessing.StandardScaler().fit(X).transform(X)\n",
    "X[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y = df1['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\taihs\\anaconda3\\envs\\new_env\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:07:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=200, n_jobs=8, num_parallel_tree=1, random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb = XGBClassifier(n_estimators=200)\n",
    "xgb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is 0.846\n",
      "roc-auc is 0.887\n"
     ]
    }
   ],
   "source": [
    "y_pred_class_xgb = xgb.predict(X_test)\n",
    "y_pred_prob_xgb = xgb.predict_proba(X_test)\n",
    "\n",
    "print('accuracy is {:.3f}'.format(accuracy_score(y_test,y_pred_class_xgb)))\n",
    "print('roc-auc is {:.3f}'.format(roc_auc_score(y_test,y_pred_prob_xgb[:,1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(n_estimators=200)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_model = RandomForestClassifier(n_estimators=200)\n",
    "rf_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is 0.845\n",
      "roc-auc is 0.884\n"
     ]
    }
   ],
   "source": [
    "y_pred_class_rf = rf_model.predict(X_test)\n",
    "y_pred_prob_rf = rf_model.predict_proba(X_test)\n",
    "\n",
    "print('accuracy is {:.3f}'.format(accuracy_score(y_test,y_pred_class_rf)))\n",
    "print('roc-auc is {:.3f}'.format(roc_auc_score(y_test,y_pred_prob_rf[:,1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.93 , 0.07 ],\n",
       "       [0.33 , 0.67 ],\n",
       "       [0.87 , 0.13 ],\n",
       "       ...,\n",
       "       [0.845, 0.155],\n",
       "       [0.99 , 0.01 ],\n",
       "       [0.95 , 0.05 ]])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_prob_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cat0</th>\n",
       "      <th>cat1</th>\n",
       "      <th>cat2</th>\n",
       "      <th>cat3</th>\n",
       "      <th>cat4</th>\n",
       "      <th>cat5</th>\n",
       "      <th>cat6</th>\n",
       "      <th>cat7</th>\n",
       "      <th>cat8</th>\n",
       "      <th>...</th>\n",
       "      <th>cont1</th>\n",
       "      <th>cont2</th>\n",
       "      <th>cont3</th>\n",
       "      <th>cont4</th>\n",
       "      <th>cont5</th>\n",
       "      <th>cont6</th>\n",
       "      <th>cont7</th>\n",
       "      <th>cont8</th>\n",
       "      <th>cont9</th>\n",
       "      <th>cont10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>A</td>\n",
       "      <td>F</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>F</td>\n",
       "      <td>BI</td>\n",
       "      <td>A</td>\n",
       "      <td>AH</td>\n",
       "      <td>AX</td>\n",
       "      <td>...</td>\n",
       "      <td>0.735690</td>\n",
       "      <td>0.578366</td>\n",
       "      <td>0.723154</td>\n",
       "      <td>0.228037</td>\n",
       "      <td>0.356227</td>\n",
       "      <td>0.551249</td>\n",
       "      <td>0.655693</td>\n",
       "      <td>0.598331</td>\n",
       "      <td>0.359987</td>\n",
       "      <td>0.947489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>A</td>\n",
       "      <td>H</td>\n",
       "      <td>C</td>\n",
       "      <td>A</td>\n",
       "      <td>E</td>\n",
       "      <td>AB</td>\n",
       "      <td>I</td>\n",
       "      <td>F</td>\n",
       "      <td>N</td>\n",
       "      <td>...</td>\n",
       "      <td>0.313703</td>\n",
       "      <td>0.928885</td>\n",
       "      <td>0.516602</td>\n",
       "      <td>0.600169</td>\n",
       "      <td>0.795224</td>\n",
       "      <td>0.248987</td>\n",
       "      <td>0.654614</td>\n",
       "      <td>0.347944</td>\n",
       "      <td>0.565520</td>\n",
       "      <td>0.388580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>A</td>\n",
       "      <td>N</td>\n",
       "      <td>C</td>\n",
       "      <td>A</td>\n",
       "      <td>F</td>\n",
       "      <td>AB</td>\n",
       "      <td>A</td>\n",
       "      <td>AH</td>\n",
       "      <td>BC</td>\n",
       "      <td>...</td>\n",
       "      <td>0.448201</td>\n",
       "      <td>0.424876</td>\n",
       "      <td>0.344729</td>\n",
       "      <td>0.242073</td>\n",
       "      <td>0.270632</td>\n",
       "      <td>0.746740</td>\n",
       "      <td>0.335590</td>\n",
       "      <td>0.341238</td>\n",
       "      <td>0.252289</td>\n",
       "      <td>0.411592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>B</td>\n",
       "      <td>L</td>\n",
       "      <td>C</td>\n",
       "      <td>A</td>\n",
       "      <td>F</td>\n",
       "      <td>BI</td>\n",
       "      <td>A</td>\n",
       "      <td>E</td>\n",
       "      <td>AX</td>\n",
       "      <td>...</td>\n",
       "      <td>0.666092</td>\n",
       "      <td>0.598943</td>\n",
       "      <td>0.561971</td>\n",
       "      <td>0.806347</td>\n",
       "      <td>0.735983</td>\n",
       "      <td>0.538724</td>\n",
       "      <td>0.381566</td>\n",
       "      <td>0.481660</td>\n",
       "      <td>0.348514</td>\n",
       "      <td>0.325723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>A</td>\n",
       "      <td>F</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>F</td>\n",
       "      <td>BI</td>\n",
       "      <td>A</td>\n",
       "      <td>AH</td>\n",
       "      <td>I</td>\n",
       "      <td>...</td>\n",
       "      <td>0.772229</td>\n",
       "      <td>0.479572</td>\n",
       "      <td>0.767745</td>\n",
       "      <td>0.252454</td>\n",
       "      <td>0.354810</td>\n",
       "      <td>0.178920</td>\n",
       "      <td>0.763479</td>\n",
       "      <td>0.562491</td>\n",
       "      <td>0.466261</td>\n",
       "      <td>0.585781</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id cat0 cat1 cat2 cat3 cat4 cat5 cat6 cat7 cat8  ...     cont1     cont2  \\\n",
       "0   5    A    F    A    A    F   BI    A   AH   AX  ...  0.735690  0.578366   \n",
       "1   6    A    H    C    A    E   AB    I    F    N  ...  0.313703  0.928885   \n",
       "2   8    A    N    C    A    F   AB    A   AH   BC  ...  0.448201  0.424876   \n",
       "3   9    B    L    C    A    F   BI    A    E   AX  ...  0.666092  0.598943   \n",
       "4  11    A    F    A    B    F   BI    A   AH    I  ...  0.772229  0.479572   \n",
       "\n",
       "      cont3     cont4     cont5     cont6     cont7     cont8     cont9  \\\n",
       "0  0.723154  0.228037  0.356227  0.551249  0.655693  0.598331  0.359987   \n",
       "1  0.516602  0.600169  0.795224  0.248987  0.654614  0.347944  0.565520   \n",
       "2  0.344729  0.242073  0.270632  0.746740  0.335590  0.341238  0.252289   \n",
       "3  0.561971  0.806347  0.735983  0.538724  0.381566  0.481660  0.348514   \n",
       "4  0.767745  0.252454  0.354810  0.178920  0.763479  0.562491  0.466261   \n",
       "\n",
       "     cont10  \n",
       "0  0.947489  \n",
       "1  0.388580  \n",
       "2  0.411592  \n",
       "3  0.325723  \n",
       "4  0.585781  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.read_csv('C:/Users/taihs/OneDrive/Documents/tps competition/test.csv')\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "icat0=le.fit_transform(df1['cat0'])\n",
    "icat1=le.fit_transform(df1['cat1'])\n",
    "icat2=le.fit_transform(df1['cat2'])\n",
    "icat3=le.fit_transform(df1['cat3'])\n",
    "icat4=le.fit_transform(df1['cat4'])\n",
    "icat5=le.fit_transform(df1['cat5'])\n",
    "icat6=le.fit_transform(df1['cat6'])\n",
    "icat7=le.fit_transform(df1['cat7'])\n",
    "icat8=le.fit_transform(df1['cat8'])\n",
    "icat9=le.fit_transform(df1['cat9'])\n",
    "icat10=le.fit_transform(df1['cat10'])\n",
    "icat11=le.fit_transform(df1['cat11'])\n",
    "icat12=le.fit_transform(df1['cat12'])\n",
    "icat13=le.fit_transform(df1['cat13'])\n",
    "icat14=le.fit_transform(df1['cat14'])\n",
    "icat15=le.fit_transform(df1['cat15'])\n",
    "icat16=le.fit_transform(df1['cat16'])\n",
    "icat17=le.fit_transform(df1['cat17'])\n",
    "icat18=le.fit_transform(df1['cat18'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>icat0</th>\n",
       "      <th>icat1</th>\n",
       "      <th>icat2</th>\n",
       "      <th>icat3</th>\n",
       "      <th>icat4</th>\n",
       "      <th>icat5</th>\n",
       "      <th>icat6</th>\n",
       "      <th>icat7</th>\n",
       "      <th>icat8</th>\n",
       "      <th>icat9</th>\n",
       "      <th>icat10</th>\n",
       "      <th>icat11</th>\n",
       "      <th>icat12</th>\n",
       "      <th>icat13</th>\n",
       "      <th>icat14</th>\n",
       "      <th>icat15</th>\n",
       "      <th>icat16</th>\n",
       "      <th>icat17</th>\n",
       "      <th>icat18</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>249</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>31</td>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "      <td>269</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>121</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>162</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>46</td>\n",
       "      <td>0</td>\n",
       "      <td>173</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199995</th>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>75</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199996</th>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>55</td>\n",
       "      <td>4</td>\n",
       "      <td>180</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199997</th>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>171</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199998</th>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>43</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>162</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199999</th>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>33</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>38</td>\n",
       "      <td>7</td>\n",
       "      <td>69</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200000 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        icat0  icat1  icat2  icat3  icat4  icat5  icat6  icat7  icat8  icat9  \\\n",
       "0           0      5      0      0      5     33      0      8     23      0   \n",
       "1           0      7      2      0      4      2      7     31     51      0   \n",
       "2           0     13      2      0      5      2      0      8     28      0   \n",
       "3           1     11      2      0      5     33      0     30     23      0   \n",
       "4           0      5      0      1      5     33      0      8     46      0   \n",
       "...       ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "199995      0     13      0      3      5     33      0      6     23      0   \n",
       "199996      1      8      0      1      4     33      0     19     55      4   \n",
       "199997      0     11      3      0      7     33      0     14     47      0   \n",
       "199998      0     10      0      2      5     33      0     43     38      0   \n",
       "199999      0     10      0      0      4     33      2     15     38      7   \n",
       "\n",
       "        icat10  icat11  icat12  icat13  icat14  icat15  icat16  icat17  icat18  \n",
       "0          249       0       0       0       0       1       3       3       1  \n",
       "1          269       0       0       0       1       3       1       3       1  \n",
       "2          121       0       0       0       1       1       3       3       1  \n",
       "3          162       0       0       0       0       1       3       1       1  \n",
       "4          173       0       0       0       0       3       3       3       1  \n",
       "...        ...     ...     ...     ...     ...     ...     ...     ...     ...  \n",
       "199995      75       0       0       0       1       3       1       2       1  \n",
       "199996     180       0       0       0       1       1       3       3       1  \n",
       "199997     171       0       0       0       1       3       1       3       1  \n",
       "199998     162       0       0       0       0       1       3       3       1  \n",
       "199999      69       0       1       0       0       1       1       1       1  \n",
       "\n",
       "[200000 rows x 19 columns]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.DataFrame ({'icat0':icat0,'icat1':icat1,'icat2':icat2,'icat3':icat3,'icat4':icat4,'icat5':icat5,\\\n",
    "                     'icat6':icat6,'icat7':icat7,'icat8':icat8,'icat9':icat9,'icat10':icat10,'icat11':icat11,\\\n",
    "                     'icat12':icat12,'icat13':icat13,'icat14':icat14,'icat15':icat15,'icat16':icat16,'icat17':icat17,\\\n",
    "                     'icat18':icat18})\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cont0</th>\n",
       "      <th>cont2</th>\n",
       "      <th>cont3</th>\n",
       "      <th>cont4</th>\n",
       "      <th>cont5</th>\n",
       "      <th>cont6</th>\n",
       "      <th>cont7</th>\n",
       "      <th>cont8</th>\n",
       "      <th>cont9</th>\n",
       "      <th>cont10</th>\n",
       "      <th>...</th>\n",
       "      <th>icat9</th>\n",
       "      <th>icat10</th>\n",
       "      <th>icat11</th>\n",
       "      <th>icat12</th>\n",
       "      <th>icat13</th>\n",
       "      <th>icat14</th>\n",
       "      <th>icat15</th>\n",
       "      <th>icat16</th>\n",
       "      <th>icat17</th>\n",
       "      <th>icat18</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.708120</td>\n",
       "      <td>0.578366</td>\n",
       "      <td>0.723154</td>\n",
       "      <td>0.228037</td>\n",
       "      <td>0.356227</td>\n",
       "      <td>0.551249</td>\n",
       "      <td>0.655693</td>\n",
       "      <td>0.598331</td>\n",
       "      <td>0.359987</td>\n",
       "      <td>0.947489</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>249</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.611637</td>\n",
       "      <td>0.928885</td>\n",
       "      <td>0.516602</td>\n",
       "      <td>0.600169</td>\n",
       "      <td>0.795224</td>\n",
       "      <td>0.248987</td>\n",
       "      <td>0.654614</td>\n",
       "      <td>0.347944</td>\n",
       "      <td>0.565520</td>\n",
       "      <td>0.388580</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>269</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.456289</td>\n",
       "      <td>0.424876</td>\n",
       "      <td>0.344729</td>\n",
       "      <td>0.242073</td>\n",
       "      <td>0.270632</td>\n",
       "      <td>0.746740</td>\n",
       "      <td>0.335590</td>\n",
       "      <td>0.341238</td>\n",
       "      <td>0.252289</td>\n",
       "      <td>0.411592</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>121</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.338692</td>\n",
       "      <td>0.598943</td>\n",
       "      <td>0.561971</td>\n",
       "      <td>0.806347</td>\n",
       "      <td>0.735983</td>\n",
       "      <td>0.538724</td>\n",
       "      <td>0.381566</td>\n",
       "      <td>0.481660</td>\n",
       "      <td>0.348514</td>\n",
       "      <td>0.325723</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>162</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.631671</td>\n",
       "      <td>0.479572</td>\n",
       "      <td>0.767745</td>\n",
       "      <td>0.252454</td>\n",
       "      <td>0.354810</td>\n",
       "      <td>0.178920</td>\n",
       "      <td>0.763479</td>\n",
       "      <td>0.562491</td>\n",
       "      <td>0.466261</td>\n",
       "      <td>0.585781</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>173</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199995</th>\n",
       "      <td>0.451496</td>\n",
       "      <td>0.351946</td>\n",
       "      <td>0.327670</td>\n",
       "      <td>0.205547</td>\n",
       "      <td>0.679195</td>\n",
       "      <td>0.485967</td>\n",
       "      <td>0.319130</td>\n",
       "      <td>0.520681</td>\n",
       "      <td>0.519545</td>\n",
       "      <td>0.427119</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>75</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199996</th>\n",
       "      <td>0.862988</td>\n",
       "      <td>0.628843</td>\n",
       "      <td>0.677765</td>\n",
       "      <td>0.624935</td>\n",
       "      <td>0.555306</td>\n",
       "      <td>0.242424</td>\n",
       "      <td>0.700829</td>\n",
       "      <td>0.531728</td>\n",
       "      <td>0.528427</td>\n",
       "      <td>0.922645</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>180</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199997</th>\n",
       "      <td>0.463067</td>\n",
       "      <td>0.803348</td>\n",
       "      <td>0.324762</td>\n",
       "      <td>0.665624</td>\n",
       "      <td>0.488447</td>\n",
       "      <td>0.853213</td>\n",
       "      <td>0.578641</td>\n",
       "      <td>0.811941</td>\n",
       "      <td>0.537106</td>\n",
       "      <td>0.531758</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>171</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199998</th>\n",
       "      <td>0.519671</td>\n",
       "      <td>0.820635</td>\n",
       "      <td>0.561449</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.555089</td>\n",
       "      <td>0.746532</td>\n",
       "      <td>0.369986</td>\n",
       "      <td>0.438712</td>\n",
       "      <td>0.715524</td>\n",
       "      <td>0.381978</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>162</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199999</th>\n",
       "      <td>0.498328</td>\n",
       "      <td>0.270483</td>\n",
       "      <td>0.581868</td>\n",
       "      <td>0.218993</td>\n",
       "      <td>0.553284</td>\n",
       "      <td>0.565213</td>\n",
       "      <td>0.378355</td>\n",
       "      <td>0.547927</td>\n",
       "      <td>0.273595</td>\n",
       "      <td>0.448016</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>69</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200000 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           cont0     cont2     cont3     cont4     cont5     cont6     cont7  \\\n",
       "0       0.708120  0.578366  0.723154  0.228037  0.356227  0.551249  0.655693   \n",
       "1       0.611637  0.928885  0.516602  0.600169  0.795224  0.248987  0.654614   \n",
       "2       0.456289  0.424876  0.344729  0.242073  0.270632  0.746740  0.335590   \n",
       "3       0.338692  0.598943  0.561971  0.806347  0.735983  0.538724  0.381566   \n",
       "4       0.631671  0.479572  0.767745  0.252454  0.354810  0.178920  0.763479   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "199995  0.451496  0.351946  0.327670  0.205547  0.679195  0.485967  0.319130   \n",
       "199996  0.862988  0.628843  0.677765  0.624935  0.555306  0.242424  0.700829   \n",
       "199997  0.463067  0.803348  0.324762  0.665624  0.488447  0.853213  0.578641   \n",
       "199998  0.519671  0.820635  0.561449  0.797434  0.555089  0.746532  0.369986   \n",
       "199999  0.498328  0.270483  0.581868  0.218993  0.553284  0.565213  0.378355   \n",
       "\n",
       "           cont8     cont9    cont10  ...  icat9  icat10  icat11  icat12  \\\n",
       "0       0.598331  0.359987  0.947489  ...      0     249       0       0   \n",
       "1       0.347944  0.565520  0.388580  ...      0     269       0       0   \n",
       "2       0.341238  0.252289  0.411592  ...      0     121       0       0   \n",
       "3       0.481660  0.348514  0.325723  ...      0     162       0       0   \n",
       "4       0.562491  0.466261  0.585781  ...      0     173       0       0   \n",
       "...          ...       ...       ...  ...    ...     ...     ...     ...   \n",
       "199995  0.520681  0.519545  0.427119  ...      0      75       0       0   \n",
       "199996  0.531728  0.528427  0.922645  ...      4     180       0       0   \n",
       "199997  0.811941  0.537106  0.531758  ...      0     171       0       0   \n",
       "199998  0.438712  0.715524  0.381978  ...      0     162       0       0   \n",
       "199999  0.547927  0.273595  0.448016  ...      7      69       0       1   \n",
       "\n",
       "        icat13  icat14  icat15  icat16  icat17  icat18  \n",
       "0            0       0       1       3       3       1  \n",
       "1            0       1       3       1       3       1  \n",
       "2            0       1       1       3       3       1  \n",
       "3            0       0       1       3       1       1  \n",
       "4            0       0       3       3       3       1  \n",
       "...        ...     ...     ...     ...     ...     ...  \n",
       "199995       0       1       3       1       2       1  \n",
       "199996       0       1       1       3       3       1  \n",
       "199997       0       1       3       1       3       1  \n",
       "199998       0       0       1       3       3       1  \n",
       "199999       0       0       1       1       1       1  \n",
       "\n",
       "[200000 rows x 29 columns]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = pd.concat([df1[['cont0','cont2','cont3','cont4','cont5','cont6','cont7','cont8','cont9','cont10']],df2],axis=1)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.97741412,  0.28843199,  1.14267841, -1.21803283, -0.60053348,\n",
       "         0.30252531,  0.75080726,  0.61488305, -0.56321106,  2.15036926,\n",
       "        -0.58487154, -1.01120305, -0.63623041, -0.51805892,  0.129466  ,\n",
       "         0.26538736, -0.59773745, -1.11838839, -0.55275334, -0.58627124,\n",
       "         1.47795544, -0.39855056, -0.41099968, -0.15986519, -0.93597511,\n",
       "        -0.5627497 ,  0.6599291 ,  0.42860004, -0.38534137],\n",
       "       [ 0.51225066,  1.9206372 ,  0.19047001,  0.4198866 ,  1.21820512,\n",
       "        -1.12772714,  0.74551361, -0.78475036,  0.49081065, -0.59038497,\n",
       "        -0.58487154, -0.36206283, -0.15275887, -0.51805892, -0.53313698,\n",
       "        -2.17765193,  2.71391909,  0.5377579 ,  0.92031982, -0.58627124,\n",
       "         1.75172996, -0.39855056, -0.41099968, -0.15986519,  1.06840447,\n",
       "         1.56988773, -1.49372835,  0.42860004, -0.38534137],\n",
       "       [-0.23671531, -0.42630047, -0.60187195, -1.15625432, -0.95514916,\n",
       "         1.22755467, -0.81992081, -0.82223402, -1.11551595, -0.47753823,\n",
       "        -0.58487154,  1.58535784, -0.15275887, -0.51805892,  0.129466  ,\n",
       "        -2.17765193, -0.59773745, -1.11838839, -0.28970456, -0.58627124,\n",
       "        -0.27420147, -0.39855056, -0.41099968, -0.15986519,  1.06840447,\n",
       "        -0.5627497 ,  0.6599291 ,  0.42860004, -0.38534137],\n",
       "       [-0.80367591,  0.3842466 ,  0.39962001,  1.32736709,  0.97277597,\n",
       "         0.24326041, -0.59431757, -0.03729403, -0.62205003, -0.89862072,\n",
       "         1.70977717,  0.93621762, -0.15275887, -0.51805892,  0.129466  ,\n",
       "         0.26538736, -0.59773745,  0.46575154, -0.55275334, -0.58627124,\n",
       "         0.28703629, -0.39855056, -0.41099968, -0.15986519, -0.93597511,\n",
       "        -0.5627497 ,  0.6599291 , -2.7478621 , -0.38534137],\n",
       "       [ 0.60883821, -0.17160652,  1.34824586, -1.11056186, -0.60640473,\n",
       "        -1.45927325,  1.27970508,  0.41454401, -0.01821405,  0.3766383 ,\n",
       "        -0.58487154, -1.01120305, -0.63623041,  0.31427121,  0.129466  ,\n",
       "         0.26538736, -0.59773745, -1.11838839,  0.65727104, -0.58627124,\n",
       "         0.43761228, -0.39855056, -0.41099968, -0.15986519, -0.93597511,\n",
       "         1.56988773,  0.6599291 ,  0.42860004, -0.38534137]])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X= preprocessing.StandardScaler().fit(X).transform(X)\n",
    "X[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#y_pred_class_rf = rf_model.predict(X_test)\n",
    "#y_pred_prob_rf = rf_model.predict_proba(X_test)\n",
    "\n",
    "y_pred_class_xgb = xgb.predict(X)\n",
    "y_pred_prob_xgb = xgb.predict_proba(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00328044, 0.00677099, 0.00414476, 0.00469036, 0.00994086,\n",
       "       0.0056187 , 0.00335744, 0.00450182, 0.0032738 , 0.00357411,\n",
       "       0.06638327, 0.01326421, 0.01182697, 0.00342129, 0.0174576 ,\n",
       "       0.00457167, 0.00685967, 0.00482599, 0.00437564, 0.00501656,\n",
       "       0.00458398, 0.03178615, 0.00824781, 0.03230882, 0.03149007,\n",
       "       0.06722327, 0.55155146, 0.01565459, 0.06999774], dtype=float32)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_model.feature_importances_\n",
    "xgb.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.10615856, 0.5230667 , 0.01071627, ..., 0.44984958, 0.08184002,\n",
       "       0.50207084], dtype=float32)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#y_pred_prob_rf[:,1]\n",
    "y_pred_prob_xgb[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>199995</th>\n",
       "      <td>0.934737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199996</th>\n",
       "      <td>0.041234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199997</th>\n",
       "      <td>0.506456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199998</th>\n",
       "      <td>0.089119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199999</th>\n",
       "      <td>0.505440</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          target\n",
       "199995  0.934737\n",
       "199996  0.041234\n",
       "199997  0.506456\n",
       "199998  0.089119\n",
       "199999  0.505440"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#final = pd.DataFrame({'target':y_pred_prob_rf[:,1]})\n",
    "final = pd.DataFrame({'target':y_pred_prob_xgb[:,1]})\n",
    "final.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>199995</th>\n",
       "      <td>499983</td>\n",
       "      <td>0.934737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199996</th>\n",
       "      <td>499984</td>\n",
       "      <td>0.041234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199997</th>\n",
       "      <td>499987</td>\n",
       "      <td>0.506456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199998</th>\n",
       "      <td>499994</td>\n",
       "      <td>0.089119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199999</th>\n",
       "      <td>499998</td>\n",
       "      <td>0.505440</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id    target\n",
       "199995  499983  0.934737\n",
       "199996  499984  0.041234\n",
       "199997  499987  0.506456\n",
       "199998  499994  0.089119\n",
       "199999  499998  0.505440"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final1 = pd.concat([df1['id'],final],axis=1)\n",
    "final1.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "final1.to_csv('sample_submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a Neural Network model with one hidden layer with sigmoid activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 12)                360       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 13        \n",
      "=================================================================\n",
      "Total params: 373\n",
      "Trainable params: 373\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_1 = Sequential()\n",
    "model_1.add(Dense(12,input_shape = (29,),activation = 'sigmoid'))\n",
    "model_1.add(Dense(1,activation='sigmoid'))\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "7500/7500 [==============================] - 8s 1ms/step - loss: 0.5380 - accuracy: 0.7380 - val_loss: 0.5037 - val_accuracy: 0.7449\n",
      "Epoch 2/50\n",
      "7500/7500 [==============================] - 4s 532us/step - loss: 0.4838 - accuracy: 0.7728 - val_loss: 0.4675 - val_accuracy: 0.7849\n",
      "Epoch 3/50\n",
      "7500/7500 [==============================] - 4s 512us/step - loss: 0.4515 - accuracy: 0.8032 - val_loss: 0.4307 - val_accuracy: 0.8188\n",
      "Epoch 4/50\n",
      "7500/7500 [==============================] - 4s 516us/step - loss: 0.4368 - accuracy: 0.8118 - val_loss: 0.4238 - val_accuracy: 0.8149\n",
      "Epoch 5/50\n",
      "7500/7500 [==============================] - 4s 511us/step - loss: 0.4312 - accuracy: 0.8139 - val_loss: 0.4169 - val_accuracy: 0.8200\n",
      "Epoch 6/50\n",
      "7500/7500 [==============================] - 4s 513us/step - loss: 0.4273 - accuracy: 0.8159 - val_loss: 0.4183 - val_accuracy: 0.8245\n",
      "Epoch 7/50\n",
      "7500/7500 [==============================] - 4s 513us/step - loss: 0.4259 - accuracy: 0.8167 - val_loss: 0.4393 - val_accuracy: 0.8005\n",
      "Epoch 8/50\n",
      "7500/7500 [==============================] - 4s 511us/step - loss: 0.4236 - accuracy: 0.8184 - val_loss: 0.4151 - val_accuracy: 0.8244\n",
      "Epoch 9/50\n",
      "7500/7500 [==============================] - 4s 511us/step - loss: 0.4227 - accuracy: 0.8185 - val_loss: 0.4233 - val_accuracy: 0.8227\n",
      "Epoch 10/50\n",
      "7500/7500 [==============================] - 4s 511us/step - loss: 0.4210 - accuracy: 0.8192 - val_loss: 0.4279 - val_accuracy: 0.8238\n",
      "Epoch 11/50\n",
      "7500/7500 [==============================] - 4s 525us/step - loss: 0.4204 - accuracy: 0.8205 - val_loss: 0.4288 - val_accuracy: 0.8207\n",
      "Epoch 12/50\n",
      "7500/7500 [==============================] - 4s 509us/step - loss: 0.4194 - accuracy: 0.8205 - val_loss: 0.4546 - val_accuracy: 0.8192\n",
      "Epoch 13/50\n",
      "7500/7500 [==============================] - 4s 512us/step - loss: 0.4190 - accuracy: 0.8205 - val_loss: 0.4536 - val_accuracy: 0.8081\n",
      "Epoch 14/50\n",
      "7500/7500 [==============================] - 4s 532us/step - loss: 0.4179 - accuracy: 0.8217 - val_loss: 0.4169 - val_accuracy: 0.8155\n",
      "Epoch 15/50\n",
      "7500/7500 [==============================] - 4s 567us/step - loss: 0.4170 - accuracy: 0.8219 - val_loss: 0.4476 - val_accuracy: 0.7942\n",
      "Epoch 16/50\n",
      "7500/7500 [==============================] - 4s 517us/step - loss: 0.4164 - accuracy: 0.8225 - val_loss: 0.4092 - val_accuracy: 0.8232\n",
      "Epoch 17/50\n",
      "7500/7500 [==============================] - 4s 539us/step - loss: 0.4161 - accuracy: 0.8226 - val_loss: 0.4082 - val_accuracy: 0.8239\n",
      "Epoch 18/50\n",
      "7500/7500 [==============================] - 4s 574us/step - loss: 0.4151 - accuracy: 0.8226 - val_loss: 0.4068 - val_accuracy: 0.8277\n",
      "Epoch 19/50\n",
      "7500/7500 [==============================] - 4s 529us/step - loss: 0.4144 - accuracy: 0.8229 - val_loss: 0.4068 - val_accuracy: 0.8248\n",
      "Epoch 20/50\n",
      "7500/7500 [==============================] - 4s 520us/step - loss: 0.4132 - accuracy: 0.8236 - val_loss: 0.4201 - val_accuracy: 0.8212\n",
      "Epoch 21/50\n",
      "7500/7500 [==============================] - 4s 519us/step - loss: 0.4122 - accuracy: 0.8238 - val_loss: 0.4097 - val_accuracy: 0.8258\n",
      "Epoch 22/50\n",
      "7500/7500 [==============================] - 4s 518us/step - loss: 0.4117 - accuracy: 0.8242 - val_loss: 0.4079 - val_accuracy: 0.8263\n",
      "Epoch 23/50\n",
      "7500/7500 [==============================] - 4s 521us/step - loss: 0.4112 - accuracy: 0.8242 - val_loss: 0.4181 - val_accuracy: 0.8219\n",
      "Epoch 24/50\n",
      "7500/7500 [==============================] - 4s 521us/step - loss: 0.4111 - accuracy: 0.8247 - val_loss: 0.4058 - val_accuracy: 0.8301\n",
      "Epoch 25/50\n",
      "7500/7500 [==============================] - 4s 521us/step - loss: 0.4099 - accuracy: 0.8249 - val_loss: 0.4151 - val_accuracy: 0.8292\n",
      "Epoch 26/50\n",
      "7500/7500 [==============================] - 4s 528us/step - loss: 0.4096 - accuracy: 0.8251 - val_loss: 0.4045 - val_accuracy: 0.8287\n",
      "Epoch 27/50\n",
      "7500/7500 [==============================] - 4s 521us/step - loss: 0.4089 - accuracy: 0.8253 - val_loss: 0.4015 - val_accuracy: 0.8289\n",
      "Epoch 28/50\n",
      "7500/7500 [==============================] - 4s 521us/step - loss: 0.4090 - accuracy: 0.8250 - val_loss: 0.4043 - val_accuracy: 0.8300\n",
      "Epoch 29/50\n",
      "7500/7500 [==============================] - 4s 519us/step - loss: 0.4080 - accuracy: 0.8253 - val_loss: 0.4195 - val_accuracy: 0.8237\n",
      "Epoch 30/50\n",
      "7500/7500 [==============================] - 4s 519us/step - loss: 0.4080 - accuracy: 0.8256 - val_loss: 0.4036 - val_accuracy: 0.8301\n",
      "Epoch 31/50\n",
      "7500/7500 [==============================] - 4s 533us/step - loss: 0.4080 - accuracy: 0.8252 - val_loss: 0.4104 - val_accuracy: 0.8202\n",
      "Epoch 32/50\n",
      "7500/7500 [==============================] - 4s 538us/step - loss: 0.4069 - accuracy: 0.8264 - val_loss: 0.4072 - val_accuracy: 0.8199\n",
      "Epoch 33/50\n",
      "7500/7500 [==============================] - 4s 526us/step - loss: 0.4062 - accuracy: 0.8261 - val_loss: 0.4053 - val_accuracy: 0.8261\n",
      "Epoch 34/50\n",
      "7500/7500 [==============================] - 10s 1ms/step - loss: 0.4068 - accuracy: 0.8261 - val_loss: 0.4020 - val_accuracy: 0.8289\n",
      "Epoch 35/50\n",
      "7500/7500 [==============================] - 4s 505us/step - loss: 0.4057 - accuracy: 0.8266 - val_loss: 0.4131 - val_accuracy: 0.8289\n",
      "Epoch 36/50\n",
      "7500/7500 [==============================] - 4s 537us/step - loss: 0.4058 - accuracy: 0.8270 - val_loss: 0.4098 - val_accuracy: 0.8290\n",
      "Epoch 37/50\n",
      "7500/7500 [==============================] - 4s 527us/step - loss: 0.4055 - accuracy: 0.8267 - val_loss: 0.4010 - val_accuracy: 0.8309\n",
      "Epoch 38/50\n",
      "7500/7500 [==============================] - 4s 527us/step - loss: 0.4051 - accuracy: 0.8266 - val_loss: 0.3990 - val_accuracy: 0.8280\n",
      "Epoch 39/50\n",
      "7500/7500 [==============================] - 4s 510us/step - loss: 0.4049 - accuracy: 0.8265 - val_loss: 0.3965 - val_accuracy: 0.8307\n",
      "Epoch 40/50\n",
      "7500/7500 [==============================] - 4s 524us/step - loss: 0.4048 - accuracy: 0.8269 - val_loss: 0.4103 - val_accuracy: 0.8313\n",
      "Epoch 41/50\n",
      "7500/7500 [==============================] - 4s 526us/step - loss: 0.4042 - accuracy: 0.8269 - val_loss: 0.3982 - val_accuracy: 0.8289\n",
      "Epoch 42/50\n",
      "7500/7500 [==============================] - 4s 516us/step - loss: 0.4046 - accuracy: 0.8271 - val_loss: 0.3989 - val_accuracy: 0.8322\n",
      "Epoch 43/50\n",
      "7500/7500 [==============================] - 4s 527us/step - loss: 0.4037 - accuracy: 0.8274 - val_loss: 0.3991 - val_accuracy: 0.8258\n",
      "Epoch 44/50\n",
      "7500/7500 [==============================] - 4s 533us/step - loss: 0.4036 - accuracy: 0.8275 - val_loss: 0.3972 - val_accuracy: 0.8310\n",
      "Epoch 45/50\n",
      "7500/7500 [==============================] - 4s 517us/step - loss: 0.4032 - accuracy: 0.8278 - val_loss: 0.3941 - val_accuracy: 0.8321\n",
      "Epoch 46/50\n",
      "7500/7500 [==============================] - 4s 535us/step - loss: 0.4033 - accuracy: 0.8276 - val_loss: 0.4020 - val_accuracy: 0.8240\n",
      "Epoch 47/50\n",
      "7500/7500 [==============================] - 4s 537us/step - loss: 0.4029 - accuracy: 0.8274 - val_loss: 0.4447 - val_accuracy: 0.8157\n",
      "Epoch 48/50\n",
      "7500/7500 [==============================] - 4s 512us/step - loss: 0.4031 - accuracy: 0.8281 - val_loss: 0.3943 - val_accuracy: 0.8311\n",
      "Epoch 49/50\n",
      "7500/7500 [==============================] - 4s 511us/step - loss: 0.4024 - accuracy: 0.8279 - val_loss: 0.3971 - val_accuracy: 0.8298\n",
      "Epoch 50/50\n",
      "7500/7500 [==============================] - 4s 511us/step - loss: 0.4021 - accuracy: 0.8281 - val_loss: 0.4076 - val_accuracy: 0.8301\n"
     ]
    }
   ],
   "source": [
    "model_1.compile(SGD(lr = .003), \"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "run_hist_1 = model_1.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-28-98292c949c9b>:1: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
      "Instructions for updating:\n",
      "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n"
     ]
    }
   ],
   "source": [
    "y_pred_class_nn_1 = model_1.predict_classes(X_test)\n",
    "y_pred_prob_nn_1 = model_1.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is 0.830\n",
      "roc-auc is 0.851\n"
     ]
    }
   ],
   "source": [
    "print('accuracy is {:.3f}'.format(accuracy_score(y_test,y_pred_class_nn_1)))\n",
    "print('roc-auc is {:.3f}'.format(roc_auc_score(y_test,y_pred_prob_nn_1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x16e03f30d90>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(run_hist_1.history[\"loss\"],'r', marker='.', label=\"Train Loss\")\n",
    "ax.plot(run_hist_1.history[\"val_loss\"],'b', marker='.', label=\"Validation Loss\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add two hidden layers to the Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_6 (Dense)              (None, 12)                360       \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 12)                156       \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 13        \n",
      "=================================================================\n",
      "Total params: 529\n",
      "Trainable params: 529\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_1 = Sequential()\n",
    "model_1.add(Dense(12,input_shape = (29,),activation = 'sigmoid'))\n",
    "model_1.add(Dense(12,activation='sigmoid'))\n",
    "model_1.add(Dense(1,activation='sigmoid'))\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "7500/7500 [==============================] - 5s 647us/step - loss: 0.5857 - accuracy: 0.7224 - val_loss: 0.5747 - val_accuracy: 0.7362\n",
      "Epoch 2/50\n",
      "7500/7500 [==============================] - 4s 544us/step - loss: 0.5746 - accuracy: 0.7348 - val_loss: 0.5713 - val_accuracy: 0.7362\n",
      "Epoch 3/50\n",
      "7500/7500 [==============================] - 4s 558us/step - loss: 0.5691 - accuracy: 0.7348 - val_loss: 0.5628 - val_accuracy: 0.7362\n",
      "Epoch 4/50\n",
      "7500/7500 [==============================] - 4s 566us/step - loss: 0.5564 - accuracy: 0.7348 - val_loss: 0.5486 - val_accuracy: 0.7362\n",
      "Epoch 5/50\n",
      "7500/7500 [==============================] - 4s 561us/step - loss: 0.5326 - accuracy: 0.7348 - val_loss: 0.5190 - val_accuracy: 0.7362\n",
      "Epoch 6/50\n",
      "7500/7500 [==============================] - 4s 584us/step - loss: 0.4928 - accuracy: 0.7529 - val_loss: 0.4795 - val_accuracy: 0.7778\n",
      "Epoch 7/50\n",
      "7500/7500 [==============================] - 9s 1ms/step - loss: 0.4572 - accuracy: 0.8031 - val_loss: 0.4330 - val_accuracy: 0.8194\n",
      "Epoch 8/50\n",
      "7500/7500 [==============================] - 4s 538us/step - loss: 0.4426 - accuracy: 0.8093 - val_loss: 0.4269 - val_accuracy: 0.8209\n",
      "Epoch 9/50\n",
      "7500/7500 [==============================] - 5s 633us/step - loss: 0.4364 - accuracy: 0.8114 - val_loss: 0.4576 - val_accuracy: 0.7848\n",
      "Epoch 10/50\n",
      "7500/7500 [==============================] - 4s 586us/step - loss: 0.4336 - accuracy: 0.8130 - val_loss: 0.5352 - val_accuracy: 0.7153\n",
      "Epoch 11/50\n",
      "7500/7500 [==============================] - 4s 561us/step - loss: 0.4329 - accuracy: 0.8133 - val_loss: 0.4202 - val_accuracy: 0.8234\n",
      "Epoch 12/50\n",
      "7500/7500 [==============================] - 4s 584us/step - loss: 0.4303 - accuracy: 0.8150 - val_loss: 0.4318 - val_accuracy: 0.8187\n",
      "Epoch 13/50\n",
      "7500/7500 [==============================] - 5s 702us/step - loss: 0.4293 - accuracy: 0.8156 - val_loss: 0.4196 - val_accuracy: 0.8175\n",
      "Epoch 14/50\n",
      "7500/7500 [==============================] - 4s 539us/step - loss: 0.4283 - accuracy: 0.8162 - val_loss: 0.4460 - val_accuracy: 0.8120\n",
      "Epoch 15/50\n",
      "7500/7500 [==============================] - 4s 550us/step - loss: 0.4270 - accuracy: 0.8167 - val_loss: 0.4206 - val_accuracy: 0.8150\n",
      "Epoch 16/50\n",
      "7500/7500 [==============================] - 4s 540us/step - loss: 0.4262 - accuracy: 0.8177 - val_loss: 0.4228 - val_accuracy: 0.8117\n",
      "Epoch 17/50\n",
      "7500/7500 [==============================] - 5s 670us/step - loss: 0.4244 - accuracy: 0.8189 - val_loss: 0.4177 - val_accuracy: 0.8279\n",
      "Epoch 18/50\n",
      "7500/7500 [==============================] - 4s 583us/step - loss: 0.4240 - accuracy: 0.8193 - val_loss: 0.4295 - val_accuracy: 0.8062\n",
      "Epoch 19/50\n",
      "7500/7500 [==============================] - 6s 835us/step - loss: 0.4232 - accuracy: 0.8199 - val_loss: 0.4182 - val_accuracy: 0.8274\n",
      "Epoch 20/50\n",
      "7500/7500 [==============================] - 4s 557us/step - loss: 0.4233 - accuracy: 0.8192 - val_loss: 0.4140 - val_accuracy: 0.8245\n",
      "Epoch 21/50\n",
      "7500/7500 [==============================] - 5s 637us/step - loss: 0.4220 - accuracy: 0.8198 - val_loss: 0.4328 - val_accuracy: 0.8120\n",
      "Epoch 22/50\n",
      "7500/7500 [==============================] - 12s 2ms/step - loss: 0.4215 - accuracy: 0.8202 - val_loss: 0.4095 - val_accuracy: 0.8282\n",
      "Epoch 23/50\n",
      "7500/7500 [==============================] - 4s 592us/step - loss: 0.4201 - accuracy: 0.8212 - val_loss: 0.4131 - val_accuracy: 0.8220\n",
      "Epoch 24/50\n",
      "7500/7500 [==============================] - 4s 563us/step - loss: 0.4199 - accuracy: 0.8209 - val_loss: 0.4123 - val_accuracy: 0.8249\n",
      "Epoch 25/50\n",
      "7500/7500 [==============================] - 4s 559us/step - loss: 0.4195 - accuracy: 0.8210 - val_loss: 0.4561 - val_accuracy: 0.8062\n",
      "Epoch 26/50\n",
      "7500/7500 [==============================] - 4s 564us/step - loss: 0.4187 - accuracy: 0.8220 - val_loss: 0.4258 - val_accuracy: 0.8199\n",
      "Epoch 27/50\n",
      "7500/7500 [==============================] - 6s 852us/step - loss: 0.4188 - accuracy: 0.8208 - val_loss: 0.4090 - val_accuracy: 0.8284\n",
      "Epoch 28/50\n",
      "7500/7500 [==============================] - 10s 1ms/step - loss: 0.4174 - accuracy: 0.8221 - val_loss: 0.4123 - val_accuracy: 0.8257\n",
      "Epoch 29/50\n",
      "7500/7500 [==============================] - 7s 970us/step - loss: 0.4169 - accuracy: 0.8223 - val_loss: 0.4188 - val_accuracy: 0.8236\n",
      "Epoch 30/50\n",
      "7500/7500 [==============================] - 4s 590us/step - loss: 0.4170 - accuracy: 0.8224 - val_loss: 0.4300 - val_accuracy: 0.8051\n",
      "Epoch 31/50\n",
      "7500/7500 [==============================] - 4s 541us/step - loss: 0.4162 - accuracy: 0.8228 - val_loss: 0.4051 - val_accuracy: 0.8275\n",
      "Epoch 32/50\n",
      "7500/7500 [==============================] - 5s 619us/step - loss: 0.4148 - accuracy: 0.8235 - val_loss: 0.4060 - val_accuracy: 0.8298\n",
      "Epoch 33/50\n",
      "7500/7500 [==============================] - 4s 573us/step - loss: 0.4154 - accuracy: 0.8229 - val_loss: 0.4224 - val_accuracy: 0.8291\n",
      "Epoch 34/50\n",
      "7500/7500 [==============================] - 6s 818us/step - loss: 0.4154 - accuracy: 0.8228 - val_loss: 0.4039 - val_accuracy: 0.8281\n",
      "Epoch 35/50\n",
      "7500/7500 [==============================] - 5s 626us/step - loss: 0.4146 - accuracy: 0.8230 - val_loss: 0.4075 - val_accuracy: 0.8244\n",
      "Epoch 36/50\n",
      "7500/7500 [==============================] - 8s 1ms/step - loss: 0.4134 - accuracy: 0.8238 - val_loss: 0.4064 - val_accuracy: 0.8275\n",
      "Epoch 37/50\n",
      "7500/7500 [==============================] - 5s 702us/step - loss: 0.4144 - accuracy: 0.8231 - val_loss: 0.4192 - val_accuracy: 0.8195\n",
      "Epoch 38/50\n",
      "7500/7500 [==============================] - 4s 569us/step - loss: 0.4132 - accuracy: 0.8236 - val_loss: 0.4099 - val_accuracy: 0.8273\n",
      "Epoch 39/50\n",
      "7500/7500 [==============================] - 4s 579us/step - loss: 0.4135 - accuracy: 0.8232 - val_loss: 0.4041 - val_accuracy: 0.8279\n",
      "Epoch 40/50\n",
      "7500/7500 [==============================] - 4s 544us/step - loss: 0.4130 - accuracy: 0.8240 - val_loss: 0.4299 - val_accuracy: 0.8231\n",
      "Epoch 41/50\n",
      "7500/7500 [==============================] - 4s 547us/step - loss: 0.4125 - accuracy: 0.8239 - val_loss: 0.4262 - val_accuracy: 0.8090\n",
      "Epoch 42/50\n",
      "7500/7500 [==============================] - 4s 576us/step - loss: 0.4127 - accuracy: 0.8234 - val_loss: 0.4083 - val_accuracy: 0.8204\n",
      "Epoch 43/50\n",
      "7500/7500 [==============================] - 4s 552us/step - loss: 0.4119 - accuracy: 0.8242 - val_loss: 0.4197 - val_accuracy: 0.8220\n",
      "Epoch 44/50\n",
      "7500/7500 [==============================] - 4s 579us/step - loss: 0.4120 - accuracy: 0.8242 - val_loss: 0.4037 - val_accuracy: 0.8300\n",
      "Epoch 45/50\n",
      "7500/7500 [==============================] - 4s 559us/step - loss: 0.4119 - accuracy: 0.8242 - val_loss: 0.4115 - val_accuracy: 0.8266\n",
      "Epoch 46/50\n",
      "7500/7500 [==============================] - 4s 544us/step - loss: 0.4113 - accuracy: 0.8244 - val_loss: 0.4060 - val_accuracy: 0.8310\n",
      "Epoch 47/50\n",
      "7500/7500 [==============================] - 4s 551us/step - loss: 0.4106 - accuracy: 0.8242 - val_loss: 0.4040 - val_accuracy: 0.8299\n",
      "Epoch 48/50\n",
      "7500/7500 [==============================] - 4s 567us/step - loss: 0.4104 - accuracy: 0.8246 - val_loss: 0.4016 - val_accuracy: 0.8285\n",
      "Epoch 49/50\n",
      "7500/7500 [==============================] - 4s 565us/step - loss: 0.4116 - accuracy: 0.8236 - val_loss: 0.4134 - val_accuracy: 0.8304\n",
      "Epoch 50/50\n",
      "7500/7500 [==============================] - 4s 558us/step - loss: 0.4107 - accuracy: 0.8249 - val_loss: 0.4034 - val_accuracy: 0.8294\n"
     ]
    }
   ],
   "source": [
    "model_1.compile(SGD(lr = .003), \"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "run_hist_1 = model_1.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is 0.829\n",
      "roc-auc is 0.857\n"
     ]
    }
   ],
   "source": [
    "y_pred_class_nn_1 = model_1.predict_classes(X_test)\n",
    "y_pred_prob_nn_1 = model_1.predict(X_test)\n",
    "\n",
    "print('accuracy is {:.3f}'.format(accuracy_score(y_test,y_pred_class_nn_1)))\n",
    "print('roc-auc is {:.3f}'.format(roc_auc_score(y_test,y_pred_prob_nn_1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x16e069fac40>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(run_hist_1.history[\"loss\"],'r', marker='.', label=\"Train Loss\")\n",
    "ax.plot(run_hist_1.history[\"val_loss\"],'b', marker='.', label=\"Validation Loss\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change to two hidden layers with activation relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_12 (Dense)             (None, 12)                360       \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 12)                156       \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 1)                 13        \n",
      "=================================================================\n",
      "Total params: 529\n",
      "Trainable params: 529\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_1 = Sequential()\n",
    "model_1.add(Dense(12,input_shape = (29,),activation = 'relu'))\n",
    "model_1.add(Dense(12,activation='relu'))\n",
    "model_1.add(Dense(1,activation='sigmoid'))\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "7500/7500 [==============================] - 8s 1ms/step - loss: 0.5405 - accuracy: 0.7368 - val_loss: 0.4932 - val_accuracy: 0.7430\n",
      "Epoch 2/50\n",
      "7500/7500 [==============================] - 4s 539us/step - loss: 0.4799 - accuracy: 0.7762 - val_loss: 0.4357 - val_accuracy: 0.8103\n",
      "Epoch 3/50\n",
      "7500/7500 [==============================] - 4s 542us/step - loss: 0.4448 - accuracy: 0.8043 - val_loss: 0.4336 - val_accuracy: 0.8063\n",
      "Epoch 4/50\n",
      "7500/7500 [==============================] - 4s 541us/step - loss: 0.4291 - accuracy: 0.8130 - val_loss: 0.4193 - val_accuracy: 0.8226\n",
      "Epoch 5/50\n",
      "7500/7500 [==============================] - 4s 527us/step - loss: 0.4219 - accuracy: 0.8178 - val_loss: 0.4099 - val_accuracy: 0.8210\n",
      "Epoch 6/50\n",
      "7500/7500 [==============================] - 4s 537us/step - loss: 0.4172 - accuracy: 0.8202 - val_loss: 0.4090 - val_accuracy: 0.8271\n",
      "Epoch 7/50\n",
      "7500/7500 [==============================] - 4s 540us/step - loss: 0.4142 - accuracy: 0.8220 - val_loss: 0.4060 - val_accuracy: 0.8259\n",
      "Epoch 8/50\n",
      "7500/7500 [==============================] - 4s 549us/step - loss: 0.4124 - accuracy: 0.8224 - val_loss: 0.4058 - val_accuracy: 0.8221\n",
      "Epoch 9/50\n",
      "7500/7500 [==============================] - 4s 554us/step - loss: 0.4113 - accuracy: 0.8237 - val_loss: 0.4153 - val_accuracy: 0.8249\n",
      "Epoch 10/50\n",
      "7500/7500 [==============================] - 4s 565us/step - loss: 0.4084 - accuracy: 0.8245 - val_loss: 0.4027 - val_accuracy: 0.8286\n",
      "Epoch 11/50\n",
      "7500/7500 [==============================] - 4s 550us/step - loss: 0.4073 - accuracy: 0.8253 - val_loss: 0.4218 - val_accuracy: 0.8175\n",
      "Epoch 12/50\n",
      "7500/7500 [==============================] - 4s 542us/step - loss: 0.4061 - accuracy: 0.8255 - val_loss: 0.4197 - val_accuracy: 0.8196\n",
      "Epoch 13/50\n",
      "7500/7500 [==============================] - 4s 534us/step - loss: 0.4049 - accuracy: 0.8262 - val_loss: 0.4027 - val_accuracy: 0.8236\n",
      "Epoch 14/50\n",
      "7500/7500 [==============================] - 4s 528us/step - loss: 0.4036 - accuracy: 0.8264 - val_loss: 0.4741 - val_accuracy: 0.7896\n",
      "Epoch 15/50\n",
      "7500/7500 [==============================] - 4s 541us/step - loss: 0.4028 - accuracy: 0.8268 - val_loss: 0.3955 - val_accuracy: 0.8312\n",
      "Epoch 16/50\n",
      "7500/7500 [==============================] - 4s 533us/step - loss: 0.4023 - accuracy: 0.8272 - val_loss: 0.4400 - val_accuracy: 0.8086\n",
      "Epoch 17/50\n",
      "7500/7500 [==============================] - 4s 535us/step - loss: 0.4004 - accuracy: 0.8277 - val_loss: 0.3998 - val_accuracy: 0.8281\n",
      "Epoch 18/50\n",
      "7500/7500 [==============================] - 4s 535us/step - loss: 0.4000 - accuracy: 0.8279 - val_loss: 0.3937 - val_accuracy: 0.8315\n",
      "Epoch 19/50\n",
      "7500/7500 [==============================] - 4s 536us/step - loss: 0.3990 - accuracy: 0.8286 - val_loss: 0.4046 - val_accuracy: 0.8281\n",
      "Epoch 20/50\n",
      "7500/7500 [==============================] - 4s 539us/step - loss: 0.3990 - accuracy: 0.8288 - val_loss: 0.3927 - val_accuracy: 0.8310\n",
      "Epoch 21/50\n",
      "7500/7500 [==============================] - 4s 537us/step - loss: 0.3981 - accuracy: 0.8285 - val_loss: 0.3934 - val_accuracy: 0.8290\n",
      "Epoch 22/50\n",
      "7500/7500 [==============================] - 4s 536us/step - loss: 0.3976 - accuracy: 0.8292 - val_loss: 0.3918 - val_accuracy: 0.8315\n",
      "Epoch 23/50\n",
      "7500/7500 [==============================] - 4s 540us/step - loss: 0.3971 - accuracy: 0.8289 - val_loss: 0.3995 - val_accuracy: 0.8253\n",
      "Epoch 24/50\n",
      "7500/7500 [==============================] - 4s 544us/step - loss: 0.3963 - accuracy: 0.8300 - val_loss: 0.4189 - val_accuracy: 0.8199\n",
      "Epoch 25/50\n",
      "7500/7500 [==============================] - 4s 586us/step - loss: 0.3964 - accuracy: 0.8293 - val_loss: 0.4049 - val_accuracy: 0.8281\n",
      "Epoch 26/50\n",
      "7500/7500 [==============================] - 4s 547us/step - loss: 0.3957 - accuracy: 0.8300 - val_loss: 0.3937 - val_accuracy: 0.8280\n",
      "Epoch 27/50\n",
      "7500/7500 [==============================] - 4s 534us/step - loss: 0.3954 - accuracy: 0.8300 - val_loss: 0.3903 - val_accuracy: 0.8324\n",
      "Epoch 28/50\n",
      "7500/7500 [==============================] - 4s 534us/step - loss: 0.3947 - accuracy: 0.8302 - val_loss: 0.3899 - val_accuracy: 0.8322\n",
      "Epoch 29/50\n",
      "7500/7500 [==============================] - 4s 554us/step - loss: 0.3946 - accuracy: 0.8297 - val_loss: 0.3946 - val_accuracy: 0.8272\n",
      "Epoch 30/50\n",
      "7500/7500 [==============================] - 4s 542us/step - loss: 0.3944 - accuracy: 0.8309 - val_loss: 0.4153 - val_accuracy: 0.8172\n",
      "Epoch 31/50\n",
      "7500/7500 [==============================] - 4s 537us/step - loss: 0.3940 - accuracy: 0.8304 - val_loss: 0.3901 - val_accuracy: 0.8327\n",
      "Epoch 32/50\n",
      "7500/7500 [==============================] - 4s 563us/step - loss: 0.3937 - accuracy: 0.8306 - val_loss: 0.3902 - val_accuracy: 0.8320\n",
      "Epoch 33/50\n",
      "7500/7500 [==============================] - 4s 531us/step - loss: 0.3933 - accuracy: 0.8311 - val_loss: 0.4001 - val_accuracy: 0.8253\n",
      "Epoch 34/50\n",
      "7500/7500 [==============================] - 4s 532us/step - loss: 0.3933 - accuracy: 0.8311 - val_loss: 0.3901 - val_accuracy: 0.8338\n",
      "Epoch 35/50\n",
      "7500/7500 [==============================] - 4s 533us/step - loss: 0.3927 - accuracy: 0.8314 - val_loss: 0.3972 - val_accuracy: 0.8260\n",
      "Epoch 36/50\n",
      "7500/7500 [==============================] - 4s 532us/step - loss: 0.3924 - accuracy: 0.8313 - val_loss: 0.4083 - val_accuracy: 0.8272\n",
      "Epoch 37/50\n",
      "7500/7500 [==============================] - 4s 549us/step - loss: 0.3928 - accuracy: 0.8313 - val_loss: 0.3985 - val_accuracy: 0.8258\n",
      "Epoch 38/50\n",
      "7500/7500 [==============================] - 4s 539us/step - loss: 0.3920 - accuracy: 0.8311 - val_loss: 0.3878 - val_accuracy: 0.8322\n",
      "Epoch 39/50\n",
      "7500/7500 [==============================] - 4s 564us/step - loss: 0.3920 - accuracy: 0.8314 - val_loss: 0.4082 - val_accuracy: 0.8218\n",
      "Epoch 40/50\n",
      "7500/7500 [==============================] - 4s 560us/step - loss: 0.3920 - accuracy: 0.8312 - val_loss: 0.4085 - val_accuracy: 0.8266\n",
      "Epoch 41/50\n",
      "7500/7500 [==============================] - 4s 572us/step - loss: 0.3916 - accuracy: 0.8320 - val_loss: 0.3874 - val_accuracy: 0.8342\n",
      "Epoch 42/50\n",
      "7500/7500 [==============================] - 4s 538us/step - loss: 0.3915 - accuracy: 0.8314 - val_loss: 0.3943 - val_accuracy: 0.8274\n",
      "Epoch 43/50\n",
      "7500/7500 [==============================] - 4s 534us/step - loss: 0.3909 - accuracy: 0.8321 - val_loss: 0.3931 - val_accuracy: 0.8286\n",
      "Epoch 44/50\n",
      "7500/7500 [==============================] - 4s 539us/step - loss: 0.3908 - accuracy: 0.8319 - val_loss: 0.3865 - val_accuracy: 0.8325\n",
      "Epoch 45/50\n",
      "7500/7500 [==============================] - 4s 540us/step - loss: 0.3908 - accuracy: 0.8316 - val_loss: 0.3876 - val_accuracy: 0.8344\n",
      "Epoch 46/50\n",
      "7500/7500 [==============================] - 4s 539us/step - loss: 0.3904 - accuracy: 0.8319 - val_loss: 0.3856 - val_accuracy: 0.8345\n",
      "Epoch 47/50\n",
      "7500/7500 [==============================] - 4s 536us/step - loss: 0.3905 - accuracy: 0.8319 - val_loss: 0.3861 - val_accuracy: 0.8345\n",
      "Epoch 48/50\n",
      "7500/7500 [==============================] - 4s 539us/step - loss: 0.3900 - accuracy: 0.8319 - val_loss: 0.3865 - val_accuracy: 0.8344\n",
      "Epoch 49/50\n",
      "7500/7500 [==============================] - 4s 539us/step - loss: 0.3900 - accuracy: 0.8320 - val_loss: 0.3882 - val_accuracy: 0.8335\n",
      "Epoch 50/50\n",
      "7500/7500 [==============================] - 4s 562us/step - loss: 0.3898 - accuracy: 0.8323 - val_loss: 0.3880 - val_accuracy: 0.8331\n"
     ]
    }
   ],
   "source": [
    "model_1.compile(SGD(lr = .003), \"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "run_hist_1 = model_1.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is 0.833\n",
      "roc-auc is 0.863\n"
     ]
    }
   ],
   "source": [
    "y_pred_class_nn_1 = model_1.predict_classes(X_test)\n",
    "y_pred_prob_nn_1 = model_1.predict(X_test)\n",
    "\n",
    "print('accuracy is {:.3f}'.format(accuracy_score(y_test,y_pred_class_nn_1)))\n",
    "print('roc-auc is {:.3f}'.format(roc_auc_score(y_test,y_pred_prob_nn_1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x16e08a2aee0>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAABAqUlEQVR4nO2deXhV1fW/35UJkEmEUBSUyaEqswGMIAaxikjBASvghLTywxbn2SpSKVhba6lTqQPaqoXiRFFRFGQ0+SqDOFBFEVECFplkkinJ+v2x70luLnc4NwNJTtb7PPe595yzzzl735t89jprr722qCqGYRhGcEmp6goYhmEYlYsJvWEYRsAxoTcMwwg4JvSGYRgBx4TeMAwj4KRVdQWi0axZM23Tpk1VV8MwDKPGsGzZss2qmhntWLUU+jZt2rB06dKqroZhGEaNQUS+iXXMXDeGYRgBx4TeMAwj4JjQG4ZhBJxq6aM3DOPQcODAAfLz89m7d29VV8XwSd26dWnVqhXp6em+zzGhN4xaTH5+Pg0bNqRNmzaISFVXx0iAqrJlyxby8/Np27at7/PMdWMYtZi9e/fStGlTE/kagojQtGnTpJ/AfAm9iPQXkVUislpE7ohyPEdEtovIitBrbMTxVBH5UEReT6p2yZKXB/ff794Nw/CFiXzNoiy/V0LXjYikAo8BPwPygSUiMlNV/xtRdJGqDoxxmeuBz4BGSdfQL7m5cMYZUFQEderA3LmQnV1ptzMMw6gp+LHoewCrVXWNqu4HpgGD/d5ARFoB5wFPla2KPlmwAAoKnNDv3w/z51fq7QzDKD9btmyhS5cudOnShRYtWtCyZcvi7f3798c9d+nSpVx33XVJ3a9NmzZs3ry5PFWukfgZjG0JrAvbzgd6RimXLSIfARuAW1R1ZWj/JOA2oGG8m4jIKGAUwDHHHOOjWhHk5ID3SJOR4bYNw6jWNG3alBUrVgAwbtw4GjRowC233FJ8vKCggLS06DKVlZVFVlbWoahmjcePRR/NIRS5LNVyoLWqdgYeAWYAiMhA4HtVXZboJqr6hKpmqWpWZmbUdA3xyc6GE0+E9u3NbWMYlUklj4WNGDGCm266ib59+3L77bfzwQcfcNppp9G1a1dOO+00Vq1aBcD8+fMZONB5i8eNG8fIkSPJycmhXbt2PPzww77v980339CvXz86depEv379+PbbbwF48cUX6dChA507d6ZPnz4ArFy5kh49etClSxc6derEl19+WcGtrxz8WPT5wNFh261wVnsxqroj7PMsEXlcRJoBvYBBIjIAqAs0EpHnVfWy8lc9Cm3bwoYNJvKGURZuuAFC1nVMtm+Hjz92LtKUFOjUCRo3jl2+SxeYNCnpqnzxxRfMmTOH1NRUduzYwcKFC0lLS2POnDncddddvPzyywed8/nnnzNv3jx27tzJCSecwDXXXOMr1nzMmDFcccUVXHnllUyZMoXrrruOGTNmcN999zF79mxatmzJDz/8AMDkyZO5/vrrufTSS9m/fz+FhYVJt60q8GPRLwGOE5G2IpIBDAVmhhcQkRYSGgoWkR6h625R1TtVtZWqtgmd926liTxAZiZs2lRplzeMWs/27U7kwb1v314pt7n44otJTU0N3XI7F198MR06dODGG29k5cqVUc8577zzqFOnDs2aNaN58+Zs3LjR173y8vIYPnw4AJdffjmLFy8GoFevXowYMYInn3yyWNCzs7OZOHEiDzzwAN988w316tUrb1MPCQktelUtEJExwGwgFZiiqitFZHTo+GRgCHCNiBQAe4ChWhWrjjdrBps3g2qJv94wDH/4sbzz8qBfPxfwkJEBL7xQKU/Q9evXL/58zz330LdvX1599VXWrl1LTozxtzp16hR/Tk1NpaCgoEz39sIXJ0+ezPvvv88bb7xBly5dWLFiBcOHD6dnz5688cYbnHPOOTz11FOceeaZZbrPocTXzFhVnQXMitg3Oezzo8CjCa4xH5ifdA2TITMT9u6F3buhQYNKvZVh1Eqys90Y2Pz5LuDhELhJt2/fTsuWLQF49tlnK/z6p512GtOmTePyyy/nhRdeoHfv3gB89dVX9OzZk549e/Laa6+xbt06tm/fTrt27bjuuutYs2YNH3/8cXCEvsbgDeJu3mxCbxiVRXb2IR0Hu+2227jyyit56KGHKkRUO3XqREqK81r/4he/4OGHH2bkyJH86U9/IjMzk2eeeQaAW2+9lS+//BJVpV+/fnTu3Jk//OEPPP/886Snp9OiRQvGjh0b71bVBqkKD0sisrKytEwLj7z2GgwaBB98AN27V3zFDCNgfPbZZ5x44olVXQ0jSaL9biKyTFWjxpsGK9eNZ9HbgKxhGEYxwRL6Zs3cey2c+WYYhhGLYAm9WfSGYRgHESyhb9QI0tPNojcMwwgjWEIv4tw3ZtEbhmEUEyyhh5JJU4ZhGAYQRKG3NAiGUWPIyclh9uzZpfZNmjSJX//613HP8cKvBwwYUJyHJpxx48bx4IMPxr33jBkz+O9/S5bVGDt2LHPmzEmi9tEJT7ZWXQie0JtFbxg1hmHDhjFt2rRS+6ZNm8awYcN8nT9r1iwOP/zwMt07Uujvu+8+zjrrrDJdq7oTPKE3i94wKpWKzFI8ZMgQXn/9dfbt2wfA2rVr2bBhA7179+aaa64hKyuLk08+mXvvvTfq+eELiUyYMIETTjiBs846qziVMcCTTz5J9+7d6dy5MxdddBE//vgjubm5zJw5k1tvvZUuXbrw1VdfMWLECF566SUA5s6dS9euXenYsSMjR44srl+bNm2499576datGx07duTzzz/33dapU6fSsWNHOnTowO233w5AYWEhI0aMoEOHDnTs2JG//OUvADz88MOcdNJJdOrUiaFDhyb5rR5MsFIggBP6bdvgwAEXgWMYhi+qIktx06ZN6dGjB2+99RaDBw9m2rRpXHLJJYgIEyZM4IgjjqCwsJB+/frx8ccf06lTp6jXWbZsGdOmTePDDz+koKCAbt26ccoppwBw4YUXcvXVVwNw99138/TTT3PttdcyaNAgBg4cyJAhQ0pda+/evYwYMYK5c+dy/PHHc8UVV/C3v/2NG264AYBmzZqxfPlyHn/8cR588EGeeirx4nkbNmzg9ttvZ9myZTRp0oSzzz6bGTNmcPTRR7N+/Xo+/fRTgGI31B/+8Ae+/vpr6tSpE9U1lSzBs+i9SVNbt1ZtPQwjgFRGluJw902422b69Ol069aNrl27snLlylJulkgWLVrEBRdcwGGHHUajRo0YNGhQ8bFPP/2U008/nY4dO/LCCy/ETHPssWrVKtq2bcvxxx8PwJVXXsnChQuLj1944YUAnHLKKaxdu9ZXG5csWUJOTg6ZmZmkpaVx6aWXsnDhQtq1a8eaNWu49tpreeutt2jUyC2r3alTJy699FKef/75mCtsJUMwLXpw7puf/KRq62IYNYiqylJ8/vnnc9NNN7F8+XL27NlDt27d+Prrr3nwwQdZsmQJTZo0YcSIEezduzfudSRGavIRI0YwY8YMOnfuzLPPPsv8BOtJJ8r/5aVDTiYVcqxrNmnShI8++ojZs2fz2GOPMX36dKZMmcIbb7zBwoULmTlzJuPHj2flypXlEvzgWvQ2IGsYFY6XpXj8+IpbsbNBgwbk5OQwcuTIYmt+x44d1K9fn8aNG7Nx40befPPNuNfo06cPr776Knv27GHnzp289tprxcd27tzJkUceyYEDB3jhhReK9zds2JCdO3cedK2f/vSnrF27ltWrVwPw3HPPccYZZ5SrjT179mTBggVs3ryZwsJCpk6dyhlnnMHmzZspKirioosuYvz48SxfvpyioiLWrVtH3759+eMf/8gPP/zArl27ynX/YFv0hmFUOJWRpXjYsGFceOGFxS6czp0707VrV04++WTatWtHr1694p7frVs3LrnkErp06ULr1q05/fTTi4+NHz+enj170rp1azp27Fgs7kOHDuXqq6/m4YcfLh6EBahbty7PPPMMF198MQUFBXTv3p3Ro0cn1Z65c+fSqlWr4u0XX3yR+++/n759+6KqDBgwgMGDB/PRRx9x1VVXURTyh91///0UFhZy2WWXsX37dlSVG2+8scyRRR6+0hSLSH/gr7gVpp5S1T9EHM8B/gN8Hdr1iqreJyJHA/8EWgBFwBOq+tdE9ytzmmKA776Do46Cxx+Ha64p2zUMo5ZgaYprJsmmKU5o0YtIKvAY8DPcQuFLRGSmqkaOjCxS1chZAgXAzaq6XEQaAstE5J0o51YcnuvGLHrDMAzAn4++B7BaVdeo6n5gGjDYz8VV9TtVXR76vBP4DGhZ1sr6Ij3dxXuZj94wDAPwJ/QtgXVh2/lEF+tsEflIRN4UkZMjD4pIG6Ar8H60m4jIKBFZKiJLN5XXGrdJU4bhm+q4ypwRm7L8Xn6EPlrMUuSdlgOtVbUz8Agwo9QFRBoALwM3qOqOaDdR1SdUNUtVszK9AdWykplpFr1h+KBu3bps2bLFxL6GoKps2bKFunXrJnWen6ibfODosO1WwIaIm+8I+zxLRB4XkWaqullE0nEi/4KqvpJU7cpKs2bw7beH5FaGUZNp1aoV+fn5lPsp2jhk1K1bt1REjx/8CP0S4DgRaQusB4YCw8MLiEgLYKOqqoj0wD0pbBE3g+Fp4DNVfSipmpWHzExYtuyQ3c4wairp6em0bdu2qqthVDIJhV5VC0RkDDAbF145RVVXisjo0PHJwBDgGhEpAPYAQ0Oi3xu4HPhERFaELnmXqs6qhLaU4GWwVHWLkRiGYdRifE2YCgnzrIh9k8M+Pwo8GuW8xUT38VcumZlujvbOnW55QcMwjFpM8FIggKVBMAzDCCOYQm9pEAzDMIoJptCbRW8YhlFMMIXeLHrDMIxigi30ZtEbhmEEVOgbNHCrIphFbxiGEVChF7F8N4ZhGCGCKfRQMmnKMAyjlhNcoTeL3jAMAwiy0JtFbxiGAQRZ6M2iNwzDAIIs9M2awfbtcOBAVdfEMAyjSgmu0FssvWEYBhBkobc0CIZhGECQhd7SIBiGYQC1QejNojcMo5YTXKH3XDdm0RuGUcvxJfQi0l9EVonIahG5I8rxHBHZLiIrQq+xfs+tNJo2de8m9IZh1HISLiUoIqnAY8DPgHxgiYjMVNX/RhRdpKoDy3huxZOWBk2amOvGMIxajx+LvgewWlXXqOp+YBow2Of1y3Nu+bFJU4ZhGL6EviWwLmw7P7QvkmwR+UhE3hSRk5M8FxEZJSJLRWTppooSZ0uDYBiG4UvoJco+jdheDrRW1c7AI8CMJM51O1WfUNUsVc3K9CJmyotZ9IZhGL6EPh84Omy7FbAhvICq7lDVXaHPs4B0EWnm59xKxSx6wzAMX0K/BDhORNqKSAYwFJgZXkBEWoiIhD73CF13i59zK5XMTCf0GvUhwjAMo1aQMOpGVQtEZAwwG0gFpqjqShEZHTo+GRgCXCMiBcAeYKiqKhD13Epqy8FkZrqkZjt2QOPGh+y2hmEY1YmEQg/F7phZEfsmh31+FHjU77mHjPBJUyb0hmHUUoI7MxYs341hGAYBEnpVeO45uO02yMsL7bQMloZhGP5cNzWBvDy44goQgUcfhblzIftIs+gNwzACY9EvWODeVWH/fpg/H7PoDcMwCJDQ5+Q4ax4gI8NtU78+1K1rFr1hGLWawAh9djb07AktW4bcNtk45bdJU4Zh1HICI/QAJ5wAqakhkfewNAiGYdRyAiX0nqaXmgjrzY41DMOopQRK6Js3hz17YPfusJ3NmplFbxhGrSZQQu/Nj/r++4idZtEbhlGLCZTQN2/u3ksZ8M2auVw3+/ZVSZ0MwzCqmkAK/UEWPZhVbxhGrSVQQh/VdWOTpgzDqOUEUuhLuW4ssZlhGLWcQAn9YYdBgwZm0RuGYYQTKKEHZ8BH9dGbRW8YRi3Fl9CLSH8RWSUiq0XkjjjluotIoYgMCdt3o4isFJFPRWSqiNStiIrHonnzCE0/4giXCsEsesMwaikJhV5EUoHHgHOBk4BhInJSjHIP4JYN9Pa1BK4DslS1A245waEVU/XoNG8eYdGnpjqxN4veMIxaih+LvgewWlXXqOp+YBowOEq5a4GXge8j9qcB9UQkDTgM2FCO+ibkINcNOMf9okVhK5IYhmHUHvwIfUtgXdh2fmhfMSHL/QJgcvh+VV0PPAh8C3wHbFfVt6PdRERGichSEVm6qRzWt+e6Kc53k5cH69bBp59Cv34m9oZh1Dr8CL1E2acR25OA21W1sNSJIk1w1n9b4CigvohcFu0mqvqEqmapalamN4BaBpo3hwMHYPv20I7586GoyH0uXpHEMAyj9uBnKcF84Oiw7VYc7H7JAqaJW/mjGTBARAqAdOBrVd0EICKvAKcBz5ez3jEJnzR1+OG4FUjS0qCgIGxFEsMwjNqDH4t+CXCciLQVkQzcYOrM8AKq2lZV26hqG+Al4NeqOgPnsjlVRA4T1wv0Az6ryAZEclC+m+xs+O1v3eenn45IVm8YhhF8Egq9qhYAY3DRNJ8B01V1pYiMFpHRCc59Hyf8y4FPQvd7oty1jkPUNAj9+7v3Bg0q89aGYRjVEj+uG1R1FjArYt/kGGVHRGzfC9xbxvolTdQMlsce696/+upQVcMwDKPaEMiZsRBh0TdtCo0awerVVVKnmkReHtx/vwUnGUaQ8GXR1yQyMqBx4wihF3FWvVn0cZk9GwYMcF9XRkbYIuuGYdRoAmfRQ5Q0COCE3iz6uLz0kotELSy0SFTDCBKBFfqDZse2bw9r17owSyMq7dq595QUi0Q1jCARSKGPmgbh2GOdyH/7bZXUqSbgjW+cc465bQwjSARS6KO6btq3d+/mp4/Jtm3uvVs3E3nDCBKBFnov8wFQEmJpfvqYbN3q3n/4oUqrYRhGBRNIoc/MdCLvCRcARx4JdeuaRR8Hz6I3oTeMYBFIoY86aSolxblvzKKPiQm9YQSTQAp91ElT4ITeLPqYeE9AxZk/DcMIBIEUes+ijxp589VXEc57w8MsesMIJoEW+qiRN3v2wHffHfI61QRsMNYwgkkghb5pU/ce1aIHc9/EwCx6wwgmgRT6tDQn9jGF3gZkD6Kw0Al8airs2mUTiA0jSARS6CHGpKljjnG9gFn0B+ENwB59dOltwzBqPoEV+qhpENLSoE0bs+ij4Llt2rZ17yb0hhEcAiv0URObgYVYxsAbiPUSm5mf3jCCgy+hF5H+IrJKRFaLyB1xynUXkUIRGRK273AReUlEPheRz0TkkGRRieq6gZJ0xaqHoho1hkiL3oTeMIJDQqEXkVTgMeBc4CRgmIicFKPcA7i1ZcP5K/CWqv4U6EwlLw7ukZkJW7ZEGVRs3975JUrlRzDMojeM4OLHou8BrFbVNaq6H5gGDI5S7lrgZaDYYSIijYA+wNMAqrpfVX8ob6X94MXSb9kSccAib6JiFr1hBBc/Qt8SWBe2nR/aV4yItAQuACIXDG8HbAKeEZEPReQpEakf7SYiMkpElorI0k1RfS7JEXN2rKUrjooNxhpGcPEj9BJlX6SDexJwu6oWRuxPA7oBf1PVrsBuIKqPX1WfUNUsVc3K9JLVlIOY+W7atXOLoppFX4qtW6FePfe9iZhFbxhBws/i4PnA0WHbrYANEWWygGkiAtAMGCAiBcD/Afmq+n6o3EvEEPqKJmYahLp1oVUrs+gj2LYNmjRxST4bNTKhN4wg4UfolwDHiUhbYD0wFBgeXkBV23qfReRZ4HVVnRHaXiciJ6jqKqAf8N+KqXp8Ylr0YOmKo7B1KxxxhPt8+OEm9IYRJBIKvaoWiMgYXDRNKjBFVVeKyOjQ8Ui/fCTXAi+ISAawBriqnHX2xRFHOOs0qtAfeyzMnHkoqlFj8Cx6MKE3jKDhx6JHVWcBsyL2RRV4VR0Rsb0C59o5pKSkOKs+6rhu+/auB9i5Exo2PNRVq5Zs3VoSWtm4sQm9YQSJwM6MhRhpEMCyWEYh0qK3qBvDCA6BFvq4aRDAhD4Mc90YRnAJvNDHdN2ADciG2L8fdu+2wVjDCCqBFvqYrptGjdxBs+iBkslS4Rb9jh224qJhBIVAC33z5s7XvG9flINecjOjOM9NuEWv6sTeMIyaT+CFHmDz5igHLV1xMZEWfePG7t0GZA0jGARa6ONOmjr2WFi3Loa5X7vwhD7cogfz0xtGUAi00MdMbAZO6FXh668PaZ2qI57rJtxHDyb0hhEUAi30nkVvkTfxiTYYCyb0hhEUAi30CS16MKGnxKL3BN6E3jCCRaCFvnFjSE+PYdE3berCLG1Alm3b3FeRFkqI4Q3GmtAbRjAItNCLxImlF4EWLWD2bMjLO+R1q05s21YyEAsWdWMYQSPQQg9x0iDk5Tlr/ssvoV+/Wi32W7eW+OfBWfYNGphFbxhBoVYIfVTXzfz5JVM/9+9327WUSIseLA2CYQSJwAt9TNdNTg5kZLjPqaluu5YSadGDCb1hBInAC31M1012Nsyd63wUvXu77VpKeOZKDxN6wwgOvoReRPqLyCoRWS0iMdd8FZHuIlIoIkMi9qeKyIci8np5K5wszZu7zIw//hjlYK9eMHw4fPBBrZ0hq1p6GUEPW3zEMIJDQqEXkVTgMeBc4CRgmIicFKPcA7glByO5HvisfFUtG3EnTQEMHgy7dsG8eYesTtWJH3+EAweiW/R+om7y8uD++2v1WLZhVHv8WPQ9gNWqukZV9wPTgMFRyl0LvAyUcpSISCvgPOCpcta1TMSdNAVw5plQvz785z+HrE7Vicg8Nx5+XDd5ee7ru+eeWh+4ZBjVGj9C3xJYF7adH9pXjIi0BC4Aoq0jOwm4DaiS7Oae0Me06OvWhf793WLhtTABe2SeGw9P6FVjnzt/PuzdC4WFtT5wyTCqNX6EXqLsi/z3nwTcrqqFpU4UGQh8r6rLEt5EZJSILBWRpZtiqnLyxM1g6TF4MGzYAMsSVjNwxLPoi4qcVysW4YFKGRm1OnDJMKo1foQ+Hzg6bLsVsCGiTBYwTUTWAkOAx0XkfKAXMCi0fxpwpog8H+0mqvqEqmapalamp84VQELXDcB557kQyxkzKuy+NYVYFr2fNAinnOLe09JgzpxaHbhkGNUaP0K/BDhORNqKSAYwFJgZXkBV26pqG1VtA7wE/FpVZ6jqnaraKrR/KPCuql5WsU2IT/36zjsT9yHhiCPg9NNrpZ8+MnOlh5fYLN6A7IZQd19QUJIM1DCM6kdCoVfVAmAMLprmM2C6qq4UkdEiMrqyK1heRJx1OndugsHCwYNh5cpal+QsnusG4lv0+fkln7/5piJrZRhGReIrjl5VZ6nq8araXlUnhPZNVtWDBl9VdYSqvhRl/3xVHVj+KidHXp5z23z4oYsQiSn2g0OBRLXMqt+61XmtGjYsvd+E3jCCQ+BnxoZHguzdC5df7rS8sDCiYNu20KlTrRN6b1asRAy5m9AbRnAIvNDn5DgffWqqy02/Ywecf75bd2TMGBg7NszKHzwYFi+OsZp4MImW5wb8C339+i6XvQm9YVRfAi/0Xkqb8eNhwQI3gPjii07IHnvM7S+e7DN4sIspfP2QZ2qoMqJlrgR/UTf5+XD00dC6tQm9YVRnAi/04MT+zjvde1oaDBkCv/gFpIRav29fyMXTrRu0alWr3DfREpqBi4uvVy9+1M369e7rMqE3jOpNrRD6aOTkQJ067rNIaLKPCAwaBG+/DXv2VGHtDh2xXDeQOA1Cfn6J0H/7bWXUzjCMiqDWCr3n0unWzQl+166hA4MHu0xfc+ZUaf0OFbFcNxBf6AsK4LvvSoT+hx/c+IdhGNWPWiv04MR+wgSn67O9nJs5OXDYYXDffYHP0lVUFNt1A/GFfuNGF7nkCT2Y+8Ywqiu1WujBDcQ2bQrTp4d2LFvmnPZLlwY+JeOOHS5pWSyLPl5Oei+0smVLE3rDqO7UeqFPT4cLL3TJK/fswY3Keikbi0dpg0msPDce8XLSe0LfqhUcc4z7bEJvGNWTWi/0AJdc4rI0zppF6VHaoiI4+eSqrFqlEiv9gUc810240P/kJy5Kx4TeMKonJvTAGWe4LJf//jclo7Q33eTUa/Lk+EnZazCxEpp5xMtJn5/v+sOmTV2Y6jHHmNAbRnXFhJ6S2PrXXw/lX8/Ohj//Gf70J3jzTZgypaqrWCn4cd0cOBA90tQLrfRSJ1gsvWFUX0zoQ1xyiRO0UpNix4xx5v6NNwYyUNyP6waiu288ofcwoTeM6osJfYjeveHII0PuG4+UFGfNFxXBL38ZOBdOIos+XhqEaEL/v/+5xHGGYVQvTOhDpKTAxRc7T02piT/t2jkXzpw58Pe/V1n9KoNt21zCt3r1oh+PtfhIUVFJ+gMPL8Ry3ToMw6hmmNCHccklLqLyoFQ3o0fDWWfBLbfA119XSd0qg3iTpSC262bTJue7jyb05r4xjOqHCX0Yp57qsjGWct+AG3F86iln9l94IUycWK0mUuXlwf33J1+lrVtj++chttCvX+/eowl9AIcyDKPG40voRaS/iKwSkdUickecct1FpFBEhoS2jxaReSLymYisFJHrK6rilUFKistq+fbbJQOVxbRuDb/5DaxYAXffXW1mzeblueVu77rLTQFYuND/uWW16MNj6D1atXLfn1n0hlH9SCj0IpIKPAacC5wEDBORk2KUewC3tqxHAXCzqp4InAr8Jtq51YlLLnFuiVdfjXKwYUNn3au6Ucd58w55/SJ5882S1bL274fzzoNx49zAaCJLP17mSkhO6NPT4aijTOgNozrix6LvAaxW1TWquh+YBgyOUu5a4GXge2+Hqn6nqstDn3fiFhdvWe5aVyJZWU6wogpk375u9NIT+/ffdyOTVUj9+u49JcVNYOrYEX73OyfCp58e/+EjXuZKcE3NyIgu9GlpbpJZODU5xLKs7i/DqAn4EfqWQHgsRT4RYi0iLYELgIMWCw8r0wboCrwf4/goEVkqIks3bdrko1qVw//9n1tMfPXqKIuJe7Nmf/97uOIKlyBnxAiXs7eK2LDBifG4ce4BIzcXvvgCevZ0ln5RkbP0o6XsSWTRQ/R8N/n5LplZSsRfT00V+rw891vfc0+18cgZRoXiR+glyr7IgPJJwO2qGrnktruASAOctX+DqkbNWq6qT6hqlqpmZWZm+qhW5TB/fomRHjWnWXa2c4j/4x9uHcLnnoPhw52/pwqYNw/69HEilZ3t9h13HDz4oOsAwFnfOTmlzztwwM0CjmfRQ/R8N5Ex9B6tW7vwyoMWXq/mzJjhPHGFhbE7RcOoyaT5KJMPHB223QrYEFEmC5gmbj58M2CAiBSo6gwRSceJ/Auq+koF1LlS8XKa7d3rvDNxc5rdfbcLQr/lFrcKx9lnuzBMT3ErmU2b4JNPYOjQg49lZ8Mrr8DAge7hI7JKnnj7seijCX3xQi1htG5dekGSmoLXIXqfIztFw6jp+LHolwDHiUhbEckAhgIzwwuoaltVbaOqbYCXgF+HRF6Ap4HPVPWhCq57peB5Z26+GVJT3WBnXG6+2SVAW7wYxo51fvxD9OzvWZ5nnhn9+HnnuTGHTz89+Jg3KzZZi141tkVfU9MVe4PL4CZCH6J+2jAOGQmFXlULgDG4aJrPgOmqulJERovI6ASn9wIuB84UkRWh14By17qSyc52k2FHjXLh82vWJDihWbPSK43feGP8xVbjkMyg4Lx50KABnHJK7DIDBrgxY0/YPRJlrvSIXHxk2zaXEyiW6wZqntDPm+d+cxH4/POqrk0JNkBsVBiqWu1ep5xyilYH1q9XrVtX9fLLExTMzVWtV081NVU1LU1VRLV5c9XnnlN97z3ViRNdGR+XqVtXFdx7olN++lPVc8+NXyYvz11v6tTS+994w+3Py4t//tVXq7ZoUbL90UfuvOnTDy67a5c7NnFi/GtWJ77+2tX5kUdUc3JUjz9etaioqmvlfvuMDNWUFPen5ePPx6jlAEs1hqbazNg4HHWUS2D5/POwcmWcgp6/Z/x4N2Np2TJo0wYuvzxxjGMY8+eXJAVLtLjVhg3O+ozltvHo3t3ljI90QSXKXOkR6bqJFkPvUb++u1dNsui97zgnB4YNcxFLH35YlTVyzJvnBobjRU0Zhl9M6BNw++3OPTJ2bIKC2dlw553uvWtXF+c4eLD7Ty0qcv6O+++HLVtiXqJ9+9LbffrEvp33j9+3b/xqpaa6MeK33iod8u/XdXP44a7z8TqgeEIPNS/Ect4853k7+WS46CI38Wvq1KqulUvF4WEDxEZ5MaFPQLNmbqz1lVfceuG+SU11vYQ3wUoEXnvN5UK+6CL44x9dPH6Ylf/66+6fesAAN+i5f3/sy8+b50S4S5fEVTn3XDc3INxSTZSi2CMyg2V+vhuOaNEievnWrWtOvhtV12Hm5Lifp2lTOOccmDatyufBsXNnyeenn7YBYqN8mND74KabnIvj7ruTPDE7G959FyZMgPfec0o7ZoxT6dtvd8HvvXvDr37F508t5oUXlOsvXs9LPf9E4wYFPP107Eu/+65bEyU1NXE1zjnHvYe7b7Ztcxkd0hIE2EamQVi/3ol8enr08p5FXxNS93/9teuUwq3lYcNcZ7Z4cZVVC3D3T/TbGIZfTOh90KiR0+XZs2HRoiRPDnfpdOkCDz3keg4vSqeoCJ5+mvuu/pZ6Rbu59YWu1Bt3O5fueYqXXyyKGrzz7bcuEiiR28ajeXMXZjlrVsm+RJkrPSIXH4kVWunRujXs3n1wlE91JJr7a9AgNzWiKt03qu7vbNAgN6dj+fKqq4sRDEzofTJmjLNkx4ypgCzF/fq5/+DUVKhXj5VP5jJNhnFtm9fIZBOoMrLwSfbuT2Hqswcv2eTlUvMr9ODcN+FhlokyV3pEc90kEnqoGX76+fNdJ3jiiSX7GjRwQysvvlhlk5359lv3Pfft63IXVYfBYaNmY0Lvk8MOc5kOPv4YfvtbN1D6+9+7KI2ioiRjnsOjdObO5XdvZ1O/vnDL34515mRqKt1SPqIzK3j6tlXOvAzzhbz7rhs76NDBf/3PPdfV8+233XayQp+MRQ/VX+hVXYfp+efDGTbMjZnPmVMlVSt+auzd243rL19eM1xhRvXFhD4JmjQpEYWCAudiP+EEF1bYu7frAHwnxQq5dD6un82LL8INN0DT/t2LOwBZvIiRNzRm2YHOfDT8D06R/vlPdOL9zJu9j5ycg5OKxaNHD+eq8fz0fl034UK/Y4d7BUHo16xxnVa0aJZzznHtrir3zeLFzl3YsSN06+Y65eryfVbkJK5Y17KJYpVArAD7qnxVlwlTkYTPi6pXT/WZZ1SnTFHNznaTbrzXmDH+r3nBBaqNGqlu3Xrwsc2bVTMyivS6Mz9RbdxYFXQ17RRUHx/8lup335VUzMekrGHD3DyuwkLVI49U/dWvEtfPmwT1wAOq//2v+/yvf8UuX1SkWr++6g03JL52VfLUU64t//1v9OO/+pVqgwaqP/54aOulqnryyar9+7vP77/v6vnKK4e+HpF4k7hEyj+JKzdXNT3dtS0lxU3+O+EE1WbNSv6PbKJYcmATpiqGCI8LI0bAVVfBn//sPC6ehf344y7P2a5d8a/34YdugZObboruRmnaFC64QHh+RQf2/fpGEOFd3Aypvv+53oVqtmzpJmX99rdu9lRubsz7hYdZ+rXoDzvMDSX88EPiGHpwTzw1IZZ+3jz4yU/gpz+NfnzYMPf7vf76oa3Xli1ucl7v3m67Y0f3/VeHAVlvEpdq4gl9iXjrrZIxEG+qSefObp6hh00UqzgsgCtJsrMPjmn2OoD5892j9iuvOPGfPh2uvda5eXJySp+Xlwe//KUb/Lvhhtj3GznSrWH7n8OG8Yu6DzBv75m00P9xwuSbYNdOeOYZN00W3Kymn/0M+vd3gwiNG7t4yDPPhOzs4jDLV15x/6h+fPQiJTnpPaFvmWDpmOou9JHx89E44wzXj06dChdffOjq5vXTp5/u3uvVc4PF1WFAtk6d0ttnnFH2a3kuQW/BnGefdf8feXnud9m/P3p6baOMxDL1q/JVXV03ybB4sWrbtiWPoSkpqqedpnrJJaoDBzr3D7jUOPEeTwsKVI85RvXss1WL3svVFg136vCzvy8pEO5PSk93yW/Cb+zd/NprVZcv16ysIm1z5B4F1b/fttpXW9q3Vx0+XPW++9zl9uyJX370aNWmTX1dukr48kvXjr/9LX656693X+nYsYfOhXDbbe6e4S6jyy93rraq5qyznGtl4ED3/c2eXfZr3XWX+7O8996Dv9v//Mdd//rry1Pb2gfmujn09OrlrHHPYiwqgrVr3driCxaULM7hWZexSE11LqJ33oG3d2bzv50N6Htx2MIs4f6kBQtcsPyaNXDbbaVj9R95BLp149yP/8ja7+oC0OQvY+O6ejy8fDf5+ZCZ6Sb7xuOYY5wLYvfuhJcuxcKFh2YQzm94aseOzr0wfvyhW3lq0SI356FevZJ93bq5HP//+1/l3z8Wn3ziopBuvhleesm57373u7JHAy1a5LKujht38BPyoEHOhbN+fXlrbXiY0Fci/fo5UQyFy/PSSy4R2ezZxVGUvvKYXHWVe7/mGvd+kECFT8ryOP/8UrH6zJgBzz3HuW1L8vA2ObDROe6vugr+9S/nwI8S8hAu9H4WFClL5M20ac4V4DP/W7mYP9/NiTj++PjlNm507146isr2F+/Z49JseG4bD2+Rl6p030ya5P6MRo1yf1Z33ulshHffTf5a+/bBBx8c3M5wevVyk8ktrLSCiGXqV+UrCK4bj1gBMT4DZYo56yz3ONu4sct8XNabFyzK1Yb8oKD6TOpI1TPPVG3SpMTNI+Je6emqf/2r6nff6UUXFelJbXdrpxb/05/33pLwtosXu0u9+abPeqpq9+4lVUhNrbxUx0VFqkcd5SKQEhEeGVKnTuW7bxYscPeaObP0/h9+cPsnTKjc+8di40bX/tGjS/bt3avasqXq6acnn9Z50SLXnldfjV3m8cddma++KlOVayXEcd1UuahHewVJ6CuKceNKhLA8YWe5uaqpKUUu531GobtOQYHqBx+4gYBw337oNTL9n3oU+XoEm/Ua+ZuLH33uOeek/cc/XOXCKrRunTt18mR/ddq0yYXtebf0k4u/rKxa5e7x97/7K//OO65uXrhjZfL737u6bYnSlx57rOpFF1V+HaLxu9+5en3+een9Dz/s9r/7bnLXmzjRnbdpU+wy3roH//xn8vWtrcQTel+uGxHpLyKrRGS1iNwRp1x3ESkUkSHJnmvEJ3xyVHncCPPng4bWez9QmOKuk5rqEtePG1fiU6pbFx5+GCZN4vAjUthEJltpSiv9Fh591OXaP+ccuPJKd16fPm7BdFy0Slqaf9fN5MmuTY8+6to5aFDlZWsMzz/vh7POgtGj3TDId99VTp08Fi926ZKjhb16M2QPNfv2uXDhAQPc5MBwrr7a/da/+11y11y0yEUSNWsWu8zJJ7tJY++9l3ydjSjE6gG8F5AKfAW0AzKAj4CTYpR7F5gFDEnm3MiXWfQHEzlZqzwWfdzrRHH33Hf1N8XW9j/Sf6n69tvONB41yoVOhD8BdO+u+uST2rbFjzq886cJK7p3r1vByrOYhw5VPfxw1d27y9a+RAwb5lw3ybgbvvzSebPGjq2cOqm6h6pGjVT/3/+Lfvz++93XG21inR+SdRV6PPOMu+/bb0c/PmmSOz5/vr/ree0cNSpx2XPOUe3QwXdVaz2Ux3UDZAOzw7bvBO6MUu4G4DfAs2FC7+vcyJcJfXTK+s9a3ut4j+igOvfhT0tfKLzXuOEGN60TtCtLtTVfa25qbzfN9LHHVP/9b3ex669XXbhQVVWffVZLhep5fupnnilfG6Px3ntutuvPfpb8uT//uWpmZuLQ0rLy4Yeu3c8/H/34W2+VzU2i6tpdp07yRkJRkWqnTk5sY3WMP/6o+pOfuKEeP3jtfO65xGXvu891sNu2+bt2bae8Qj8EeCps+3Lg0YgyLYEFIQs+XOgTnhvtZUJfvfjnP0uEftWqiIORvUZRkeae/4CmckChSOuxW3M5Narvv+jEk7RT03V68lFbtWjFR6oLF2rRhIl6Ypvd2rNnnHuUgfD1eNPTk7/U3Lnu3ClTylyFuDzyiLv+2rXRj2/c6I7/+c/JX/vSS0u+9mQGur02P/VU/HJ//rMrt2hR4mt6RkOsdka7/6xZ/upb24kn9H589NHmDkYGPU0CblfVwjKc6wqKjBKRpSKydNOmTT6qZRwqvJz0EGVWbGRopwjzW1wSGgcQ9lKXeVf/ywWBX399yWCDCPO2deHjLa24ccMtSJfO0KcP8tu7GL32Dt5/Hz5sfo5zAvfqBXfd5cYB7roLliyBH39MKvtV+Hq8RUXJj3H07euyhf71r5UT8rdokQtdPeaY6MebN3fffbJ+elUXyuiRzLKEkyY5P/rw4fHLjR7tZlmPGJH4p1i0yC2T6IXgxqNnTzdcZH76CiBWD+C98OF+Ab4G1oZeu4DvgfP9nBvtZRZ99cJzpzRp4q98bq5qvToFKhSWnuEY4eo577StmplZpHs+WqX6i1+453TQrTTReil79P/9dJ5qt25RnwaKw0C96cWjRjl/zzvvuKxrY8eWikP1fMlCodarU1CmhwMvEdq8ecmfGw+/IZ8//7nqSScld+2ZM12d+/d377fe6u+8f//blb/qqsRl/YahFhW58ZhLL/Vf/1NOUc3J8V++NkM5XTdpwBqgLSUDqifHKf8sJa6bpM71Xib01Qsv1K1jR//n5Oa6uO8+fZyuF/uWQ26Yz6cuV3CRmcX7wzqBq87bqA0aqO545/9KjwNMn6768svOKRytA4h8HXWU7u7ZV9vVzdejWav3cbfmpp2uOnWq6oEDpSucYMLDjz+61A7nn1+eb/Ng1qxxVX3ssfjl7r3XjX3v2uXvukVFbmy8bVvV/ftVTz1VtXVr9zkeubmu7/Qb6jpxYklKD4jtGvJST/gNu1VVve4697MnqrNRTqF35zMA+AIXQfPb0L7RwOgoZYuFPta5iV4m9NWLb75xfynHH5+8b3vHDtUTT3Q5Ur75pmT/6NHO+tu4MaxwmKh66Xn/9jeNLsKRA8Hz5jklufrqkkggEdXOnfW2Y6a6yBD6lO4E6tRR7dzZjc6mpbnyaWnOjL39dpeYyNufkaE6ZYr+9pa9KqL61YvLKmYmnLqpCKD68cfxy82Y4cr5vfSbb7ryTz7ptt94Q32NM4weXfIV+fHpez+F94A1fXr0clOmuOMrV/qrv2rJk8UHH/g/p7ZSbqE/1C8T+urFO++U6GZZQjs//9yF1J1yiovS2LzZXeeXv4x9TlGRateuTodjhkL66AA+fHaFpqaq/urn/yvZX6eO6m9/q3rLLS4JXCjX/0GdQPgsrtBrPUdqGvv1Rh7SYrfR5Ze7+MebbnLnpKQ4U3jBgsT1VdVBvbdonbT9unhyfKX/9lt/lr/3/WVnu4R4+/aV7OvWzU2+Cn+YCWfXLtVWrdxvnUyUTm6u6p13uq/jmmuil7nqKvdElExoa36+a/Nf/uL/nNqKCb1RLiZMSM7Ci4bnK77yypIZoJ98Ev+cv//dlcvLS/JmIUEtWJSrWVku/G/rVo3vngl/OvB8++H769Z1506cqMObzNJG/KA7aFDSA8YaR2jXzjnIhwxxjmwvvcQ996j+61+aO+qZ0FhGoYtQuvsN1S++UF2/3vWwEyYU17eoyAnlL3/+v4RPDXPmuCo8/njp/a+84vbHCuMcM8Ydf+SRsgU6jRzpvqrvvz/4WPv2qoMHJ3c9VeduGjIk9vGKCjuu6ZjQG+WioiZr3XtvSWfRo0fi8jt2qDZs6DqHWPWK9w/+l7+4+02b5qNySSQlev+pjxVUBzJTczPOcB3Djz+qvv66U7mUFGfZX3WVc/907Rr16UBBz+O1kk6U/TqRO6J3Gq1bq/burT87/APtxtKSDuOWW9zg8xtvuJ7xhhtUZ8/WPn2KtGVLNyEtvB2Fi3O1QwfnTissLN0+r3MoT3pgbxWy4rGXEBs2uP0PPpj8NYcPd2maoz0J5Oa6hy/v7+rmm50LrLCw5Hht6QRM6I1yUxH/MIsXl7jP/SYJu+Yap5F33+0mVS1Z4nTtV79y/9gi7lqRXpK1a92ShgMGJJ90KxG5uaopUqRQpBlphaXbEeuLeu891wl4TwfTp+v9N30fEvgDmsp+Z9GP/oebTTR4cMmTgoibtZSTo7cd9oims0/3kR69QwCdTx8F1YePuNctgpCTU2oMYuqpkxRUX8z+c/H+H9Kb6dFNdugJbffqj5t3J5+Nzxt9z83VgQPdmEz47GbP1/7++8l/34895s5ds+bgY+E5oMJfTZqo9url+sLyGig1BRN6o1oQHp3h1wXkDVQmetWp4yy/6dOdx+OEE9w+PxNzytKO8MwPXbqobt/u48SQSBa9l1v8dDNsmOrCxz/WiWfP09y/f1y6bJTHqGn3rVJQ/TClm9v/2muqn33mesRQpc5kjrZI36w//uJK1b593ZTesC+rIKOenpCySjvLCi0K7buSZzSVA/o+3Ut/sSkpqllZbnHjc88t6TBSU10YT/fuztwOc1ct6HaDcxuN/sj9AO+9p2NOXaKH1S0oHT0TrdMoKnIzr8JcVitWuEs/N/aLg8oPHqwKRZoiLmz2xRfdbOtf/rL0+rOVmRG1umBCb1QLyuICmjixtGF72WWqn37qgmy8a2VkaLEVGa5RZZkBm2w7PN1r29Zf+uiiIhfQA6ojRrjcL3FvFCFsXvbNpy98Peog9OKU0xVU/3zt19ErHPrivQ70PxlDdEbK+Qqq9/T/wE2DPvvs0uMOxxzjnigiB61btnTjD126lCpf1Phw7cH/aXu+1AJc59OZD7Uf77jHrIYNS/wt4eMZ0Xrw9u214MyfaaPUXTpaJpe4rG68Uf/zm9kKqpfK8zpR7nRutDlzSjU7Pc3N5Uj3++RVg309JvRGtSHZ/6N4nUPktQ4ccNGVnmZUphUXfu/33nNCn5Li3PLjx0dv33vvuUgYcCGMnh85GQoL3fdw6qlRNOrvH+txTbfo4Q0OHBxrH/Fl7d/v6ty+5Y9aP32fHn/07uLonJhfehL7X/zHbgXVl7r/QbdxuAqFOo57nSvpxhtVe/cu3YP37esmufXrV3r/iSeqnnqqnpM+RzvyUXEH8B0/0WZ8r11YrnuJGP9o2tSFF/Xpo++l9NYTWamp7NdlP7/X5VweNar0wPivf636xz+q/uY3JfszMlxs7xdfuMe1996L/YdbFjdXJXQmJvRGjSaZ/4uKGjhOlu3bnWcj3OMxcKBbA/aPf3Shh+HrBPtePCaC3NwSt1FqqhuDuOwyp4/efr9PMt6TBUSZGFVO8SoocFE2PU/eoW9kuCeGuRn9y9RpqLoMqkKhbks5Qovq1tMB3Tdq3TqFuvLBWe7pwBsAHz3apQDt37/YZbWZI7Ql6/Q4VulO6kd/ckjm1bChe3ljLuFPJb17uzpcfbX7Ibx6jRvnBpfGjSsJwc3IcImC5sxxSf6efNLNECvjH60JvVGrqKqn70jffcOG0YNtyvOkEe7KAudNadeubP7o8eMrpk6x8AZR+3TZrmkpBbr73f8rXSCJzsRLcPbmiGn6+C1fucHmh31cJ9RpzM/4mYoU6YgrC52ohncmc+eq7tzpBnfC51r85S/OldW/f+mnjFNPddFNt96qesYZpX+QFi3cE0V5OpIyWigm9IZxCIhmjBYVOQ15+eUSA7Ay1hMoy5NMZT/97N5d0gG1alW+6+/a5ep58cWuruec49P1FdYJ3HOPq8sLL2jMziH37/4HxuMemzevdJTV1Klu5uDUqbogvZ/eI/fpgrQzteiJJ13HM3Kk5sppOpE7NDelV5l6XRN6wzhExHuaqKgnjYp0/Vb208/IkSWGcHk7kxNOcNdq1MjNJ0uWAwecZ6VhQ9XVqw8+vnBhiRfooLqW5YcN279vnwuQOuccVSgq5eJr1Ej1iEb7QxPnQqm9wzsan8QTenHHqxdZWVm6dOnSqq6GYRjl5O67YcIE9zk1FcaPd1mtkyUvz2WpLihwqZbnzy/bcpPffgudO7vs1xddBE2bwrZtbhnHRYvgwIGSsj//Ofzzn3D44YnrNn++S/8cXqdFi2DKFNi0yaVa/uEHt0Lnvn1O5kXgjDNcfXJzYckSBYTUFGX87yXp70lElqlqVrRjacldyjAMwz/nnQcPPeTWBE4mF34k8+c7cQQoLCy70B9zDNx2m1vW4Pe/d/tEoEsXGDwYZs50nYkqvPYaHHUUXHIJnHaaE+y+faFbN/f5++9hwQK44w7XQaSmus7owAFYswbWry+5b//+cO210KCB++x9HxMnunbk5UG/fhLaL2X+nmJhQm8YRqWRne0WVo9m8SZDTo4TxvJ2GB4iTsxTUuCee9z69lDaOs/IgCeegOeeg2efTXzNggL46KOSBd43bHD38DqAAQNcuWjfR0V9TzHba64bwzBqArFcJGW5Tr9+JZ3G3LnxrzduHNx3X4m75ayzYMgQt+rX99+7hdMOHCh9rWTvURHEc92Y0BuGUetIptNIJNqxrlVRHZNfTOgNwzDKwaEW7bJgg7GGYRjlIDu7+gq8H1L8FBKR/iKySkRWi8gdUY4PFpGPRWSFiCwVkd5hx24UkZUi8qmITBWRuhXZAMMwDCM+CYVeRFKBx4BzgZOAYSJyUkSxuUBnVe0CjASeCp3bErgOyFLVDkAqMLTCam8YhmEkxI9F3wNYraprVHU/MA0YHF5AVXdpibO/PhDu+E8D6olIGnAYsKH81TYMwzD84kfoWwLrwrbzQ/tKISIXiMjnwBs4qx5VXQ88CHwLfAdsV9W3o91EREaF3D5LN23alFwrDMMwjJj4EXqJsu+gUB1VfVVVfwqcD4wHEJEmOOu/LXAUUF9ELot2E1V9QlWzVDUrMzPTZ/UNwzCMRPgR+nzg6LDtVsRxv6jqQqC9iDQDzgK+VtVNqnoAeAU4rRz1NQzDMJLET3jlEuA4EWkLrMcNpg4PLyAixwJfqaqKSDcgA9iCc9mcKiKHAXuAfkDCAPlly5ZtFpFvkmpJCc2AzWU8tyZj7a5dWLtrF37a3TrWgYRCr6oFIjIGmI2LmpmiqitFZHTo+GTgIuAKETmAE/RLQoOz74vIS8ByoAD4EHjCxz3L7LsRkaWxJg0EGWt37cLaXbsob7ur5czY8mB/CLULa3ftwtpdNnxNmDIMwzBqLkEU+oSuoYBi7a5dWLtrF+Vqd+BcN4ZhGEZpgmjRG4ZhGGGY0BuGYQScwAh9ogybQUJEpojI9yLyadi+I0TkHRH5MvTepCrrWNGIyNEiMk9EPgtlQ70+tD/o7a4rIh+IyEehdv8utD/Q7fYQkVQR+VBEXg9t15Z2rxWRT7yMwKF9ZW57IITeZ4bNIPEs0D9i3x3AXFU9DpdNNGidXQFws6qeCJwK/Cb0Gwe93fuAM1W1M9AF6C8ipxL8dntcD3wWtl1b2g3QV1W7hIVVlrntgRB6fGTYDBKhNBNbI3YPBv4R+vwPXM6hwKCq36nq8tDnnbh//pYEv92qqrtCm+mhlxLwdgOISCvgPEJpz0MEvt1xKHPbgyL0vjJsBpyfqOp34EQRaF7F9ak0RKQN0BV4n1rQ7pD7YgXwPfCOqtaKdgOTgNuAorB9taHd4Drzt0VkmYiMCu0rc9uDspSgrwybRs1HRBoALwM3qOoOkWg/fbBQ1UKgi4gcDrwqIh2quEqVjogMBL5X1WUiklPF1akKeqnqBhFpDrwTSgFfZoJi0SeVYTOgbBSRIwFC799XcX0qHBFJx4n8C6r6Smh34Nvtoao/APNx4zNBb3cvYJCIrMW5Ys8UkecJfrsBUNUNoffvgVdx7ukytz0oQl+cYVNEMnAZNmdWcZ0ONTOBK0OfrwT+U4V1qXDEme5PA5+p6kNhh4Le7syQJY+I1MOl/v6cgLdbVe9U1Vaq2gb3//yuql5GwNsNICL1RaSh9xk4G/iUcrQ9MDNjRWQAzqfnZdicULU1qjxEZCqQg0tduhG4F5gBTAeOwaWHvlhVIwdsayziFpxfBHxCic/2LpyfPsjt7oQbeEvFGWbTVfU+EWlKgNsdTsh1c4uqDqwN7RaRdjgrHpx7/V+qOqE8bQ+M0BuGYRjRCYrrxjAMw4iBCb1hGEbAMaE3DMMIOCb0hmEYAceE3jAMI+CY0BuGYQQcE3rDMIyA8/8BnOmJifCNXaQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(run_hist_1.history[\"loss\"],'r', marker='.', label=\"Train Loss\")\n",
    "ax.plot(run_hist_1.history[\"val_loss\"],'b', marker='.', label=\"Validation Loss\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change to epochs 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "7424/7500 [============================>.] - ETA: 0s - loss: 0.3809 - accuracy: 0.8361WARNING:tensorflow:Callbacks method `on_test_batch_begin` is slow compared to the batch time (batch time: 0.0000s vs `on_test_batch_begin` time: 0.0010s). Check your callbacks.\n",
      "7500/7500 [==============================] - 4s 549us/step - loss: 0.3810 - accuracy: 0.8360 - val_loss: 0.3859 - val_accuracy: 0.8334\n",
      "Epoch 2/500\n",
      "7500/7500 [==============================] - 4s 551us/step - loss: 0.3807 - accuracy: 0.8361 - val_loss: 0.3809 - val_accuracy: 0.8354\n",
      "Epoch 3/500\n",
      "7500/7500 [==============================] - 6s 742us/step - loss: 0.3807 - accuracy: 0.8364 - val_loss: 0.3799 - val_accuracy: 0.8360\n",
      "Epoch 4/500\n",
      "7500/7500 [==============================] - 4s 559us/step - loss: 0.3809 - accuracy: 0.8364 - val_loss: 0.3798 - val_accuracy: 0.8357\n",
      "Epoch 5/500\n",
      "7500/7500 [==============================] - 4s 546us/step - loss: 0.3809 - accuracy: 0.8363 - val_loss: 0.3863 - val_accuracy: 0.8321\n",
      "Epoch 6/500\n",
      "7500/7500 [==============================] - 4s 544us/step - loss: 0.3809 - accuracy: 0.8359 - val_loss: 0.3809 - val_accuracy: 0.8365\n",
      "Epoch 7/500\n",
      "7500/7500 [==============================] - 4s 542us/step - loss: 0.3805 - accuracy: 0.8366 - val_loss: 0.3806 - val_accuracy: 0.8360\n",
      "Epoch 8/500\n",
      "7500/7500 [==============================] - 4s 544us/step - loss: 0.3807 - accuracy: 0.8362 - val_loss: 0.3789 - val_accuracy: 0.8361\n",
      "Epoch 9/500\n",
      "7500/7500 [==============================] - 4s 544us/step - loss: 0.3804 - accuracy: 0.8361 - val_loss: 0.3868 - val_accuracy: 0.8340\n",
      "Epoch 10/500\n",
      "7500/7500 [==============================] - 4s 543us/step - loss: 0.3806 - accuracy: 0.8363 - val_loss: 0.3809 - val_accuracy: 0.8367\n",
      "Epoch 11/500\n",
      "7500/7500 [==============================] - 4s 563us/step - loss: 0.3807 - accuracy: 0.8369 - val_loss: 0.3790 - val_accuracy: 0.8369\n",
      "Epoch 12/500\n",
      "7500/7500 [==============================] - 4s 558us/step - loss: 0.3807 - accuracy: 0.8360 - val_loss: 0.3895 - val_accuracy: 0.8336\n",
      "Epoch 13/500\n",
      "7500/7500 [==============================] - 4s 543us/step - loss: 0.3806 - accuracy: 0.8364 - val_loss: 0.3840 - val_accuracy: 0.8349\n",
      "Epoch 14/500\n",
      "7500/7500 [==============================] - 4s 544us/step - loss: 0.3804 - accuracy: 0.8361 - val_loss: 0.3797 - val_accuracy: 0.8365\n",
      "Epoch 15/500\n",
      "7500/7500 [==============================] - 4s 544us/step - loss: 0.3806 - accuracy: 0.8363 - val_loss: 0.3926 - val_accuracy: 0.8288\n",
      "Epoch 16/500\n",
      "7500/7500 [==============================] - 4s 553us/step - loss: 0.3808 - accuracy: 0.8360 - val_loss: 0.3790 - val_accuracy: 0.8370\n",
      "Epoch 17/500\n",
      "7500/7500 [==============================] - 4s 582us/step - loss: 0.3805 - accuracy: 0.8361 - val_loss: 0.3856 - val_accuracy: 0.8333\n",
      "Epoch 18/500\n",
      "7500/7500 [==============================] - 4s 545us/step - loss: 0.3804 - accuracy: 0.8369 - val_loss: 0.3804 - val_accuracy: 0.8372\n",
      "Epoch 19/500\n",
      "7500/7500 [==============================] - 4s 548us/step - loss: 0.3804 - accuracy: 0.8361 - val_loss: 0.3940 - val_accuracy: 0.8317\n",
      "Epoch 20/500\n",
      "7500/7500 [==============================] - 4s 551us/step - loss: 0.3804 - accuracy: 0.8362 - val_loss: 0.3782 - val_accuracy: 0.8363\n",
      "Epoch 21/500\n",
      "7500/7500 [==============================] - 4s 545us/step - loss: 0.3806 - accuracy: 0.8359 - val_loss: 0.3811 - val_accuracy: 0.8363\n",
      "Epoch 22/500\n",
      "7500/7500 [==============================] - 4s 543us/step - loss: 0.3805 - accuracy: 0.8364 - val_loss: 0.3784 - val_accuracy: 0.8374\n",
      "Epoch 23/500\n",
      "7500/7500 [==============================] - 4s 545us/step - loss: 0.3804 - accuracy: 0.8364 - val_loss: 0.3859 - val_accuracy: 0.8318\n",
      "Epoch 24/500\n",
      "7500/7500 [==============================] - 4s 545us/step - loss: 0.3802 - accuracy: 0.8365 - val_loss: 0.3807 - val_accuracy: 0.8361\n",
      "Epoch 25/500\n",
      "7500/7500 [==============================] - 4s 542us/step - loss: 0.3804 - accuracy: 0.8361 - val_loss: 0.3792 - val_accuracy: 0.8369\n",
      "Epoch 26/500\n",
      "7500/7500 [==============================] - 4s 542us/step - loss: 0.3804 - accuracy: 0.8361 - val_loss: 0.3807 - val_accuracy: 0.8362\n",
      "Epoch 27/500\n",
      "7500/7500 [==============================] - 4s 567us/step - loss: 0.3805 - accuracy: 0.8362 - val_loss: 0.3787 - val_accuracy: 0.8364\n",
      "Epoch 28/500\n",
      "7500/7500 [==============================] - 4s 558us/step - loss: 0.3804 - accuracy: 0.8361 - val_loss: 0.3788 - val_accuracy: 0.8361\n",
      "Epoch 29/500\n",
      "7500/7500 [==============================] - 4s 544us/step - loss: 0.3800 - accuracy: 0.8363 - val_loss: 0.4103 - val_accuracy: 0.8199\n",
      "Epoch 30/500\n",
      "7500/7500 [==============================] - 4s 541us/step - loss: 0.3802 - accuracy: 0.8364 - val_loss: 0.3796 - val_accuracy: 0.8363\n",
      "Epoch 31/500\n",
      "7500/7500 [==============================] - 4s 542us/step - loss: 0.3804 - accuracy: 0.8363 - val_loss: 0.3789 - val_accuracy: 0.8365\n",
      "Epoch 32/500\n",
      "7500/7500 [==============================] - 4s 543us/step - loss: 0.3803 - accuracy: 0.8366 - val_loss: 0.3829 - val_accuracy: 0.8345\n",
      "Epoch 33/500\n",
      "7500/7500 [==============================] - 4s 543us/step - loss: 0.3802 - accuracy: 0.8366 - val_loss: 0.3918 - val_accuracy: 0.8321\n",
      "Epoch 34/500\n",
      "7500/7500 [==============================] - 4s 552us/step - loss: 0.3804 - accuracy: 0.8363 - val_loss: 0.3829 - val_accuracy: 0.8355\n",
      "Epoch 35/500\n",
      "7500/7500 [==============================] - 4s 545us/step - loss: 0.3801 - accuracy: 0.8365 - val_loss: 0.3832 - val_accuracy: 0.8337\n",
      "Epoch 36/500\n",
      "7500/7500 [==============================] - 4s 546us/step - loss: 0.3802 - accuracy: 0.8361 - val_loss: 0.3782 - val_accuracy: 0.8371\n",
      "Epoch 37/500\n",
      "7500/7500 [==============================] - 4s 543us/step - loss: 0.3800 - accuracy: 0.8362 - val_loss: 0.3782 - val_accuracy: 0.8360\n",
      "Epoch 38/500\n",
      "7500/7500 [==============================] - 4s 540us/step - loss: 0.3801 - accuracy: 0.8363 - val_loss: 0.3776 - val_accuracy: 0.8372\n",
      "Epoch 39/500\n",
      "7500/7500 [==============================] - 4s 543us/step - loss: 0.3803 - accuracy: 0.8365 - val_loss: 0.3827 - val_accuracy: 0.8345\n",
      "Epoch 40/500\n",
      "7500/7500 [==============================] - 4s 542us/step - loss: 0.3799 - accuracy: 0.8368 - val_loss: 0.3896 - val_accuracy: 0.8295\n",
      "Epoch 41/500\n",
      "7500/7500 [==============================] - 4s 543us/step - loss: 0.3801 - accuracy: 0.8363 - val_loss: 0.3869 - val_accuracy: 0.8321\n",
      "Epoch 42/500\n",
      "7500/7500 [==============================] - 4s 540us/step - loss: 0.3802 - accuracy: 0.8362 - val_loss: 0.3812 - val_accuracy: 0.8359\n",
      "Epoch 43/500\n",
      "7500/7500 [==============================] - 4s 555us/step - loss: 0.3802 - accuracy: 0.8361 - val_loss: 0.3785 - val_accuracy: 0.8368\n",
      "Epoch 44/500\n",
      "7500/7500 [==============================] - 4s 563us/step - loss: 0.3800 - accuracy: 0.8367 - val_loss: 0.3802 - val_accuracy: 0.8358\n",
      "Epoch 45/500\n",
      "7500/7500 [==============================] - 4s 550us/step - loss: 0.3798 - accuracy: 0.8367 - val_loss: 0.3842 - val_accuracy: 0.8343\n",
      "Epoch 46/500\n",
      "7500/7500 [==============================] - 4s 552us/step - loss: 0.3798 - accuracy: 0.8368 - val_loss: 0.3790 - val_accuracy: 0.8368\n",
      "Epoch 47/500\n",
      "7500/7500 [==============================] - 4s 540us/step - loss: 0.3801 - accuracy: 0.8367 - val_loss: 0.3806 - val_accuracy: 0.8357\n",
      "Epoch 48/500\n",
      "7500/7500 [==============================] - 4s 539us/step - loss: 0.3799 - accuracy: 0.8367 - val_loss: 0.3782 - val_accuracy: 0.8371\n",
      "Epoch 49/500\n",
      "7500/7500 [==============================] - 4s 541us/step - loss: 0.3798 - accuracy: 0.8364 - val_loss: 0.3780 - val_accuracy: 0.8370\n",
      "Epoch 50/500\n",
      "7500/7500 [==============================] - 4s 539us/step - loss: 0.3799 - accuracy: 0.8365 - val_loss: 0.3862 - val_accuracy: 0.8334\n",
      "Epoch 51/500\n",
      "7500/7500 [==============================] - 4s 540us/step - loss: 0.3800 - accuracy: 0.8367 - val_loss: 0.3793 - val_accuracy: 0.8368\n",
      "Epoch 52/500\n",
      "7500/7500 [==============================] - 4s 540us/step - loss: 0.3801 - accuracy: 0.8369 - val_loss: 0.3778 - val_accuracy: 0.8374\n",
      "Epoch 53/500\n",
      "7500/7500 [==============================] - 4s 541us/step - loss: 0.3798 - accuracy: 0.8366 - val_loss: 0.3830 - val_accuracy: 0.8347\n",
      "Epoch 54/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 4s 539us/step - loss: 0.3797 - accuracy: 0.8370 - val_loss: 0.3855 - val_accuracy: 0.8332\n",
      "Epoch 55/500\n",
      "7500/7500 [==============================] - 4s 545us/step - loss: 0.3799 - accuracy: 0.8369 - val_loss: 0.3793 - val_accuracy: 0.8373\n",
      "Epoch 56/500\n",
      "7500/7500 [==============================] - 4s 542us/step - loss: 0.3797 - accuracy: 0.8362 - val_loss: 0.3777 - val_accuracy: 0.8377\n",
      "Epoch 57/500\n",
      "7500/7500 [==============================] - 4s 541us/step - loss: 0.3798 - accuracy: 0.8363 - val_loss: 0.3847 - val_accuracy: 0.8363\n",
      "Epoch 58/500\n",
      "7500/7500 [==============================] - 4s 539us/step - loss: 0.3798 - accuracy: 0.8362 - val_loss: 0.3786 - val_accuracy: 0.8371\n",
      "Epoch 59/500\n",
      "7500/7500 [==============================] - 4s 549us/step - loss: 0.3798 - accuracy: 0.8367 - val_loss: 0.3776 - val_accuracy: 0.8377\n",
      "Epoch 60/500\n",
      "7500/7500 [==============================] - 4s 565us/step - loss: 0.3795 - accuracy: 0.8367 - val_loss: 0.3890 - val_accuracy: 0.8316\n",
      "Epoch 61/500\n",
      "7500/7500 [==============================] - 4s 566us/step - loss: 0.3799 - accuracy: 0.8364 - val_loss: 0.3811 - val_accuracy: 0.8354\n",
      "Epoch 62/500\n",
      "7500/7500 [==============================] - 4s 542us/step - loss: 0.3798 - accuracy: 0.8364 - val_loss: 0.3783 - val_accuracy: 0.8377\n",
      "Epoch 63/500\n",
      "7500/7500 [==============================] - 4s 543us/step - loss: 0.3797 - accuracy: 0.8368 - val_loss: 0.3779 - val_accuracy: 0.8372\n",
      "Epoch 64/500\n",
      "7500/7500 [==============================] - 4s 543us/step - loss: 0.3796 - accuracy: 0.8367 - val_loss: 0.3863 - val_accuracy: 0.8320\n",
      "Epoch 65/500\n",
      "7500/7500 [==============================] - 4s 541us/step - loss: 0.3795 - accuracy: 0.8372 - val_loss: 0.3786 - val_accuracy: 0.8369\n",
      "Epoch 66/500\n",
      "7500/7500 [==============================] - 4s 541us/step - loss: 0.3796 - accuracy: 0.8369 - val_loss: 0.3786 - val_accuracy: 0.8372\n",
      "Epoch 67/500\n",
      "7500/7500 [==============================] - 4s 539us/step - loss: 0.3797 - accuracy: 0.8369 - val_loss: 0.3933 - val_accuracy: 0.8293\n",
      "Epoch 68/500\n",
      "7500/7500 [==============================] - 4s 541us/step - loss: 0.3798 - accuracy: 0.8367 - val_loss: 0.3803 - val_accuracy: 0.8363\n",
      "Epoch 69/500\n",
      "7500/7500 [==============================] - 4s 539us/step - loss: 0.3793 - accuracy: 0.8370 - val_loss: 0.3986 - val_accuracy: 0.8298\n",
      "Epoch 70/500\n",
      "7500/7500 [==============================] - 4s 539us/step - loss: 0.3795 - accuracy: 0.8367 - val_loss: 0.3787 - val_accuracy: 0.8372\n",
      "Epoch 71/500\n",
      "7500/7500 [==============================] - 4s 541us/step - loss: 0.3795 - accuracy: 0.8368 - val_loss: 0.3972 - val_accuracy: 0.8322\n",
      "Epoch 72/500\n",
      "7500/7500 [==============================] - 4s 542us/step - loss: 0.3796 - accuracy: 0.8367 - val_loss: 0.3781 - val_accuracy: 0.8374\n",
      "Epoch 73/500\n",
      "7500/7500 [==============================] - 4s 537us/step - loss: 0.3796 - accuracy: 0.8367 - val_loss: 0.3802 - val_accuracy: 0.8357\n",
      "Epoch 74/500\n",
      "7500/7500 [==============================] - 4s 539us/step - loss: 0.3795 - accuracy: 0.8367 - val_loss: 0.3778 - val_accuracy: 0.8374\n",
      "Epoch 75/500\n",
      "7500/7500 [==============================] - 4s 566us/step - loss: 0.3795 - accuracy: 0.8369 - val_loss: 0.3802 - val_accuracy: 0.8362\n",
      "Epoch 76/500\n",
      "7500/7500 [==============================] - 4s 588us/step - loss: 0.3794 - accuracy: 0.8368 - val_loss: 0.3805 - val_accuracy: 0.8357\n",
      "Epoch 77/500\n",
      "7500/7500 [==============================] - 4s 545us/step - loss: 0.3794 - accuracy: 0.8369 - val_loss: 0.3827 - val_accuracy: 0.8346\n",
      "Epoch 78/500\n",
      "7500/7500 [==============================] - 4s 539us/step - loss: 0.3795 - accuracy: 0.8369 - val_loss: 0.3789 - val_accuracy: 0.8369\n",
      "Epoch 79/500\n",
      "7500/7500 [==============================] - 4s 546us/step - loss: 0.3795 - accuracy: 0.8370 - val_loss: 0.3787 - val_accuracy: 0.8368\n",
      "Epoch 80/500\n",
      "7500/7500 [==============================] - 4s 542us/step - loss: 0.3796 - accuracy: 0.8365 - val_loss: 0.3782 - val_accuracy: 0.8376\n",
      "Epoch 81/500\n",
      "7500/7500 [==============================] - 4s 538us/step - loss: 0.3795 - accuracy: 0.8365 - val_loss: 0.3816 - val_accuracy: 0.8356\n",
      "Epoch 82/500\n",
      "7500/7500 [==============================] - 4s 562us/step - loss: 0.3795 - accuracy: 0.8364 - val_loss: 0.3810 - val_accuracy: 0.8364\n",
      "Epoch 83/500\n",
      "7500/7500 [==============================] - 4s 544us/step - loss: 0.3795 - accuracy: 0.8368 - val_loss: 0.3792 - val_accuracy: 0.8372\n",
      "Epoch 84/500\n",
      "7500/7500 [==============================] - 4s 543us/step - loss: 0.3794 - accuracy: 0.8368 - val_loss: 0.3779 - val_accuracy: 0.8367\n",
      "Epoch 85/500\n",
      "7500/7500 [==============================] - 4s 541us/step - loss: 0.3792 - accuracy: 0.8368 - val_loss: 0.3886 - val_accuracy: 0.8324\n",
      "Epoch 86/500\n",
      "7500/7500 [==============================] - 4s 548us/step - loss: 0.3793 - accuracy: 0.8365 - val_loss: 0.4133 - val_accuracy: 0.8196\n",
      "Epoch 87/500\n",
      "7500/7500 [==============================] - 4s 544us/step - loss: 0.3792 - accuracy: 0.8367 - val_loss: 0.3780 - val_accuracy: 0.8365\n",
      "Epoch 88/500\n",
      "7500/7500 [==============================] - 4s 545us/step - loss: 0.3793 - accuracy: 0.8365 - val_loss: 0.3798 - val_accuracy: 0.8360\n",
      "Epoch 89/500\n",
      "7500/7500 [==============================] - 4s 540us/step - loss: 0.3792 - accuracy: 0.8368 - val_loss: 0.3892 - val_accuracy: 0.8309\n",
      "Epoch 90/500\n",
      "7500/7500 [==============================] - 4s 544us/step - loss: 0.3793 - accuracy: 0.8367 - val_loss: 0.3819 - val_accuracy: 0.8357\n",
      "Epoch 91/500\n",
      "7500/7500 [==============================] - 4s 566us/step - loss: 0.3791 - accuracy: 0.8367 - val_loss: 0.3785 - val_accuracy: 0.8366\n",
      "Epoch 92/500\n",
      "7500/7500 [==============================] - 4s 569us/step - loss: 0.3792 - accuracy: 0.8369 - val_loss: 0.3792 - val_accuracy: 0.8376\n",
      "Epoch 93/500\n",
      "7500/7500 [==============================] - 4s 547us/step - loss: 0.3790 - accuracy: 0.8369 - val_loss: 0.4199 - val_accuracy: 0.8155\n",
      "Epoch 94/500\n",
      "7500/7500 [==============================] - 4s 544us/step - loss: 0.3790 - accuracy: 0.8367 - val_loss: 0.3780 - val_accuracy: 0.8375\n",
      "Epoch 95/500\n",
      "7500/7500 [==============================] - 4s 540us/step - loss: 0.3793 - accuracy: 0.8368 - val_loss: 0.3878 - val_accuracy: 0.8339\n",
      "Epoch 96/500\n",
      "7500/7500 [==============================] - 4s 539us/step - loss: 0.3791 - accuracy: 0.8368 - val_loss: 0.3795 - val_accuracy: 0.8363\n",
      "Epoch 97/500\n",
      "7500/7500 [==============================] - 4s 541us/step - loss: 0.3790 - accuracy: 0.8372 - val_loss: 0.3781 - val_accuracy: 0.8375\n",
      "Epoch 98/500\n",
      "7500/7500 [==============================] - 4s 539us/step - loss: 0.3793 - accuracy: 0.8371 - val_loss: 0.3809 - val_accuracy: 0.8362\n",
      "Epoch 99/500\n",
      "7500/7500 [==============================] - 5s 604us/step - loss: 0.3792 - accuracy: 0.8365 - val_loss: 0.3775 - val_accuracy: 0.8374\n",
      "Epoch 100/500\n",
      "7500/7500 [==============================] - 5s 657us/step - loss: 0.3792 - accuracy: 0.8366 - val_loss: 0.3962 - val_accuracy: 0.8274\n",
      "Epoch 101/500\n",
      "7500/7500 [==============================] - 5s 617us/step - loss: 0.3792 - accuracy: 0.8370 - val_loss: 0.3866 - val_accuracy: 0.8341\n",
      "Epoch 102/500\n",
      "7500/7500 [==============================] - 4s 556us/step - loss: 0.3791 - accuracy: 0.8370 - val_loss: 0.3783 - val_accuracy: 0.8372\n",
      "Epoch 103/500\n",
      "7500/7500 [==============================] - 4s 542us/step - loss: 0.3792 - accuracy: 0.8368 - val_loss: 0.3893 - val_accuracy: 0.8329\n",
      "Epoch 104/500\n",
      "7500/7500 [==============================] - 4s 562us/step - loss: 0.3790 - accuracy: 0.8367 - val_loss: 0.3793 - val_accuracy: 0.8376\n",
      "Epoch 105/500\n",
      "7500/7500 [==============================] - 4s 540us/step - loss: 0.3790 - accuracy: 0.8367 - val_loss: 0.3786 - val_accuracy: 0.8370\n",
      "Epoch 106/500\n",
      "7500/7500 [==============================] - 4s 542us/step - loss: 0.3788 - accuracy: 0.8372 - val_loss: 0.3821 - val_accuracy: 0.8369\n",
      "Epoch 107/500\n",
      "7500/7500 [==============================] - 4s 567us/step - loss: 0.3792 - accuracy: 0.8367 - val_loss: 0.3783 - val_accuracy: 0.8372\n",
      "Epoch 108/500\n",
      "7500/7500 [==============================] - 4s 552us/step - loss: 0.3791 - accuracy: 0.8368 - val_loss: 0.3779 - val_accuracy: 0.8373\n",
      "Epoch 109/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 4s 541us/step - loss: 0.3791 - accuracy: 0.8368 - val_loss: 0.3872 - val_accuracy: 0.8328\n",
      "Epoch 110/500\n",
      "7500/7500 [==============================] - 4s 540us/step - loss: 0.3793 - accuracy: 0.8372 - val_loss: 0.3788 - val_accuracy: 0.8366\n",
      "Epoch 111/500\n",
      "7500/7500 [==============================] - 4s 544us/step - loss: 0.3789 - accuracy: 0.8371 - val_loss: 0.3773 - val_accuracy: 0.8376\n",
      "Epoch 112/500\n",
      "7500/7500 [==============================] - 4s 538us/step - loss: 0.3791 - accuracy: 0.8368 - val_loss: 0.3807 - val_accuracy: 0.8375\n",
      "Epoch 113/500\n",
      "7500/7500 [==============================] - 4s 538us/step - loss: 0.3788 - accuracy: 0.8370 - val_loss: 0.3810 - val_accuracy: 0.8354\n",
      "Epoch 114/500\n",
      "7500/7500 [==============================] - 4s 541us/step - loss: 0.3789 - accuracy: 0.8370 - val_loss: 0.3831 - val_accuracy: 0.8353\n",
      "Epoch 115/500\n",
      "7500/7500 [==============================] - 4s 546us/step - loss: 0.3789 - accuracy: 0.8374 - val_loss: 0.3793 - val_accuracy: 0.8369\n",
      "Epoch 116/500\n",
      "7500/7500 [==============================] - 4s 537us/step - loss: 0.3788 - accuracy: 0.8373 - val_loss: 0.3887 - val_accuracy: 0.8322\n",
      "Epoch 117/500\n",
      "7500/7500 [==============================] - 4s 537us/step - loss: 0.3789 - accuracy: 0.8374 - val_loss: 0.3785 - val_accuracy: 0.8367\n",
      "Epoch 118/500\n",
      "7500/7500 [==============================] - 4s 540us/step - loss: 0.3790 - accuracy: 0.8373 - val_loss: 0.3810 - val_accuracy: 0.8360\n",
      "Epoch 119/500\n",
      "7500/7500 [==============================] - 4s 540us/step - loss: 0.3788 - accuracy: 0.8369 - val_loss: 0.3834 - val_accuracy: 0.8349\n",
      "Epoch 120/500\n",
      "7500/7500 [==============================] - 4s 538us/step - loss: 0.3787 - accuracy: 0.8374 - val_loss: 0.3783 - val_accuracy: 0.8369\n",
      "Epoch 121/500\n",
      "7500/7500 [==============================] - 4s 538us/step - loss: 0.3789 - accuracy: 0.8371 - val_loss: 0.3776 - val_accuracy: 0.8375\n",
      "Epoch 122/500\n",
      "7500/7500 [==============================] - 4s 540us/step - loss: 0.3790 - accuracy: 0.8373 - val_loss: 0.3789 - val_accuracy: 0.8366\n",
      "Epoch 123/500\n",
      "7500/7500 [==============================] - 4s 565us/step - loss: 0.3787 - accuracy: 0.8372 - val_loss: 0.3768 - val_accuracy: 0.8380\n",
      "Epoch 124/500\n",
      "7500/7500 [==============================] - 4s 566us/step - loss: 0.3789 - accuracy: 0.8370 - val_loss: 0.3832 - val_accuracy: 0.8356\n",
      "Epoch 125/500\n",
      "7500/7500 [==============================] - 4s 541us/step - loss: 0.3787 - accuracy: 0.8370 - val_loss: 0.3777 - val_accuracy: 0.8371\n",
      "Epoch 126/500\n",
      "7500/7500 [==============================] - 4s 541us/step - loss: 0.3790 - accuracy: 0.8372 - val_loss: 0.3784 - val_accuracy: 0.8376\n",
      "Epoch 127/500\n",
      "7500/7500 [==============================] - 4s 540us/step - loss: 0.3789 - accuracy: 0.8371 - val_loss: 0.3860 - val_accuracy: 0.8327\n",
      "Epoch 128/500\n",
      "7500/7500 [==============================] - 4s 539us/step - loss: 0.3787 - accuracy: 0.8373 - val_loss: 0.3868 - val_accuracy: 0.8331\n",
      "Epoch 129/500\n",
      "7500/7500 [==============================] - 4s 543us/step - loss: 0.3786 - accuracy: 0.8373 - val_loss: 0.3860 - val_accuracy: 0.8326\n",
      "Epoch 130/500\n",
      "7500/7500 [==============================] - 4s 541us/step - loss: 0.3786 - accuracy: 0.8373 - val_loss: 0.3785 - val_accuracy: 0.8375\n",
      "Epoch 131/500\n",
      "7500/7500 [==============================] - 4s 542us/step - loss: 0.3787 - accuracy: 0.8375 - val_loss: 0.3787 - val_accuracy: 0.8363\n",
      "Epoch 132/500\n",
      "7500/7500 [==============================] - 4s 545us/step - loss: 0.3788 - accuracy: 0.8370 - val_loss: 0.3828 - val_accuracy: 0.8354\n",
      "Epoch 133/500\n",
      "7500/7500 [==============================] - 4s 554us/step - loss: 0.3786 - accuracy: 0.8375 - val_loss: 0.3784 - val_accuracy: 0.8369\n",
      "Epoch 134/500\n",
      "7500/7500 [==============================] - 4s 561us/step - loss: 0.3786 - accuracy: 0.8367 - val_loss: 0.3788 - val_accuracy: 0.8366\n",
      "Epoch 135/500\n",
      "7500/7500 [==============================] - 4s 588us/step - loss: 0.3786 - accuracy: 0.8374 - val_loss: 0.3897 - val_accuracy: 0.8336\n",
      "Epoch 136/500\n",
      "7500/7500 [==============================] - 4s 597us/step - loss: 0.3788 - accuracy: 0.8368 - val_loss: 0.3838 - val_accuracy: 0.8333\n",
      "Epoch 137/500\n",
      "7500/7500 [==============================] - 4s 584us/step - loss: 0.3788 - accuracy: 0.8369 - val_loss: 0.3777 - val_accuracy: 0.8373\n",
      "Epoch 138/500\n",
      "7500/7500 [==============================] - 4s 548us/step - loss: 0.3786 - accuracy: 0.8371 - val_loss: 0.3770 - val_accuracy: 0.8377\n",
      "Epoch 139/500\n",
      "7500/7500 [==============================] - 4s 591us/step - loss: 0.3786 - accuracy: 0.8372 - val_loss: 0.3848 - val_accuracy: 0.8335\n",
      "Epoch 140/500\n",
      "7500/7500 [==============================] - 4s 596us/step - loss: 0.3787 - accuracy: 0.8373 - val_loss: 0.3790 - val_accuracy: 0.8363\n",
      "Epoch 141/500\n",
      "7500/7500 [==============================] - 4s 560us/step - loss: 0.3786 - accuracy: 0.8368 - val_loss: 0.3793 - val_accuracy: 0.8370\n",
      "Epoch 142/500\n",
      "7500/7500 [==============================] - 4s 598us/step - loss: 0.3787 - accuracy: 0.8373 - val_loss: 0.3811 - val_accuracy: 0.8361\n",
      "Epoch 143/500\n",
      "7500/7500 [==============================] - 7s 933us/step - loss: 0.3786 - accuracy: 0.8367 - val_loss: 0.3844 - val_accuracy: 0.8343\n",
      "Epoch 144/500\n",
      "7500/7500 [==============================] - 4s 543us/step - loss: 0.3786 - accuracy: 0.8372 - val_loss: 0.3784 - val_accuracy: 0.8367\n",
      "Epoch 145/500\n",
      "7500/7500 [==============================] - 5s 671us/step - loss: 0.3786 - accuracy: 0.8370 - val_loss: 0.3780 - val_accuracy: 0.8369\n",
      "Epoch 146/500\n",
      "7500/7500 [==============================] - 7s 966us/step - loss: 0.3785 - accuracy: 0.8375 - val_loss: 0.3781 - val_accuracy: 0.8377\n",
      "Epoch 147/500\n",
      "7500/7500 [==============================] - 5s 611us/step - loss: 0.3788 - accuracy: 0.8373 - val_loss: 0.3786 - val_accuracy: 0.8367\n",
      "Epoch 148/500\n",
      "7500/7500 [==============================] - 4s 594us/step - loss: 0.3785 - accuracy: 0.8370 - val_loss: 0.3809 - val_accuracy: 0.8354\n",
      "Epoch 149/500\n",
      "7500/7500 [==============================] - 4s 563us/step - loss: 0.3785 - accuracy: 0.8368 - val_loss: 0.3823 - val_accuracy: 0.8351\n",
      "Epoch 150/500\n",
      "7500/7500 [==============================] - 4s 569us/step - loss: 0.3785 - accuracy: 0.8369 - val_loss: 0.3848 - val_accuracy: 0.8334\n",
      "Epoch 151/500\n",
      "7500/7500 [==============================] - 4s 563us/step - loss: 0.3786 - accuracy: 0.8371 - val_loss: 0.3783 - val_accuracy: 0.8373\n",
      "Epoch 152/500\n",
      "7500/7500 [==============================] - 4s 544us/step - loss: 0.3786 - accuracy: 0.8373 - val_loss: 0.3778 - val_accuracy: 0.8374\n",
      "Epoch 153/500\n",
      "7500/7500 [==============================] - 5s 667us/step - loss: 0.3785 - accuracy: 0.8368 - val_loss: 0.3807 - val_accuracy: 0.8362\n",
      "Epoch 154/500\n",
      "7500/7500 [==============================] - 4s 577us/step - loss: 0.3784 - accuracy: 0.8373 - val_loss: 0.3825 - val_accuracy: 0.8357\n",
      "Epoch 155/500\n",
      "7500/7500 [==============================] - 4s 559us/step - loss: 0.3787 - accuracy: 0.8368 - val_loss: 0.3817 - val_accuracy: 0.8351\n",
      "Epoch 156/500\n",
      "7500/7500 [==============================] - 4s 559us/step - loss: 0.3785 - accuracy: 0.8371 - val_loss: 0.3809 - val_accuracy: 0.8356\n",
      "Epoch 157/500\n",
      "7500/7500 [==============================] - 4s 535us/step - loss: 0.3782 - accuracy: 0.8373 - val_loss: 0.3784 - val_accuracy: 0.8368\n",
      "Epoch 158/500\n",
      "7500/7500 [==============================] - 4s 545us/step - loss: 0.3785 - accuracy: 0.8373 - val_loss: 0.3780 - val_accuracy: 0.8378\n",
      "Epoch 159/500\n",
      "7500/7500 [==============================] - 4s 539us/step - loss: 0.3785 - accuracy: 0.8367 - val_loss: 0.3768 - val_accuracy: 0.8381\n",
      "Epoch 160/500\n",
      "7500/7500 [==============================] - 4s 564us/step - loss: 0.3784 - accuracy: 0.8367 - val_loss: 0.3780 - val_accuracy: 0.8375\n",
      "Epoch 161/500\n",
      "7500/7500 [==============================] - 4s 544us/step - loss: 0.3783 - accuracy: 0.8375 - val_loss: 0.3818 - val_accuracy: 0.8357\n",
      "Epoch 162/500\n",
      "7500/7500 [==============================] - 4s 568us/step - loss: 0.3783 - accuracy: 0.8371 - val_loss: 0.3908 - val_accuracy: 0.8322\n",
      "Epoch 163/500\n",
      "7500/7500 [==============================] - 6s 847us/step - loss: 0.3783 - accuracy: 0.8374 - val_loss: 0.3787 - val_accuracy: 0.8360\n",
      "Epoch 164/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 5s 704us/step - loss: 0.3783 - accuracy: 0.8373 - val_loss: 0.3794 - val_accuracy: 0.8366\n",
      "Epoch 165/500\n",
      "7500/7500 [==============================] - 4s 579us/step - loss: 0.3783 - accuracy: 0.8368 - val_loss: 0.3808 - val_accuracy: 0.8357\n",
      "Epoch 166/500\n",
      "7500/7500 [==============================] - 4s 553us/step - loss: 0.3786 - accuracy: 0.8371 - val_loss: 0.3774 - val_accuracy: 0.8375\n",
      "Epoch 167/500\n",
      "7500/7500 [==============================] - 4s 536us/step - loss: 0.3785 - accuracy: 0.8371 - val_loss: 0.3764 - val_accuracy: 0.8378\n",
      "Epoch 168/500\n",
      "7500/7500 [==============================] - 4s 588us/step - loss: 0.3783 - accuracy: 0.8374 - val_loss: 0.3770 - val_accuracy: 0.8375\n",
      "Epoch 169/500\n",
      "7500/7500 [==============================] - 4s 587us/step - loss: 0.3785 - accuracy: 0.8369 - val_loss: 0.3766 - val_accuracy: 0.8381\n",
      "Epoch 170/500\n",
      "7500/7500 [==============================] - 4s 556us/step - loss: 0.3785 - accuracy: 0.8374 - val_loss: 0.3785 - val_accuracy: 0.8368\n",
      "Epoch 171/500\n",
      "7500/7500 [==============================] - 5s 627us/step - loss: 0.3783 - accuracy: 0.8373 - val_loss: 0.3775 - val_accuracy: 0.8372\n",
      "Epoch 172/500\n",
      "7500/7500 [==============================] - 4s 544us/step - loss: 0.3784 - accuracy: 0.8373 - val_loss: 0.3807 - val_accuracy: 0.8363\n",
      "Epoch 173/500\n",
      "7500/7500 [==============================] - 4s 545us/step - loss: 0.3783 - accuracy: 0.8373 - val_loss: 0.3801 - val_accuracy: 0.8362\n",
      "Epoch 174/500\n",
      "7500/7500 [==============================] - 4s 541us/step - loss: 0.3782 - accuracy: 0.8372 - val_loss: 0.3887 - val_accuracy: 0.8314\n",
      "Epoch 175/500\n",
      "7500/7500 [==============================] - 6s 843us/step - loss: 0.3782 - accuracy: 0.8370 - val_loss: 0.3833 - val_accuracy: 0.8353\n",
      "Epoch 176/500\n",
      "7500/7500 [==============================] - 4s 584us/step - loss: 0.3784 - accuracy: 0.8372 - val_loss: 0.3769 - val_accuracy: 0.8376\n",
      "Epoch 177/500\n",
      "7500/7500 [==============================] - 6s 755us/step - loss: 0.3784 - accuracy: 0.8374 - val_loss: 0.3778 - val_accuracy: 0.8373\n",
      "Epoch 178/500\n",
      "7500/7500 [==============================] - 11s 1ms/step - loss: 0.3783 - accuracy: 0.8372 - val_loss: 0.3773 - val_accuracy: 0.8379\n",
      "Epoch 179/500\n",
      "7500/7500 [==============================] - 17s 2ms/step - loss: 0.3783 - accuracy: 0.8368 - val_loss: 0.3876 - val_accuracy: 0.8331\n",
      "Epoch 180/500\n",
      "7500/7500 [==============================] - 12s 2ms/step - loss: 0.3783 - accuracy: 0.8371 - val_loss: 0.3776 - val_accuracy: 0.8367\n",
      "Epoch 181/500\n",
      "7500/7500 [==============================] - 7s 956us/step - loss: 0.3781 - accuracy: 0.8374 - val_loss: 0.3835 - val_accuracy: 0.8343\n",
      "Epoch 182/500\n",
      "7500/7500 [==============================] - 4s 593us/step - loss: 0.3781 - accuracy: 0.8371 - val_loss: 0.3805 - val_accuracy: 0.8360\n",
      "Epoch 183/500\n",
      "7500/7500 [==============================] - 4s 550us/step - loss: 0.3780 - accuracy: 0.8377 - val_loss: 0.3786 - val_accuracy: 0.8375\n",
      "Epoch 184/500\n",
      "7500/7500 [==============================] - 4s 570us/step - loss: 0.3783 - accuracy: 0.8372 - val_loss: 0.3791 - val_accuracy: 0.8363\n",
      "Epoch 185/500\n",
      "7500/7500 [==============================] - 4s 521us/step - loss: 0.3782 - accuracy: 0.8374 - val_loss: 0.3774 - val_accuracy: 0.8380\n",
      "Epoch 186/500\n",
      "7500/7500 [==============================] - 4s 546us/step - loss: 0.3782 - accuracy: 0.8372 - val_loss: 0.3770 - val_accuracy: 0.8379\n",
      "Epoch 187/500\n",
      "7500/7500 [==============================] - 4s 528us/step - loss: 0.3781 - accuracy: 0.8375 - val_loss: 0.3788 - val_accuracy: 0.8370\n",
      "Epoch 188/500\n",
      "7500/7500 [==============================] - 4s 518us/step - loss: 0.3782 - accuracy: 0.8370 - val_loss: 0.3779 - val_accuracy: 0.8376\n",
      "Epoch 189/500\n",
      "7500/7500 [==============================] - 4s 543us/step - loss: 0.3783 - accuracy: 0.8369 - val_loss: 0.3769 - val_accuracy: 0.8378\n",
      "Epoch 190/500\n",
      "7500/7500 [==============================] - 4s 517us/step - loss: 0.3781 - accuracy: 0.8374 - val_loss: 0.3781 - val_accuracy: 0.8374\n",
      "Epoch 191/500\n",
      "7500/7500 [==============================] - 4s 546us/step - loss: 0.3782 - accuracy: 0.8370 - val_loss: 0.3782 - val_accuracy: 0.8375\n",
      "Epoch 192/500\n",
      "7500/7500 [==============================] - 4s 592us/step - loss: 0.3782 - accuracy: 0.8377 - val_loss: 0.3824 - val_accuracy: 0.8356\n",
      "Epoch 193/500\n",
      "7500/7500 [==============================] - 4s 547us/step - loss: 0.3782 - accuracy: 0.8376 - val_loss: 0.3777 - val_accuracy: 0.8377\n",
      "Epoch 194/500\n",
      "7500/7500 [==============================] - 4s 529us/step - loss: 0.3782 - accuracy: 0.8373 - val_loss: 0.3790 - val_accuracy: 0.8375\n",
      "Epoch 195/500\n",
      "7500/7500 [==============================] - 4s 546us/step - loss: 0.3780 - accuracy: 0.8373 - val_loss: 0.3785 - val_accuracy: 0.8375\n",
      "Epoch 196/500\n",
      "7500/7500 [==============================] - 4s 556us/step - loss: 0.3782 - accuracy: 0.8370 - val_loss: 0.3776 - val_accuracy: 0.8371\n",
      "Epoch 197/500\n",
      "7500/7500 [==============================] - 4s 544us/step - loss: 0.3778 - accuracy: 0.8372 - val_loss: 0.3808 - val_accuracy: 0.8359\n",
      "Epoch 198/500\n",
      "7500/7500 [==============================] - 4s 521us/step - loss: 0.3782 - accuracy: 0.8377 - val_loss: 0.3922 - val_accuracy: 0.8312\n",
      "Epoch 199/500\n",
      "7500/7500 [==============================] - 4s 546us/step - loss: 0.3781 - accuracy: 0.8373 - val_loss: 0.3781 - val_accuracy: 0.8377\n",
      "Epoch 200/500\n",
      "7500/7500 [==============================] - 4s 531us/step - loss: 0.3782 - accuracy: 0.8374 - val_loss: 0.3801 - val_accuracy: 0.8363\n",
      "Epoch 201/500\n",
      "7500/7500 [==============================] - 4s 532us/step - loss: 0.3780 - accuracy: 0.8371 - val_loss: 0.3835 - val_accuracy: 0.8348\n",
      "Epoch 202/500\n",
      "7500/7500 [==============================] - 4s 540us/step - loss: 0.3781 - accuracy: 0.8373 - val_loss: 0.3781 - val_accuracy: 0.8373\n",
      "Epoch 203/500\n",
      "7500/7500 [==============================] - 4s 549us/step - loss: 0.3781 - accuracy: 0.8373 - val_loss: 0.3785 - val_accuracy: 0.8374\n",
      "Epoch 204/500\n",
      "7500/7500 [==============================] - 4s 541us/step - loss: 0.3779 - accuracy: 0.8374 - val_loss: 0.3812 - val_accuracy: 0.8347\n",
      "Epoch 205/500\n",
      "7500/7500 [==============================] - 4s 553us/step - loss: 0.3777 - accuracy: 0.8376 - val_loss: 0.3813 - val_accuracy: 0.8362\n",
      "Epoch 206/500\n",
      "7500/7500 [==============================] - 4s 541us/step - loss: 0.3779 - accuracy: 0.8375 - val_loss: 0.3861 - val_accuracy: 0.8335\n",
      "Epoch 207/500\n",
      "7500/7500 [==============================] - 4s 537us/step - loss: 0.3779 - accuracy: 0.8371 - val_loss: 0.3809 - val_accuracy: 0.8352\n",
      "Epoch 208/500\n",
      "7500/7500 [==============================] - 4s 560us/step - loss: 0.3779 - accuracy: 0.8371 - val_loss: 0.3771 - val_accuracy: 0.8379\n",
      "Epoch 209/500\n",
      "7500/7500 [==============================] - 4s 556us/step - loss: 0.3781 - accuracy: 0.8373 - val_loss: 0.3782 - val_accuracy: 0.8376\n",
      "Epoch 210/500\n",
      "7500/7500 [==============================] - 4s 536us/step - loss: 0.3780 - accuracy: 0.8370 - val_loss: 0.3782 - val_accuracy: 0.8373\n",
      "Epoch 211/500\n",
      "7500/7500 [==============================] - 4s 528us/step - loss: 0.3779 - accuracy: 0.8373 - val_loss: 0.3782 - val_accuracy: 0.8367\n",
      "Epoch 212/500\n",
      "7500/7500 [==============================] - 4s 519us/step - loss: 0.3781 - accuracy: 0.8370 - val_loss: 0.3814 - val_accuracy: 0.8353\n",
      "Epoch 213/500\n",
      "7500/7500 [==============================] - 5s 665us/step - loss: 0.3780 - accuracy: 0.8371 - val_loss: 0.3850 - val_accuracy: 0.8328\n",
      "Epoch 214/500\n",
      "7500/7500 [==============================] - 4s 564us/step - loss: 0.3779 - accuracy: 0.8374 - val_loss: 0.3892 - val_accuracy: 0.8313\n",
      "Epoch 215/500\n",
      "7500/7500 [==============================] - 4s 531us/step - loss: 0.3779 - accuracy: 0.8373 - val_loss: 0.3831 - val_accuracy: 0.8346\n",
      "Epoch 216/500\n",
      "7500/7500 [==============================] - 4s 534us/step - loss: 0.3779 - accuracy: 0.8372 - val_loss: 0.3784 - val_accuracy: 0.8368\n",
      "Epoch 217/500\n",
      "7500/7500 [==============================] - 4s 525us/step - loss: 0.3780 - accuracy: 0.8374 - val_loss: 0.3771 - val_accuracy: 0.8374\n",
      "Epoch 218/500\n",
      "7500/7500 [==============================] - 4s 529us/step - loss: 0.3779 - accuracy: 0.8373 - val_loss: 0.3781 - val_accuracy: 0.8372\n",
      "Epoch 219/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 4s 540us/step - loss: 0.3778 - accuracy: 0.8373 - val_loss: 0.3773 - val_accuracy: 0.8381\n",
      "Epoch 220/500\n",
      "7500/7500 [==============================] - 4s 534us/step - loss: 0.3779 - accuracy: 0.8378 - val_loss: 0.3771 - val_accuracy: 0.8380\n",
      "Epoch 221/500\n",
      "7500/7500 [==============================] - 4s 521us/step - loss: 0.3778 - accuracy: 0.8374 - val_loss: 0.3818 - val_accuracy: 0.8357\n",
      "Epoch 222/500\n",
      "7500/7500 [==============================] - 4s 513us/step - loss: 0.3780 - accuracy: 0.8372 - val_loss: 0.3778 - val_accuracy: 0.8370\n",
      "Epoch 223/500\n",
      "7500/7500 [==============================] - 4s 516us/step - loss: 0.3777 - accuracy: 0.8371 - val_loss: 0.3855 - val_accuracy: 0.8338\n",
      "Epoch 224/500\n",
      "7500/7500 [==============================] - 4s 550us/step - loss: 0.3779 - accuracy: 0.8375 - val_loss: 0.3777 - val_accuracy: 0.8382\n",
      "Epoch 225/500\n",
      "7500/7500 [==============================] - 4s 522us/step - loss: 0.3778 - accuracy: 0.8377 - val_loss: 0.3811 - val_accuracy: 0.8366\n",
      "Epoch 226/500\n",
      "7500/7500 [==============================] - 4s 522us/step - loss: 0.3778 - accuracy: 0.8379 - val_loss: 0.3774 - val_accuracy: 0.8376\n",
      "Epoch 227/500\n",
      "7500/7500 [==============================] - 4s 524us/step - loss: 0.3780 - accuracy: 0.8371 - val_loss: 0.3777 - val_accuracy: 0.8371\n",
      "Epoch 228/500\n",
      "7500/7500 [==============================] - 4s 524us/step - loss: 0.3779 - accuracy: 0.8370 - val_loss: 0.3806 - val_accuracy: 0.8360\n",
      "Epoch 229/500\n",
      "7500/7500 [==============================] - 4s 515us/step - loss: 0.3778 - accuracy: 0.8377 - val_loss: 0.3778 - val_accuracy: 0.8366\n",
      "Epoch 230/500\n",
      "7500/7500 [==============================] - 4s 529us/step - loss: 0.3779 - accuracy: 0.8373 - val_loss: 0.3774 - val_accuracy: 0.8369\n",
      "Epoch 231/500\n",
      "7500/7500 [==============================] - 4s 514us/step - loss: 0.3780 - accuracy: 0.8373 - val_loss: 0.3776 - val_accuracy: 0.8372\n",
      "Epoch 232/500\n",
      "7500/7500 [==============================] - 4s 513us/step - loss: 0.3776 - accuracy: 0.8374 - val_loss: 0.3788 - val_accuracy: 0.8376\n",
      "Epoch 233/500\n",
      "7500/7500 [==============================] - 4s 512us/step - loss: 0.3778 - accuracy: 0.8375 - val_loss: 0.3769 - val_accuracy: 0.8378\n",
      "Epoch 234/500\n",
      "7500/7500 [==============================] - 4s 513us/step - loss: 0.3779 - accuracy: 0.8377 - val_loss: 0.3786 - val_accuracy: 0.8376\n",
      "Epoch 235/500\n",
      "7500/7500 [==============================] - 4s 513us/step - loss: 0.3776 - accuracy: 0.8380 - val_loss: 0.4013 - val_accuracy: 0.8253\n",
      "Epoch 236/500\n",
      "7500/7500 [==============================] - 4s 546us/step - loss: 0.3779 - accuracy: 0.8373 - val_loss: 0.3776 - val_accuracy: 0.8369\n",
      "Epoch 237/500\n",
      "7500/7500 [==============================] - 4s 529us/step - loss: 0.3779 - accuracy: 0.8376 - val_loss: 0.3769 - val_accuracy: 0.8374\n",
      "Epoch 238/500\n",
      "7500/7500 [==============================] - 4s 530us/step - loss: 0.3778 - accuracy: 0.8372 - val_loss: 0.3784 - val_accuracy: 0.8368\n",
      "Epoch 239/500\n",
      "7500/7500 [==============================] - 4s 537us/step - loss: 0.3778 - accuracy: 0.8375 - val_loss: 0.3769 - val_accuracy: 0.8373\n",
      "Epoch 240/500\n",
      "7500/7500 [==============================] - 4s 535us/step - loss: 0.3777 - accuracy: 0.8378 - val_loss: 0.3768 - val_accuracy: 0.8376\n",
      "Epoch 241/500\n",
      "7500/7500 [==============================] - 4s 538us/step - loss: 0.3778 - accuracy: 0.8373 - val_loss: 0.3766 - val_accuracy: 0.8378\n",
      "Epoch 242/500\n",
      "7500/7500 [==============================] - 4s 518us/step - loss: 0.3777 - accuracy: 0.8375 - val_loss: 0.3791 - val_accuracy: 0.8367\n",
      "Epoch 243/500\n",
      "7500/7500 [==============================] - 4s 531us/step - loss: 0.3777 - accuracy: 0.8374 - val_loss: 0.3776 - val_accuracy: 0.8374\n",
      "Epoch 244/500\n",
      "7500/7500 [==============================] - 4s 527us/step - loss: 0.3777 - accuracy: 0.8373 - val_loss: 0.3775 - val_accuracy: 0.8369\n",
      "Epoch 245/500\n",
      "7500/7500 [==============================] - 4s 524us/step - loss: 0.3777 - accuracy: 0.8372 - val_loss: 0.3885 - val_accuracy: 0.8325\n",
      "Epoch 246/500\n",
      "7500/7500 [==============================] - 4s 523us/step - loss: 0.3778 - accuracy: 0.8373 - val_loss: 0.3789 - val_accuracy: 0.8364\n",
      "Epoch 247/500\n",
      "7500/7500 [==============================] - 4s 537us/step - loss: 0.3777 - accuracy: 0.8375 - val_loss: 0.3771 - val_accuracy: 0.8376\n",
      "Epoch 248/500\n",
      "7500/7500 [==============================] - 4s 538us/step - loss: 0.3777 - accuracy: 0.8374 - val_loss: 0.3775 - val_accuracy: 0.8369\n",
      "Epoch 249/500\n",
      "7500/7500 [==============================] - 4s 554us/step - loss: 0.3777 - accuracy: 0.8373 - val_loss: 0.3777 - val_accuracy: 0.8376\n",
      "Epoch 250/500\n",
      "7500/7500 [==============================] - 5s 604us/step - loss: 0.3775 - accuracy: 0.8374 - val_loss: 0.4049 - val_accuracy: 0.8238\n",
      "Epoch 251/500\n",
      "7500/7500 [==============================] - 8s 1ms/step - loss: 0.3775 - accuracy: 0.8376 - val_loss: 0.3774 - val_accuracy: 0.8373\n",
      "Epoch 252/500\n",
      "7500/7500 [==============================] - 9s 1ms/step - loss: 0.3777 - accuracy: 0.8375 - val_loss: 0.3770 - val_accuracy: 0.8375\n",
      "Epoch 253/500\n",
      "7500/7500 [==============================] - 4s 527us/step - loss: 0.3775 - accuracy: 0.8370 - val_loss: 0.3813 - val_accuracy: 0.8354\n",
      "Epoch 254/500\n",
      "7500/7500 [==============================] - 6s 793us/step - loss: 0.3777 - accuracy: 0.8372 - val_loss: 0.3796 - val_accuracy: 0.8363\n",
      "Epoch 255/500\n",
      "7500/7500 [==============================] - 7s 930us/step - loss: 0.3776 - accuracy: 0.8373 - val_loss: 0.3798 - val_accuracy: 0.8354\n",
      "Epoch 256/500\n",
      "7500/7500 [==============================] - 8s 1ms/step - loss: 0.3777 - accuracy: 0.8374 - val_loss: 0.3776 - val_accuracy: 0.8377\n",
      "Epoch 257/500\n",
      "7500/7500 [==============================] - 5s 647us/step - loss: 0.3777 - accuracy: 0.8373 - val_loss: 0.3803 - val_accuracy: 0.8359\n",
      "Epoch 258/500\n",
      "7500/7500 [==============================] - 12s 2ms/step - loss: 0.3777 - accuracy: 0.8374 - val_loss: 0.3780 - val_accuracy: 0.8374\n",
      "Epoch 259/500\n",
      "7500/7500 [==============================] - 8s 1ms/step - loss: 0.3777 - accuracy: 0.8375 - val_loss: 0.3770 - val_accuracy: 0.8372\n",
      "Epoch 260/500\n",
      "7500/7500 [==============================] - 6s 859us/step - loss: 0.3777 - accuracy: 0.8376 - val_loss: 0.3795 - val_accuracy: 0.8363\n",
      "Epoch 261/500\n",
      "7500/7500 [==============================] - 6s 846us/step - loss: 0.3777 - accuracy: 0.8378 - val_loss: 0.3767 - val_accuracy: 0.8380\n",
      "Epoch 262/500\n",
      "7500/7500 [==============================] - 6s 774us/step - loss: 0.3777 - accuracy: 0.8372 - val_loss: 0.3767 - val_accuracy: 0.8379\n",
      "Epoch 263/500\n",
      "7500/7500 [==============================] - 5s 609us/step - loss: 0.3775 - accuracy: 0.8377 - val_loss: 0.3775 - val_accuracy: 0.8379\n",
      "Epoch 264/500\n",
      "7500/7500 [==============================] - 5s 709us/step - loss: 0.3776 - accuracy: 0.8376 - val_loss: 0.3765 - val_accuracy: 0.8384\n",
      "Epoch 265/500\n",
      "7500/7500 [==============================] - 4s 579us/step - loss: 0.3775 - accuracy: 0.8380 - val_loss: 0.3767 - val_accuracy: 0.8381\n",
      "Epoch 266/500\n",
      "7500/7500 [==============================] - 5s 657us/step - loss: 0.3775 - accuracy: 0.8377 - val_loss: 0.3773 - val_accuracy: 0.8377\n",
      "Epoch 267/500\n",
      "7500/7500 [==============================] - 4s 540us/step - loss: 0.3776 - accuracy: 0.8382 - val_loss: 0.3766 - val_accuracy: 0.8379\n",
      "Epoch 268/500\n",
      "7500/7500 [==============================] - 4s 544us/step - loss: 0.3777 - accuracy: 0.8376 - val_loss: 0.3785 - val_accuracy: 0.8369\n",
      "Epoch 269/500\n",
      "7500/7500 [==============================] - 4s 542us/step - loss: 0.3775 - accuracy: 0.8373 - val_loss: 0.3775 - val_accuracy: 0.8373\n",
      "Epoch 270/500\n",
      "7500/7500 [==============================] - 4s 542us/step - loss: 0.3776 - accuracy: 0.8373 - val_loss: 0.3773 - val_accuracy: 0.8375\n",
      "Epoch 271/500\n",
      "7500/7500 [==============================] - 4s 542us/step - loss: 0.3776 - accuracy: 0.8375 - val_loss: 0.3804 - val_accuracy: 0.8369\n",
      "Epoch 272/500\n",
      "7500/7500 [==============================] - 4s 546us/step - loss: 0.3775 - accuracy: 0.8374 - val_loss: 0.3906 - val_accuracy: 0.8302\n",
      "Epoch 273/500\n",
      "7500/7500 [==============================] - 4s 543us/step - loss: 0.3773 - accuracy: 0.8378 - val_loss: 0.3780 - val_accuracy: 0.8374\n",
      "Epoch 274/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 4s 542us/step - loss: 0.3776 - accuracy: 0.8373 - val_loss: 0.3798 - val_accuracy: 0.8370\n",
      "Epoch 275/500\n",
      "7500/7500 [==============================] - 4s 540us/step - loss: 0.3776 - accuracy: 0.8379 - val_loss: 0.3784 - val_accuracy: 0.8374\n",
      "Epoch 276/500\n",
      "7500/7500 [==============================] - 4s 556us/step - loss: 0.3776 - accuracy: 0.8376 - val_loss: 0.3798 - val_accuracy: 0.8367\n",
      "Epoch 277/500\n",
      "7500/7500 [==============================] - 4s 551us/step - loss: 0.3776 - accuracy: 0.8375 - val_loss: 0.3781 - val_accuracy: 0.8378\n",
      "Epoch 278/500\n",
      "7500/7500 [==============================] - 4s 540us/step - loss: 0.3776 - accuracy: 0.8377 - val_loss: 0.3800 - val_accuracy: 0.8370\n",
      "Epoch 279/500\n",
      "7500/7500 [==============================] - 4s 566us/step - loss: 0.3776 - accuracy: 0.8374 - val_loss: 0.3785 - val_accuracy: 0.8368\n",
      "Epoch 280/500\n",
      "7500/7500 [==============================] - 4s 552us/step - loss: 0.3776 - accuracy: 0.8374 - val_loss: 0.3770 - val_accuracy: 0.8374\n",
      "Epoch 281/500\n",
      "7500/7500 [==============================] - 4s 541us/step - loss: 0.3774 - accuracy: 0.8379 - val_loss: 0.3773 - val_accuracy: 0.8378\n",
      "Epoch 282/500\n",
      "7500/7500 [==============================] - 4s 541us/step - loss: 0.3776 - accuracy: 0.8375 - val_loss: 0.3772 - val_accuracy: 0.8379\n",
      "Epoch 283/500\n",
      "7500/7500 [==============================] - 4s 544us/step - loss: 0.3775 - accuracy: 0.8372 - val_loss: 0.3851 - val_accuracy: 0.8335\n",
      "Epoch 284/500\n",
      "7500/7500 [==============================] - 4s 549us/step - loss: 0.3774 - accuracy: 0.8378 - val_loss: 0.3774 - val_accuracy: 0.8375\n",
      "Epoch 285/500\n",
      "7500/7500 [==============================] - 4s 543us/step - loss: 0.3775 - accuracy: 0.8376 - val_loss: 0.3802 - val_accuracy: 0.8370\n",
      "Epoch 286/500\n",
      "7500/7500 [==============================] - 4s 547us/step - loss: 0.3774 - accuracy: 0.8373 - val_loss: 0.4053 - val_accuracy: 0.8263\n",
      "Epoch 287/500\n",
      "7500/7500 [==============================] - 4s 550us/step - loss: 0.3772 - accuracy: 0.8376 - val_loss: 0.3765 - val_accuracy: 0.8380\n",
      "Epoch 288/500\n",
      "7500/7500 [==============================] - 4s 554us/step - loss: 0.3775 - accuracy: 0.8374 - val_loss: 0.3792 - val_accuracy: 0.8364\n",
      "Epoch 289/500\n",
      "7500/7500 [==============================] - 4s 562us/step - loss: 0.3776 - accuracy: 0.8372 - val_loss: 0.3767 - val_accuracy: 0.8382\n",
      "Epoch 290/500\n",
      "7500/7500 [==============================] - 4s 547us/step - loss: 0.3773 - accuracy: 0.8379 - val_loss: 0.3782 - val_accuracy: 0.8366\n",
      "Epoch 291/500\n",
      "7500/7500 [==============================] - 4s 545us/step - loss: 0.3775 - accuracy: 0.8375 - val_loss: 0.3768 - val_accuracy: 0.8377\n",
      "Epoch 292/500\n",
      "7500/7500 [==============================] - 4s 544us/step - loss: 0.3776 - accuracy: 0.8377 - val_loss: 0.3802 - val_accuracy: 0.8359\n",
      "Epoch 293/500\n",
      "7500/7500 [==============================] - 4s 545us/step - loss: 0.3773 - accuracy: 0.8374 - val_loss: 0.3765 - val_accuracy: 0.8382\n",
      "Epoch 294/500\n",
      "7500/7500 [==============================] - 4s 542us/step - loss: 0.3773 - accuracy: 0.8378 - val_loss: 0.3784 - val_accuracy: 0.8374\n",
      "Epoch 295/500\n",
      "7500/7500 [==============================] - 4s 567us/step - loss: 0.3774 - accuracy: 0.8373 - val_loss: 0.3773 - val_accuracy: 0.8372\n",
      "Epoch 296/500\n",
      "7500/7500 [==============================] - 4s 557us/step - loss: 0.3772 - accuracy: 0.8374 - val_loss: 0.3815 - val_accuracy: 0.8349\n",
      "Epoch 297/500\n",
      "7500/7500 [==============================] - 4s 543us/step - loss: 0.3774 - accuracy: 0.8375 - val_loss: 0.3844 - val_accuracy: 0.8335\n",
      "Epoch 298/500\n",
      "7500/7500 [==============================] - 4s 541us/step - loss: 0.3772 - accuracy: 0.8373 - val_loss: 0.3801 - val_accuracy: 0.8364\n",
      "Epoch 299/500\n",
      "7500/7500 [==============================] - 4s 542us/step - loss: 0.3774 - accuracy: 0.8371 - val_loss: 0.3821 - val_accuracy: 0.8349\n",
      "Epoch 300/500\n",
      "7500/7500 [==============================] - 4s 542us/step - loss: 0.3773 - accuracy: 0.8380 - val_loss: 0.3787 - val_accuracy: 0.8368\n",
      "Epoch 301/500\n",
      "7500/7500 [==============================] - 4s 541us/step - loss: 0.3775 - accuracy: 0.8373 - val_loss: 0.3771 - val_accuracy: 0.8378\n",
      "Epoch 302/500\n",
      "7500/7500 [==============================] - 4s 542us/step - loss: 0.3773 - accuracy: 0.8373 - val_loss: 0.3793 - val_accuracy: 0.8366\n",
      "Epoch 303/500\n",
      "7500/7500 [==============================] - 4s 542us/step - loss: 0.3773 - accuracy: 0.8380 - val_loss: 0.3781 - val_accuracy: 0.8371\n",
      "Epoch 304/500\n",
      "7500/7500 [==============================] - 4s 541us/step - loss: 0.3774 - accuracy: 0.8376 - val_loss: 0.3767 - val_accuracy: 0.8378\n",
      "Epoch 305/500\n",
      "7500/7500 [==============================] - 4s 540us/step - loss: 0.3772 - accuracy: 0.8379 - val_loss: 0.3801 - val_accuracy: 0.8354\n",
      "Epoch 306/500\n",
      "7500/7500 [==============================] - 4s 544us/step - loss: 0.3772 - accuracy: 0.8375 - val_loss: 0.3767 - val_accuracy: 0.8380\n",
      "Epoch 307/500\n",
      "7500/7500 [==============================] - 4s 542us/step - loss: 0.3772 - accuracy: 0.8374 - val_loss: 0.3775 - val_accuracy: 0.8375\n",
      "Epoch 308/500\n",
      "7500/7500 [==============================] - 4s 545us/step - loss: 0.3773 - accuracy: 0.8374 - val_loss: 0.3770 - val_accuracy: 0.8374\n",
      "Epoch 309/500\n",
      "7500/7500 [==============================] - 4s 541us/step - loss: 0.3772 - accuracy: 0.8375 - val_loss: 0.3817 - val_accuracy: 0.8356\n",
      "Epoch 310/500\n",
      "7500/7500 [==============================] - 4s 542us/step - loss: 0.3774 - accuracy: 0.8381 - val_loss: 0.3821 - val_accuracy: 0.8354\n",
      "Epoch 311/500\n",
      "7500/7500 [==============================] - 4s 581us/step - loss: 0.3775 - accuracy: 0.8375 - val_loss: 0.3824 - val_accuracy: 0.8354\n",
      "Epoch 312/500\n",
      "7500/7500 [==============================] - 4s 556us/step - loss: 0.3774 - accuracy: 0.8375 - val_loss: 0.3811 - val_accuracy: 0.8364\n",
      "Epoch 313/500\n",
      "7500/7500 [==============================] - 4s 539us/step - loss: 0.3772 - accuracy: 0.8382 - val_loss: 0.3793 - val_accuracy: 0.8363\n",
      "Epoch 314/500\n",
      "7500/7500 [==============================] - 4s 544us/step - loss: 0.3773 - accuracy: 0.8375 - val_loss: 0.3815 - val_accuracy: 0.8360\n",
      "Epoch 315/500\n",
      "7500/7500 [==============================] - 4s 543us/step - loss: 0.3771 - accuracy: 0.8379 - val_loss: 0.3790 - val_accuracy: 0.8371\n",
      "Epoch 316/500\n",
      "7500/7500 [==============================] - 4s 540us/step - loss: 0.3773 - accuracy: 0.8373 - val_loss: 0.3786 - val_accuracy: 0.8371\n",
      "Epoch 317/500\n",
      "7500/7500 [==============================] - 4s 567us/step - loss: 0.3774 - accuracy: 0.8375 - val_loss: 0.3760 - val_accuracy: 0.8379\n",
      "Epoch 318/500\n",
      "7500/7500 [==============================] - 4s 539us/step - loss: 0.3770 - accuracy: 0.8377 - val_loss: 0.3788 - val_accuracy: 0.8370\n",
      "Epoch 319/500\n",
      "7500/7500 [==============================] - 4s 542us/step - loss: 0.3772 - accuracy: 0.8375 - val_loss: 0.3850 - val_accuracy: 0.8330\n",
      "Epoch 320/500\n",
      "7500/7500 [==============================] - 4s 541us/step - loss: 0.3772 - accuracy: 0.8378 - val_loss: 0.3769 - val_accuracy: 0.8370\n",
      "Epoch 321/500\n",
      "7500/7500 [==============================] - 4s 540us/step - loss: 0.3773 - accuracy: 0.8376 - val_loss: 0.3782 - val_accuracy: 0.8374\n",
      "Epoch 322/500\n",
      "7500/7500 [==============================] - 4s 540us/step - loss: 0.3773 - accuracy: 0.8374 - val_loss: 0.3789 - val_accuracy: 0.8372\n",
      "Epoch 323/500\n",
      "7500/7500 [==============================] - 4s 575us/step - loss: 0.3772 - accuracy: 0.8374 - val_loss: 0.3758 - val_accuracy: 0.8381\n",
      "Epoch 324/500\n",
      "7500/7500 [==============================] - 4s 540us/step - loss: 0.3771 - accuracy: 0.8377 - val_loss: 0.3821 - val_accuracy: 0.8342\n",
      "Epoch 325/500\n",
      "7500/7500 [==============================] - 4s 563us/step - loss: 0.3773 - accuracy: 0.8379 - val_loss: 0.3785 - val_accuracy: 0.8372\n",
      "Epoch 326/500\n",
      "7500/7500 [==============================] - 4s 543us/step - loss: 0.3772 - accuracy: 0.8376 - val_loss: 0.3771 - val_accuracy: 0.8375\n",
      "Epoch 327/500\n",
      "7500/7500 [==============================] - 4s 556us/step - loss: 0.3774 - accuracy: 0.8376 - val_loss: 0.3766 - val_accuracy: 0.8377\n",
      "Epoch 328/500\n",
      "7500/7500 [==============================] - 4s 570us/step - loss: 0.3772 - accuracy: 0.8377 - val_loss: 0.3767 - val_accuracy: 0.8380\n",
      "Epoch 329/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 4s 542us/step - loss: 0.3770 - accuracy: 0.8376 - val_loss: 0.3784 - val_accuracy: 0.8377\n",
      "Epoch 330/500\n",
      "7500/7500 [==============================] - 4s 539us/step - loss: 0.3772 - accuracy: 0.8375 - val_loss: 0.3787 - val_accuracy: 0.8375\n",
      "Epoch 331/500\n",
      "7500/7500 [==============================] - 4s 559us/step - loss: 0.3772 - accuracy: 0.8374 - val_loss: 0.3757 - val_accuracy: 0.8381\n",
      "Epoch 332/500\n",
      "7500/7500 [==============================] - 4s 548us/step - loss: 0.3772 - accuracy: 0.8379 - val_loss: 0.3774 - val_accuracy: 0.8373\n",
      "Epoch 333/500\n",
      "7500/7500 [==============================] - 4s 544us/step - loss: 0.3770 - accuracy: 0.8378 - val_loss: 0.3777 - val_accuracy: 0.8367\n",
      "Epoch 334/500\n",
      "7500/7500 [==============================] - 47s 6ms/step - loss: 0.3773 - accuracy: 0.8377 - val_loss: 0.3804 - val_accuracy: 0.8360\n",
      "Epoch 335/500\n",
      "7500/7500 [==============================] - 8s 1ms/step - loss: 0.3772 - accuracy: 0.8378 - val_loss: 0.3765 - val_accuracy: 0.8378\n",
      "Epoch 336/500\n",
      "7500/7500 [==============================] - 4s 590us/step - loss: 0.3772 - accuracy: 0.8375 - val_loss: 0.3792 - val_accuracy: 0.8361\n",
      "Epoch 337/500\n",
      "7500/7500 [==============================] - 5s 652us/step - loss: 0.3772 - accuracy: 0.8373 - val_loss: 0.3778 - val_accuracy: 0.8374\n",
      "Epoch 338/500\n",
      "7500/7500 [==============================] - 5s 606us/step - loss: 0.3771 - accuracy: 0.8373 - val_loss: 0.3776 - val_accuracy: 0.8376\n",
      "Epoch 339/500\n",
      "7500/7500 [==============================] - 4s 586us/step - loss: 0.3770 - accuracy: 0.8377 - val_loss: 0.3792 - val_accuracy: 0.8362\n",
      "Epoch 340/500\n",
      "7500/7500 [==============================] - 5s 601us/step - loss: 0.3769 - accuracy: 0.8380 - val_loss: 0.3782 - val_accuracy: 0.8374\n",
      "Epoch 341/500\n",
      "7500/7500 [==============================] - 5s 718us/step - loss: 0.3771 - accuracy: 0.8376 - val_loss: 0.3773 - val_accuracy: 0.8372\n",
      "Epoch 342/500\n",
      "7500/7500 [==============================] - 5s 664us/step - loss: 0.3771 - accuracy: 0.8376 - val_loss: 0.4040 - val_accuracy: 0.8240\n",
      "Epoch 343/500\n",
      "7500/7500 [==============================] - 5s 726us/step - loss: 0.3771 - accuracy: 0.8376 - val_loss: 0.3785 - val_accuracy: 0.8357\n",
      "Epoch 344/500\n",
      "7500/7500 [==============================] - 5s 641us/step - loss: 0.3770 - accuracy: 0.8380 - val_loss: 0.3796 - val_accuracy: 0.8360\n",
      "Epoch 345/500\n",
      "7500/7500 [==============================] - 5s 643us/step - loss: 0.3773 - accuracy: 0.8372 - val_loss: 0.3775 - val_accuracy: 0.8377\n",
      "Epoch 346/500\n",
      "7500/7500 [==============================] - 6s 804us/step - loss: 0.3770 - accuracy: 0.8378 - val_loss: 0.3768 - val_accuracy: 0.8378\n",
      "Epoch 347/500\n",
      "7500/7500 [==============================] - 6s 765us/step - loss: 0.3770 - accuracy: 0.8373 - val_loss: 0.3769 - val_accuracy: 0.8382\n",
      "Epoch 348/500\n",
      "7500/7500 [==============================] - 5s 695us/step - loss: 0.3768 - accuracy: 0.8375 - val_loss: 0.3781 - val_accuracy: 0.8375\n",
      "Epoch 349/500\n",
      "7500/7500 [==============================] - 5s 694us/step - loss: 0.3769 - accuracy: 0.8381 - val_loss: 0.3761 - val_accuracy: 0.8380\n",
      "Epoch 350/500\n",
      "7500/7500 [==============================] - 5s 683us/step - loss: 0.3771 - accuracy: 0.8375 - val_loss: 0.3823 - val_accuracy: 0.8350\n",
      "Epoch 351/500\n",
      "7500/7500 [==============================] - 6s 750us/step - loss: 0.3773 - accuracy: 0.8375 - val_loss: 0.3778 - val_accuracy: 0.8368\n",
      "Epoch 352/500\n",
      "7500/7500 [==============================] - 6s 787us/step - loss: 0.3770 - accuracy: 0.8376 - val_loss: 0.3776 - val_accuracy: 0.8372\n",
      "Epoch 353/500\n",
      "7500/7500 [==============================] - 7s 966us/step - loss: 0.3771 - accuracy: 0.8378 - val_loss: 0.3781 - val_accuracy: 0.8369\n",
      "Epoch 354/500\n",
      "7500/7500 [==============================] - 8s 1ms/step - loss: 0.3772 - accuracy: 0.8377 - val_loss: 0.3881 - val_accuracy: 0.8328\n",
      "Epoch 355/500\n",
      "7500/7500 [==============================] - 7s 880us/step - loss: 0.3770 - accuracy: 0.8378 - val_loss: 0.3787 - val_accuracy: 0.8366\n",
      "Epoch 356/500\n",
      "7500/7500 [==============================] - 5s 685us/step - loss: 0.3770 - accuracy: 0.8377 - val_loss: 0.3864 - val_accuracy: 0.8334\n",
      "Epoch 357/500\n",
      "7500/7500 [==============================] - 5s 646us/step - loss: 0.3769 - accuracy: 0.8376 - val_loss: 0.3784 - val_accuracy: 0.8371\n",
      "Epoch 358/500\n",
      "7500/7500 [==============================] - 5s 670us/step - loss: 0.3771 - accuracy: 0.8379 - val_loss: 0.3815 - val_accuracy: 0.8356\n",
      "Epoch 359/500\n",
      "7500/7500 [==============================] - 5s 625us/step - loss: 0.3771 - accuracy: 0.8379 - val_loss: 0.3760 - val_accuracy: 0.8382\n",
      "Epoch 360/500\n",
      "7500/7500 [==============================] - 5s 607us/step - loss: 0.3768 - accuracy: 0.8379 - val_loss: 0.3796 - val_accuracy: 0.8361\n",
      "Epoch 361/500\n",
      "7500/7500 [==============================] - 5s 646us/step - loss: 0.3771 - accuracy: 0.8380 - val_loss: 0.3788 - val_accuracy: 0.8364\n",
      "Epoch 362/500\n",
      "7500/7500 [==============================] - 5s 612us/step - loss: 0.3771 - accuracy: 0.8376 - val_loss: 0.3781 - val_accuracy: 0.8377\n",
      "Epoch 363/500\n",
      "7500/7500 [==============================] - 5s 606us/step - loss: 0.3769 - accuracy: 0.8375 - val_loss: 0.3796 - val_accuracy: 0.8367\n",
      "Epoch 364/500\n",
      "7500/7500 [==============================] - 4s 598us/step - loss: 0.3769 - accuracy: 0.8379 - val_loss: 0.3908 - val_accuracy: 0.8317\n",
      "Epoch 365/500\n",
      "7500/7500 [==============================] - 5s 614us/step - loss: 0.3770 - accuracy: 0.8379 - val_loss: 0.3769 - val_accuracy: 0.8379\n",
      "Epoch 366/500\n",
      "7500/7500 [==============================] - 5s 614us/step - loss: 0.3769 - accuracy: 0.8381 - val_loss: 0.3766 - val_accuracy: 0.8382\n",
      "Epoch 367/500\n",
      "7500/7500 [==============================] - 4s 576us/step - loss: 0.3771 - accuracy: 0.8373 - val_loss: 0.3764 - val_accuracy: 0.8375\n",
      "Epoch 368/500\n",
      "7500/7500 [==============================] - 4s 565us/step - loss: 0.3768 - accuracy: 0.8377 - val_loss: 0.3774 - val_accuracy: 0.8378\n",
      "Epoch 369/500\n",
      "7500/7500 [==============================] - 4s 575us/step - loss: 0.3772 - accuracy: 0.8377 - val_loss: 0.3846 - val_accuracy: 0.8344\n",
      "Epoch 370/500\n",
      "7500/7500 [==============================] - 4s 568us/step - loss: 0.3768 - accuracy: 0.8376 - val_loss: 0.3769 - val_accuracy: 0.8382\n",
      "Epoch 371/500\n",
      "7500/7500 [==============================] - 4s 554us/step - loss: 0.3770 - accuracy: 0.8376 - val_loss: 0.3853 - val_accuracy: 0.8329\n",
      "Epoch 372/500\n",
      "7500/7500 [==============================] - 5s 651us/step - loss: 0.3769 - accuracy: 0.8375 - val_loss: 0.3772 - val_accuracy: 0.8373\n",
      "Epoch 373/500\n",
      "7500/7500 [==============================] - 5s 619us/step - loss: 0.3769 - accuracy: 0.8377 - val_loss: 0.3772 - val_accuracy: 0.8370\n",
      "Epoch 374/500\n",
      "7500/7500 [==============================] - 4s 592us/step - loss: 0.3768 - accuracy: 0.8374 - val_loss: 0.3791 - val_accuracy: 0.8367\n",
      "Epoch 375/500\n",
      "7500/7500 [==============================] - 5s 601us/step - loss: 0.3768 - accuracy: 0.8378 - val_loss: 0.3785 - val_accuracy: 0.8371\n",
      "Epoch 376/500\n",
      "7500/7500 [==============================] - 5s 618us/step - loss: 0.3768 - accuracy: 0.8380 - val_loss: 0.3774 - val_accuracy: 0.8377\n",
      "Epoch 377/500\n",
      "7500/7500 [==============================] - 5s 604us/step - loss: 0.3769 - accuracy: 0.8377 - val_loss: 0.3814 - val_accuracy: 0.8354\n",
      "Epoch 378/500\n",
      "7500/7500 [==============================] - 5s 632us/step - loss: 0.3769 - accuracy: 0.8378 - val_loss: 0.3815 - val_accuracy: 0.8360\n",
      "Epoch 379/500\n",
      "7500/7500 [==============================] - 5s 607us/step - loss: 0.3769 - accuracy: 0.8377 - val_loss: 0.3771 - val_accuracy: 0.8378\n",
      "Epoch 380/500\n",
      "7500/7500 [==============================] - 5s 657us/step - loss: 0.3769 - accuracy: 0.8380 - val_loss: 0.3773 - val_accuracy: 0.8371\n",
      "Epoch 381/500\n",
      "7500/7500 [==============================] - 5s 645us/step - loss: 0.3769 - accuracy: 0.8377 - val_loss: 0.3770 - val_accuracy: 0.8371\n",
      "Epoch 382/500\n",
      "7500/7500 [==============================] - 5s 717us/step - loss: 0.3769 - accuracy: 0.8377 - val_loss: 0.3831 - val_accuracy: 0.8348\n",
      "Epoch 383/500\n",
      "7500/7500 [==============================] - 5s 622us/step - loss: 0.3768 - accuracy: 0.8379 - val_loss: 0.3775 - val_accuracy: 0.8379\n",
      "Epoch 384/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 4s 591us/step - loss: 0.3771 - accuracy: 0.8375 - val_loss: 0.3766 - val_accuracy: 0.8385\n",
      "Epoch 385/500\n",
      "7500/7500 [==============================] - 4s 591us/step - loss: 0.3768 - accuracy: 0.8379 - val_loss: 0.3774 - val_accuracy: 0.8372\n",
      "Epoch 386/500\n",
      "7500/7500 [==============================] - 5s 684us/step - loss: 0.3768 - accuracy: 0.8376 - val_loss: 0.3804 - val_accuracy: 0.8358\n",
      "Epoch 387/500\n",
      "7500/7500 [==============================] - 5s 656us/step - loss: 0.3767 - accuracy: 0.8378 - val_loss: 0.3763 - val_accuracy: 0.8382\n",
      "Epoch 388/500\n",
      "7500/7500 [==============================] - 5s 679us/step - loss: 0.3768 - accuracy: 0.8378 - val_loss: 0.3792 - val_accuracy: 0.8361\n",
      "Epoch 389/500\n",
      "7500/7500 [==============================] - 5s 690us/step - loss: 0.3769 - accuracy: 0.8378 - val_loss: 0.3788 - val_accuracy: 0.8371\n",
      "Epoch 390/500\n",
      "7500/7500 [==============================] - 5s 696us/step - loss: 0.3768 - accuracy: 0.8375 - val_loss: 0.3765 - val_accuracy: 0.8381\n",
      "Epoch 391/500\n",
      "7500/7500 [==============================] - 5s 650us/step - loss: 0.3768 - accuracy: 0.8375 - val_loss: 0.3788 - val_accuracy: 0.8363\n",
      "Epoch 392/500\n",
      "7500/7500 [==============================] - 5s 687us/step - loss: 0.3767 - accuracy: 0.8382 - val_loss: 0.3773 - val_accuracy: 0.8379\n",
      "Epoch 393/500\n",
      "7500/7500 [==============================] - 6s 759us/step - loss: 0.3768 - accuracy: 0.8372 - val_loss: 0.3762 - val_accuracy: 0.8375\n",
      "Epoch 394/500\n",
      "7500/7500 [==============================] - 7s 899us/step - loss: 0.3768 - accuracy: 0.8377 - val_loss: 0.3761 - val_accuracy: 0.8380\n",
      "Epoch 395/500\n",
      "7500/7500 [==============================] - 5s 728us/step - loss: 0.3769 - accuracy: 0.8374 - val_loss: 0.3880 - val_accuracy: 0.8316\n",
      "Epoch 396/500\n",
      "7500/7500 [==============================] - 6s 770us/step - loss: 0.3769 - accuracy: 0.8376 - val_loss: 0.3789 - val_accuracy: 0.8367\n",
      "Epoch 397/500\n",
      "7500/7500 [==============================] - 6s 791us/step - loss: 0.3769 - accuracy: 0.8378 - val_loss: 0.3774 - val_accuracy: 0.8379\n",
      "Epoch 398/500\n",
      "7500/7500 [==============================] - 6s 842us/step - loss: 0.3767 - accuracy: 0.8378 - val_loss: 0.3775 - val_accuracy: 0.8371\n",
      "Epoch 399/500\n",
      "7500/7500 [==============================] - 5s 673us/step - loss: 0.3767 - accuracy: 0.8378 - val_loss: 0.3766 - val_accuracy: 0.8378\n",
      "Epoch 400/500\n",
      "7500/7500 [==============================] - 5s 729us/step - loss: 0.3768 - accuracy: 0.8378 - val_loss: 0.3772 - val_accuracy: 0.8374\n",
      "Epoch 401/500\n",
      "7500/7500 [==============================] - 5s 623us/step - loss: 0.3768 - accuracy: 0.8375 - val_loss: 0.3802 - val_accuracy: 0.8357\n",
      "Epoch 402/500\n",
      "7500/7500 [==============================] - 5s 643us/step - loss: 0.3769 - accuracy: 0.8379 - val_loss: 0.3831 - val_accuracy: 0.8365\n",
      "Epoch 403/500\n",
      "7500/7500 [==============================] - 4s 518us/step - loss: 0.3769 - accuracy: 0.8381 - val_loss: 0.3830 - val_accuracy: 0.8352\n",
      "Epoch 404/500\n",
      "7500/7500 [==============================] - 4s 527us/step - loss: 0.3768 - accuracy: 0.8382 - val_loss: 0.3761 - val_accuracy: 0.8383\n",
      "Epoch 405/500\n",
      "7500/7500 [==============================] - 4s 571us/step - loss: 0.3767 - accuracy: 0.8379 - val_loss: 0.3763 - val_accuracy: 0.8378\n",
      "Epoch 406/500\n",
      "7500/7500 [==============================] - 4s 596us/step - loss: 0.3767 - accuracy: 0.8381 - val_loss: 0.3764 - val_accuracy: 0.8381\n",
      "Epoch 407/500\n",
      "7500/7500 [==============================] - 4s 595us/step - loss: 0.3768 - accuracy: 0.8376 - val_loss: 0.3775 - val_accuracy: 0.8369\n",
      "Epoch 408/500\n",
      "7500/7500 [==============================] - 4s 586us/step - loss: 0.3768 - accuracy: 0.8379 - val_loss: 0.3822 - val_accuracy: 0.8358\n",
      "Epoch 409/500\n",
      "7500/7500 [==============================] - 4s 575us/step - loss: 0.3768 - accuracy: 0.8377 - val_loss: 0.3883 - val_accuracy: 0.8310\n",
      "Epoch 410/500\n",
      "7500/7500 [==============================] - 4s 534us/step - loss: 0.3767 - accuracy: 0.8378 - val_loss: 0.3762 - val_accuracy: 0.8381\n",
      "Epoch 411/500\n",
      "7500/7500 [==============================] - 4s 538us/step - loss: 0.3767 - accuracy: 0.8377 - val_loss: 0.3799 - val_accuracy: 0.8355\n",
      "Epoch 412/500\n",
      "7500/7500 [==============================] - 4s 538us/step - loss: 0.3768 - accuracy: 0.8378 - val_loss: 0.3809 - val_accuracy: 0.8357\n",
      "Epoch 413/500\n",
      "7500/7500 [==============================] - 4s 566us/step - loss: 0.3770 - accuracy: 0.8379 - val_loss: 0.3761 - val_accuracy: 0.8379\n",
      "Epoch 414/500\n",
      "7500/7500 [==============================] - 4s 539us/step - loss: 0.3768 - accuracy: 0.8377 - val_loss: 0.3894 - val_accuracy: 0.8308\n",
      "Epoch 415/500\n",
      "7500/7500 [==============================] - 4s 534us/step - loss: 0.3767 - accuracy: 0.8374 - val_loss: 0.3814 - val_accuracy: 0.8358\n",
      "Epoch 416/500\n",
      "7500/7500 [==============================] - 4s 532us/step - loss: 0.3767 - accuracy: 0.8376 - val_loss: 0.3763 - val_accuracy: 0.8375\n",
      "Epoch 417/500\n",
      "7500/7500 [==============================] - 4s 521us/step - loss: 0.3767 - accuracy: 0.8377 - val_loss: 0.3773 - val_accuracy: 0.8371\n",
      "Epoch 418/500\n",
      "7500/7500 [==============================] - 4s 518us/step - loss: 0.3768 - accuracy: 0.8381 - val_loss: 0.3825 - val_accuracy: 0.8338\n",
      "Epoch 419/500\n",
      "7500/7500 [==============================] - 4s 519us/step - loss: 0.3766 - accuracy: 0.8380 - val_loss: 0.3765 - val_accuracy: 0.8380\n",
      "Epoch 420/500\n",
      "7500/7500 [==============================] - 4s 554us/step - loss: 0.3768 - accuracy: 0.8377 - val_loss: 0.3786 - val_accuracy: 0.8372\n",
      "Epoch 421/500\n",
      "7500/7500 [==============================] - 4s 553us/step - loss: 0.3766 - accuracy: 0.8379 - val_loss: 0.3790 - val_accuracy: 0.8361\n",
      "Epoch 422/500\n",
      "7500/7500 [==============================] - 4s 577us/step - loss: 0.3766 - accuracy: 0.8377 - val_loss: 0.3821 - val_accuracy: 0.8342\n",
      "Epoch 423/500\n",
      "7500/7500 [==============================] - 4s 557us/step - loss: 0.3767 - accuracy: 0.8380 - val_loss: 0.3760 - val_accuracy: 0.8375\n",
      "Epoch 424/500\n",
      "7500/7500 [==============================] - 4s 557us/step - loss: 0.3767 - accuracy: 0.8379 - val_loss: 0.3769 - val_accuracy: 0.8374\n",
      "Epoch 425/500\n",
      "7500/7500 [==============================] - 4s 532us/step - loss: 0.3766 - accuracy: 0.8381 - val_loss: 0.3785 - val_accuracy: 0.8364\n",
      "Epoch 426/500\n",
      "7500/7500 [==============================] - 4s 525us/step - loss: 0.3767 - accuracy: 0.8376 - val_loss: 0.3768 - val_accuracy: 0.8372\n",
      "Epoch 427/500\n",
      "7500/7500 [==============================] - 4s 528us/step - loss: 0.3768 - accuracy: 0.8377 - val_loss: 0.3774 - val_accuracy: 0.8376\n",
      "Epoch 428/500\n",
      "7500/7500 [==============================] - 4s 513us/step - loss: 0.3767 - accuracy: 0.8376 - val_loss: 0.3770 - val_accuracy: 0.8383\n",
      "Epoch 429/500\n",
      "7500/7500 [==============================] - 4s 537us/step - loss: 0.3766 - accuracy: 0.8379 - val_loss: 0.3772 - val_accuracy: 0.8385\n",
      "Epoch 430/500\n",
      "7500/7500 [==============================] - 4s 553us/step - loss: 0.3767 - accuracy: 0.8378 - val_loss: 0.3780 - val_accuracy: 0.8374\n",
      "Epoch 431/500\n",
      "7500/7500 [==============================] - 4s 514us/step - loss: 0.3765 - accuracy: 0.8379 - val_loss: 0.3901 - val_accuracy: 0.8316\n",
      "Epoch 432/500\n",
      "7500/7500 [==============================] - 4s 515us/step - loss: 0.3768 - accuracy: 0.8380 - val_loss: 0.3777 - val_accuracy: 0.8368\n",
      "Epoch 433/500\n",
      "7500/7500 [==============================] - 4s 551us/step - loss: 0.3766 - accuracy: 0.8377 - val_loss: 0.3759 - val_accuracy: 0.8378\n",
      "Epoch 434/500\n",
      "7500/7500 [==============================] - 4s 533us/step - loss: 0.3767 - accuracy: 0.8380 - val_loss: 0.3788 - val_accuracy: 0.8371\n",
      "Epoch 435/500\n",
      "7500/7500 [==============================] - 4s 561us/step - loss: 0.3766 - accuracy: 0.8379 - val_loss: 0.3770 - val_accuracy: 0.8375\n",
      "Epoch 436/500\n",
      "7500/7500 [==============================] - 4s 546us/step - loss: 0.3767 - accuracy: 0.8379 - val_loss: 0.3772 - val_accuracy: 0.8372\n",
      "Epoch 437/500\n",
      "7500/7500 [==============================] - 4s 533us/step - loss: 0.3767 - accuracy: 0.8372 - val_loss: 0.3781 - val_accuracy: 0.8373\n",
      "Epoch 438/500\n",
      "7500/7500 [==============================] - 4s 510us/step - loss: 0.3767 - accuracy: 0.8377 - val_loss: 0.3789 - val_accuracy: 0.8366\n",
      "Epoch 439/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 4s 513us/step - loss: 0.3767 - accuracy: 0.8379 - val_loss: 0.3795 - val_accuracy: 0.8360\n",
      "Epoch 440/500\n",
      "7500/7500 [==============================] - 4s 525us/step - loss: 0.3768 - accuracy: 0.8380 - val_loss: 0.3775 - val_accuracy: 0.8370\n",
      "Epoch 441/500\n",
      "7500/7500 [==============================] - 4s 511us/step - loss: 0.3767 - accuracy: 0.8377 - val_loss: 0.3786 - val_accuracy: 0.8364\n",
      "Epoch 442/500\n",
      "7500/7500 [==============================] - 4s 512us/step - loss: 0.3765 - accuracy: 0.8377 - val_loss: 0.3780 - val_accuracy: 0.8375\n",
      "Epoch 443/500\n",
      "7500/7500 [==============================] - 4s 510us/step - loss: 0.3765 - accuracy: 0.8375 - val_loss: 0.3775 - val_accuracy: 0.8373\n",
      "Epoch 444/500\n",
      "7500/7500 [==============================] - 4s 512us/step - loss: 0.3765 - accuracy: 0.8383 - val_loss: 0.3761 - val_accuracy: 0.8377\n",
      "Epoch 445/500\n",
      "7500/7500 [==============================] - 4s 509us/step - loss: 0.3767 - accuracy: 0.8377 - val_loss: 0.3822 - val_accuracy: 0.8349\n",
      "Epoch 446/500\n",
      "7500/7500 [==============================] - 4s 547us/step - loss: 0.3767 - accuracy: 0.8376 - val_loss: 0.3780 - val_accuracy: 0.8379\n",
      "Epoch 447/500\n",
      "7500/7500 [==============================] - 4s 534us/step - loss: 0.3766 - accuracy: 0.8376 - val_loss: 0.3846 - val_accuracy: 0.8332\n",
      "Epoch 448/500\n",
      "7500/7500 [==============================] - 4s 508us/step - loss: 0.3765 - accuracy: 0.8383 - val_loss: 0.3768 - val_accuracy: 0.8372\n",
      "Epoch 449/500\n",
      "7500/7500 [==============================] - 4s 532us/step - loss: 0.3767 - accuracy: 0.8376 - val_loss: 0.3839 - val_accuracy: 0.8350\n",
      "Epoch 450/500\n",
      "7500/7500 [==============================] - 5s 608us/step - loss: 0.3766 - accuracy: 0.8382 - val_loss: 0.3766 - val_accuracy: 0.8382\n",
      "Epoch 451/500\n",
      "7500/7500 [==============================] - 4s 582us/step - loss: 0.3765 - accuracy: 0.8378 - val_loss: 0.3854 - val_accuracy: 0.8344\n",
      "Epoch 452/500\n",
      "7500/7500 [==============================] - 4s 518us/step - loss: 0.3765 - accuracy: 0.8377 - val_loss: 0.3812 - val_accuracy: 0.8363\n",
      "Epoch 453/500\n",
      "7500/7500 [==============================] - 4s 523us/step - loss: 0.3764 - accuracy: 0.8380 - val_loss: 0.3767 - val_accuracy: 0.8377\n",
      "Epoch 454/500\n",
      "7500/7500 [==============================] - 4s 533us/step - loss: 0.3764 - accuracy: 0.8380 - val_loss: 0.3812 - val_accuracy: 0.8357\n",
      "Epoch 455/500\n",
      "7500/7500 [==============================] - 4s 516us/step - loss: 0.3766 - accuracy: 0.8377 - val_loss: 0.3768 - val_accuracy: 0.8379\n",
      "Epoch 456/500\n",
      "7500/7500 [==============================] - 4s 515us/step - loss: 0.3767 - accuracy: 0.8379 - val_loss: 0.3850 - val_accuracy: 0.8337\n",
      "Epoch 457/500\n",
      "7500/7500 [==============================] - 4s 518us/step - loss: 0.3766 - accuracy: 0.8377 - val_loss: 0.3768 - val_accuracy: 0.8375\n",
      "Epoch 458/500\n",
      "7500/7500 [==============================] - 4s 515us/step - loss: 0.3765 - accuracy: 0.8380 - val_loss: 0.3852 - val_accuracy: 0.8333\n",
      "Epoch 459/500\n",
      "7500/7500 [==============================] - 4s 515us/step - loss: 0.3766 - accuracy: 0.8377 - val_loss: 0.3763 - val_accuracy: 0.8373\n",
      "Epoch 460/500\n",
      "7500/7500 [==============================] - 4s 515us/step - loss: 0.3766 - accuracy: 0.8375 - val_loss: 0.3787 - val_accuracy: 0.8373\n",
      "Epoch 461/500\n",
      "7500/7500 [==============================] - 4s 514us/step - loss: 0.3766 - accuracy: 0.8376 - val_loss: 0.3762 - val_accuracy: 0.8380\n",
      "Epoch 462/500\n",
      "7500/7500 [==============================] - 4s 516us/step - loss: 0.3766 - accuracy: 0.8379 - val_loss: 0.3783 - val_accuracy: 0.8370\n",
      "Epoch 463/500\n",
      "7500/7500 [==============================] - 4s 566us/step - loss: 0.3765 - accuracy: 0.8377 - val_loss: 0.3805 - val_accuracy: 0.8359\n",
      "Epoch 464/500\n",
      "7500/7500 [==============================] - 4s 536us/step - loss: 0.3766 - accuracy: 0.8375 - val_loss: 0.3757 - val_accuracy: 0.8383\n",
      "Epoch 465/500\n",
      "7500/7500 [==============================] - 5s 637us/step - loss: 0.3762 - accuracy: 0.8379 - val_loss: 0.3767 - val_accuracy: 0.8371\n",
      "Epoch 466/500\n",
      "7500/7500 [==============================] - 4s 539us/step - loss: 0.3764 - accuracy: 0.8382 - val_loss: 0.3774 - val_accuracy: 0.8373\n",
      "Epoch 467/500\n",
      "7500/7500 [==============================] - 4s 514us/step - loss: 0.3765 - accuracy: 0.8380 - val_loss: 0.3851 - val_accuracy: 0.8344\n",
      "Epoch 468/500\n",
      "7500/7500 [==============================] - 4s 517us/step - loss: 0.3764 - accuracy: 0.8384 - val_loss: 0.3772 - val_accuracy: 0.8377\n",
      "Epoch 469/500\n",
      "7500/7500 [==============================] - 4s 527us/step - loss: 0.3764 - accuracy: 0.8375 - val_loss: 0.3757 - val_accuracy: 0.8379\n",
      "Epoch 470/500\n",
      "7500/7500 [==============================] - 4s 584us/step - loss: 0.3765 - accuracy: 0.8379 - val_loss: 0.3773 - val_accuracy: 0.8381\n",
      "Epoch 471/500\n",
      "7500/7500 [==============================] - 4s 579us/step - loss: 0.3763 - accuracy: 0.8385 - val_loss: 0.3761 - val_accuracy: 0.8376\n",
      "Epoch 472/500\n",
      "7500/7500 [==============================] - 4s 561us/step - loss: 0.3764 - accuracy: 0.8377 - val_loss: 0.3888 - val_accuracy: 0.8321\n",
      "Epoch 473/500\n",
      "7500/7500 [==============================] - 4s 546us/step - loss: 0.3766 - accuracy: 0.8379 - val_loss: 0.3782 - val_accuracy: 0.8372\n",
      "Epoch 474/500\n",
      "7500/7500 [==============================] - 4s 562us/step - loss: 0.3764 - accuracy: 0.8381 - val_loss: 0.3764 - val_accuracy: 0.8370\n",
      "Epoch 475/500\n",
      "7500/7500 [==============================] - 4s 542us/step - loss: 0.3764 - accuracy: 0.8379 - val_loss: 0.3763 - val_accuracy: 0.8378\n",
      "Epoch 476/500\n",
      "7500/7500 [==============================] - 4s 553us/step - loss: 0.3763 - accuracy: 0.8381 - val_loss: 0.3771 - val_accuracy: 0.8375\n",
      "Epoch 477/500\n",
      "7500/7500 [==============================] - 4s 595us/step - loss: 0.3765 - accuracy: 0.8380 - val_loss: 0.3763 - val_accuracy: 0.8380\n",
      "Epoch 478/500\n",
      "7500/7500 [==============================] - 4s 528us/step - loss: 0.3767 - accuracy: 0.8375 - val_loss: 0.3764 - val_accuracy: 0.8372\n",
      "Epoch 479/500\n",
      "7500/7500 [==============================] - 4s 563us/step - loss: 0.3762 - accuracy: 0.8377 - val_loss: 0.3797 - val_accuracy: 0.8365\n",
      "Epoch 480/500\n",
      "7500/7500 [==============================] - 4s 523us/step - loss: 0.3765 - accuracy: 0.8379 - val_loss: 0.3777 - val_accuracy: 0.8368\n",
      "Epoch 481/500\n",
      "7500/7500 [==============================] - 4s 519us/step - loss: 0.3765 - accuracy: 0.8377 - val_loss: 0.3800 - val_accuracy: 0.8364\n",
      "Epoch 482/500\n",
      "7500/7500 [==============================] - 4s 516us/step - loss: 0.3764 - accuracy: 0.8381 - val_loss: 0.3770 - val_accuracy: 0.8374\n",
      "Epoch 483/500\n",
      "7500/7500 [==============================] - 4s 512us/step - loss: 0.3765 - accuracy: 0.8378 - val_loss: 0.3789 - val_accuracy: 0.8363\n",
      "Epoch 484/500\n",
      "7500/7500 [==============================] - 4s 519us/step - loss: 0.3763 - accuracy: 0.8378 - val_loss: 0.3803 - val_accuracy: 0.8370\n",
      "Epoch 485/500\n",
      "7500/7500 [==============================] - 4s 515us/step - loss: 0.3765 - accuracy: 0.8379 - val_loss: 0.3761 - val_accuracy: 0.8380\n",
      "Epoch 486/500\n",
      "7500/7500 [==============================] - 4s 512us/step - loss: 0.3765 - accuracy: 0.8381 - val_loss: 0.3839 - val_accuracy: 0.8349\n",
      "Epoch 487/500\n",
      "7500/7500 [==============================] - 4s 514us/step - loss: 0.3765 - accuracy: 0.8379 - val_loss: 0.3897 - val_accuracy: 0.8314\n",
      "Epoch 488/500\n",
      "7500/7500 [==============================] - 4s 512us/step - loss: 0.3764 - accuracy: 0.8377 - val_loss: 0.3785 - val_accuracy: 0.8364\n",
      "Epoch 489/500\n",
      "7500/7500 [==============================] - 4s 515us/step - loss: 0.3766 - accuracy: 0.8376 - val_loss: 0.3914 - val_accuracy: 0.8321\n",
      "Epoch 490/500\n",
      "7500/7500 [==============================] - 4s 513us/step - loss: 0.3766 - accuracy: 0.8378 - val_loss: 0.3836 - val_accuracy: 0.8342\n",
      "Epoch 491/500\n",
      "7500/7500 [==============================] - 4s 519us/step - loss: 0.3762 - accuracy: 0.8382 - val_loss: 0.3766 - val_accuracy: 0.8371\n",
      "Epoch 492/500\n",
      "7500/7500 [==============================] - 4s 521us/step - loss: 0.3763 - accuracy: 0.8378 - val_loss: 0.3915 - val_accuracy: 0.8319\n",
      "Epoch 493/500\n",
      "7500/7500 [==============================] - 4s 515us/step - loss: 0.3764 - accuracy: 0.8380 - val_loss: 0.3772 - val_accuracy: 0.8378\n",
      "Epoch 494/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 4s 592us/step - loss: 0.3764 - accuracy: 0.8380 - val_loss: 0.3798 - val_accuracy: 0.8364\n",
      "Epoch 495/500\n",
      "7500/7500 [==============================] - 4s 568us/step - loss: 0.3766 - accuracy: 0.8376 - val_loss: 0.3762 - val_accuracy: 0.8379\n",
      "Epoch 496/500\n",
      "7500/7500 [==============================] - 4s 569us/step - loss: 0.3763 - accuracy: 0.8376 - val_loss: 0.3813 - val_accuracy: 0.8352\n",
      "Epoch 497/500\n",
      "7500/7500 [==============================] - 4s 531us/step - loss: 0.3762 - accuracy: 0.8383 - val_loss: 0.3843 - val_accuracy: 0.8347\n",
      "Epoch 498/500\n",
      "7500/7500 [==============================] - 4s 515us/step - loss: 0.3766 - accuracy: 0.8380 - val_loss: 0.3809 - val_accuracy: 0.8359\n",
      "Epoch 499/500\n",
      "7500/7500 [==============================] - 4s 511us/step - loss: 0.3764 - accuracy: 0.8382 - val_loss: 0.3825 - val_accuracy: 0.8346\n",
      "Epoch 500/500\n",
      "7500/7500 [==============================] - 4s 510us/step - loss: 0.3764 - accuracy: 0.8381 - val_loss: 0.3769 - val_accuracy: 0.8374\n"
     ]
    }
   ],
   "source": [
    "model_1.compile(SGD(lr = .003), \"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "run_hist_1 = model_1.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is 0.837\n",
      "roc-auc is 0.871\n"
     ]
    }
   ],
   "source": [
    "y_pred_class_nn_1 = model_1.predict_classes(X_test)\n",
    "y_pred_prob_nn_1 = model_1.predict(X_test)\n",
    "\n",
    "print('accuracy is {:.3f}'.format(accuracy_score(y_test,y_pred_class_nn_1)))\n",
    "print('roc-auc is {:.3f}'.format(roc_auc_score(y_test,y_pred_prob_nn_1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_class_nn_1 = model_1.predict_classes(X)\n",
    "y_pred_prob_nn_1 = model_1.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.05876464, 0.55560994, 0.06158394, ..., 0.70166874, 0.11884412,\n",
       "       0.34610277], dtype=float32)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_prob_nn_1[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.058765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.555610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.061584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.090531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.084319</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     target\n",
       "0  0.058765\n",
       "1  0.555610\n",
       "2  0.061584\n",
       "3  0.090531\n",
       "4  0.084319"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final=pd.DataFrame({'target': y_pred_prob_nn_1[:,0]})\n",
    "final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>0.058765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>0.555610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>0.061584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>0.090531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>0.084319</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id    target\n",
       "0   5  0.058765\n",
       "1   6  0.555610\n",
       "2   8  0.061584\n",
       "3   9  0.090531\n",
       "4  11  0.084319"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final1 = pd.concat([df1['id'],final],axis=1)\n",
    "final1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "final1.to_csv('sample_submission.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
