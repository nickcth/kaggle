{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# This is a Kaggle competition to predict the probability of 0 or 1 of Tabular Playground series data\n",
    "## I will be comparing which model to use: XGB, Random Forest or Neural Networks\n",
    "\n",
    "\n",
    "Lets first load required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import NullFormatter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyodbc\n",
    "import matplotlib.ticker as ticker\n",
    "from sklearn import preprocessing\n",
    "%matplotlib inline\n",
    "import keras\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import SimpleRNN\n",
    "from keras.datasets import imdb\n",
    "from keras import initializers\n",
    "from keras.models  import Sequential\n",
    "from keras.layers import Input, Dense, Flatten, Dropout, BatchNormalization\n",
    "from keras.optimizers import Adam, SGD, RMSprop\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_curve, roc_auc_score, roc_curve, accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Lets download the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('C:/Users/taihs/OneDrive/Documents/tps competition/train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Find the number of records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300000, 32)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 300000 entries, 0 to 299999\n",
      "Data columns (total 32 columns):\n",
      " #   Column  Non-Null Count   Dtype  \n",
      "---  ------  --------------   -----  \n",
      " 0   id      300000 non-null  int64  \n",
      " 1   cat0    300000 non-null  object \n",
      " 2   cat1    300000 non-null  object \n",
      " 3   cat2    300000 non-null  object \n",
      " 4   cat3    300000 non-null  object \n",
      " 5   cat4    300000 non-null  object \n",
      " 6   cat5    300000 non-null  object \n",
      " 7   cat6    300000 non-null  object \n",
      " 8   cat7    300000 non-null  object \n",
      " 9   cat8    300000 non-null  object \n",
      " 10  cat9    300000 non-null  object \n",
      " 11  cat10   300000 non-null  object \n",
      " 12  cat11   300000 non-null  object \n",
      " 13  cat12   300000 non-null  object \n",
      " 14  cat13   300000 non-null  object \n",
      " 15  cat14   300000 non-null  object \n",
      " 16  cat15   300000 non-null  object \n",
      " 17  cat16   300000 non-null  object \n",
      " 18  cat17   300000 non-null  object \n",
      " 19  cat18   300000 non-null  object \n",
      " 20  cont0   300000 non-null  float64\n",
      " 21  cont1   300000 non-null  float64\n",
      " 22  cont2   300000 non-null  float64\n",
      " 23  cont3   300000 non-null  float64\n",
      " 24  cont4   300000 non-null  float64\n",
      " 25  cont5   300000 non-null  float64\n",
      " 26  cont6   300000 non-null  float64\n",
      " 27  cont7   300000 non-null  float64\n",
      " 28  cont8   300000 non-null  float64\n",
      " 29  cont9   300000 non-null  float64\n",
      " 30  cont10  300000 non-null  float64\n",
      " 31  target  300000 non-null  int64  \n",
      "dtypes: float64(11), int64(2), object(19)\n",
      "memory usage: 73.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cat0</th>\n",
       "      <th>cat1</th>\n",
       "      <th>cat2</th>\n",
       "      <th>cat3</th>\n",
       "      <th>cat4</th>\n",
       "      <th>cat5</th>\n",
       "      <th>cat6</th>\n",
       "      <th>cat7</th>\n",
       "      <th>cat8</th>\n",
       "      <th>...</th>\n",
       "      <th>cont2</th>\n",
       "      <th>cont3</th>\n",
       "      <th>cont4</th>\n",
       "      <th>cont5</th>\n",
       "      <th>cont6</th>\n",
       "      <th>cont7</th>\n",
       "      <th>cont8</th>\n",
       "      <th>cont9</th>\n",
       "      <th>cont10</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>A</td>\n",
       "      <td>I</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>BI</td>\n",
       "      <td>A</td>\n",
       "      <td>S</td>\n",
       "      <td>Q</td>\n",
       "      <td>...</td>\n",
       "      <td>0.759439</td>\n",
       "      <td>0.795549</td>\n",
       "      <td>0.681917</td>\n",
       "      <td>0.621672</td>\n",
       "      <td>0.592184</td>\n",
       "      <td>0.791921</td>\n",
       "      <td>0.815254</td>\n",
       "      <td>0.965006</td>\n",
       "      <td>0.665915</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>I</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>E</td>\n",
       "      <td>BI</td>\n",
       "      <td>K</td>\n",
       "      <td>W</td>\n",
       "      <td>AD</td>\n",
       "      <td>...</td>\n",
       "      <td>0.386385</td>\n",
       "      <td>0.541366</td>\n",
       "      <td>0.388982</td>\n",
       "      <td>0.357778</td>\n",
       "      <td>0.600044</td>\n",
       "      <td>0.408701</td>\n",
       "      <td>0.399353</td>\n",
       "      <td>0.927406</td>\n",
       "      <td>0.493729</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>A</td>\n",
       "      <td>K</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>E</td>\n",
       "      <td>BI</td>\n",
       "      <td>A</td>\n",
       "      <td>E</td>\n",
       "      <td>BM</td>\n",
       "      <td>...</td>\n",
       "      <td>0.343255</td>\n",
       "      <td>0.616352</td>\n",
       "      <td>0.793687</td>\n",
       "      <td>0.552877</td>\n",
       "      <td>0.352113</td>\n",
       "      <td>0.388835</td>\n",
       "      <td>0.412303</td>\n",
       "      <td>0.292696</td>\n",
       "      <td>0.549452</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>A</td>\n",
       "      <td>K</td>\n",
       "      <td>A</td>\n",
       "      <td>C</td>\n",
       "      <td>E</td>\n",
       "      <td>BI</td>\n",
       "      <td>A</td>\n",
       "      <td>Y</td>\n",
       "      <td>AD</td>\n",
       "      <td>...</td>\n",
       "      <td>0.831147</td>\n",
       "      <td>0.807807</td>\n",
       "      <td>0.800032</td>\n",
       "      <td>0.619147</td>\n",
       "      <td>0.221789</td>\n",
       "      <td>0.897617</td>\n",
       "      <td>0.633669</td>\n",
       "      <td>0.760318</td>\n",
       "      <td>0.934242</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>A</td>\n",
       "      <td>I</td>\n",
       "      <td>G</td>\n",
       "      <td>B</td>\n",
       "      <td>E</td>\n",
       "      <td>BI</td>\n",
       "      <td>C</td>\n",
       "      <td>G</td>\n",
       "      <td>Q</td>\n",
       "      <td>...</td>\n",
       "      <td>0.338818</td>\n",
       "      <td>0.277308</td>\n",
       "      <td>0.610578</td>\n",
       "      <td>0.128291</td>\n",
       "      <td>0.578764</td>\n",
       "      <td>0.279167</td>\n",
       "      <td>0.351103</td>\n",
       "      <td>0.357084</td>\n",
       "      <td>0.328960</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id cat0 cat1 cat2 cat3 cat4 cat5 cat6 cat7 cat8  ...     cont2     cont3  \\\n",
       "0   0    A    I    A    B    B   BI    A    S    Q  ...  0.759439  0.795549   \n",
       "1   1    A    I    A    A    E   BI    K    W   AD  ...  0.386385  0.541366   \n",
       "2   2    A    K    A    A    E   BI    A    E   BM  ...  0.343255  0.616352   \n",
       "3   3    A    K    A    C    E   BI    A    Y   AD  ...  0.831147  0.807807   \n",
       "4   4    A    I    G    B    E   BI    C    G    Q  ...  0.338818  0.277308   \n",
       "\n",
       "      cont4     cont5     cont6     cont7     cont8     cont9    cont10 target  \n",
       "0  0.681917  0.621672  0.592184  0.791921  0.815254  0.965006  0.665915      0  \n",
       "1  0.388982  0.357778  0.600044  0.408701  0.399353  0.927406  0.493729      0  \n",
       "2  0.793687  0.552877  0.352113  0.388835  0.412303  0.292696  0.549452      0  \n",
       "3  0.800032  0.619147  0.221789  0.897617  0.633669  0.760318  0.934242      0  \n",
       "4  0.610578  0.128291  0.578764  0.279167  0.351103  0.357084  0.328960      1  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target    1.000000\n",
       "cont5     0.215184\n",
       "cont6     0.189832\n",
       "cont8     0.183726\n",
       "cont1     0.164655\n",
       "cont2     0.140459\n",
       "cont9     0.059242\n",
       "id       -0.001407\n",
       "cont0    -0.015172\n",
       "cont7    -0.040646\n",
       "cont10   -0.047077\n",
       "cont4    -0.075585\n",
       "cont3    -0.148316\n",
       "Name: target, dtype: float64"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "co = df1.corr()\n",
    "co['target'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>target</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cat0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>A</th>\n",
       "      <td>148852</td>\n",
       "      <td>74673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B</th>\n",
       "      <td>71687</td>\n",
       "      <td>4788</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "target       0      1\n",
       "cat0                 \n",
       "A       148852  74673\n",
       "B        71687   4788"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(df1['cat0'],df1['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='cat0', ylabel='target'>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.barplot(x=df1['cat0'],y=df1['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='cat0', ylabel='count'>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(x=df1['cat0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='cat1', ylabel='target'>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.barplot(x=df1['cat1'],y=df1['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='cat9', ylabel='Count'>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.histplot(x=df1['cat9'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = df1.select_dtypes(include='object').columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>cat0</th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>148852</td>\n",
       "      <td>71687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>74673</td>\n",
       "      <td>4788</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "cat0         A      B\n",
       "target               \n",
       "0       148852  71687\n",
       "1        74673   4788"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(df1['target'],df1['cat0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Column              Hypothesis\n",
      "0    cat0  Reject Null Hypothesis\n",
      "1    cat1  Reject Null Hypothesis\n",
      "2    cat2  Reject Null Hypothesis\n",
      "3    cat3  Reject Null Hypothesis\n",
      "4    cat4  Reject Null Hypothesis\n",
      "5    cat5  Reject Null Hypothesis\n",
      "6    cat6  Reject Null Hypothesis\n",
      "7    cat7  Reject Null Hypothesis\n",
      "8    cat8  Reject Null Hypothesis\n",
      "9    cat9  Reject Null Hypothesis\n",
      "10  cat10  Reject Null Hypothesis\n",
      "11  cat11  Reject Null Hypothesis\n",
      "12  cat12  Reject Null Hypothesis\n",
      "13  cat13  Reject Null Hypothesis\n",
      "14  cat14  Reject Null Hypothesis\n",
      "15  cat15  Reject Null Hypothesis\n",
      "16  cat16  Reject Null Hypothesis\n",
      "17  cat17  Reject Null Hypothesis\n",
      "18  cat18  Reject Null Hypothesis\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "chi2_check = []\n",
    "for i in cat:\n",
    "    if chi2_contingency(pd.crosstab(df1['target'], df1[i]))[1] < 0.05:\n",
    "        chi2_check.append('Reject Null Hypothesis')\n",
    "    else:\n",
    "        chi2_check.append('Fail to Reject Null Hypothesis')\n",
    "res = pd.DataFrame(data = [cat, chi2_check] \n",
    "             ).T \n",
    "res.columns = ['Column', 'Hypothesis']\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "lst = res['Column'].tolist()\n",
    "le = LabelEncoder()\n",
    "df3 = pd.DataFrame()\n",
    "for i in lst:\n",
    "    icat = le.fit_transform(df1[i])\n",
    "    df2 = pd.DataFrame({i:icat})\n",
    "    df3 = pd.concat([df3,df2],axis=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat0</th>\n",
       "      <th>cat1</th>\n",
       "      <th>cat2</th>\n",
       "      <th>cat3</th>\n",
       "      <th>cat4</th>\n",
       "      <th>cat5</th>\n",
       "      <th>cat6</th>\n",
       "      <th>cat7</th>\n",
       "      <th>cat8</th>\n",
       "      <th>cat9</th>\n",
       "      <th>cat10</th>\n",
       "      <th>cat11</th>\n",
       "      <th>cat12</th>\n",
       "      <th>cat13</th>\n",
       "      <th>cat14</th>\n",
       "      <th>cat15</th>\n",
       "      <th>cat16</th>\n",
       "      <th>cat17</th>\n",
       "      <th>cat18</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>44</td>\n",
       "      <td>54</td>\n",
       "      <td>0</td>\n",
       "      <td>258</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>33</td>\n",
       "      <td>8</td>\n",
       "      <td>48</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>162</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>38</td>\n",
       "      <td>9</td>\n",
       "      <td>69</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>241</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>33</td>\n",
       "      <td>2</td>\n",
       "      <td>32</td>\n",
       "      <td>54</td>\n",
       "      <td>0</td>\n",
       "      <td>75</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299995</th>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>48</td>\n",
       "      <td>0</td>\n",
       "      <td>159</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299996</th>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>163</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299997</th>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>33</td>\n",
       "      <td>2</td>\n",
       "      <td>37</td>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "      <td>156</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299998</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299999</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>33</td>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>256</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300000 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        cat0  cat1  cat2  cat3  cat4  cat5  cat6  cat7  cat8  cat9  cat10  \\\n",
       "0          0     8     0     1     1    33     0    44    54     0    258   \n",
       "1          0     8     0     0     4    33     8    48     3     5    162   \n",
       "2          0    10     0     0     4    33     0    30    38     9     69   \n",
       "3          0    10     0     2     4    33     0    50     3     5    241   \n",
       "4          0     8     6     1     4    33     2    32    54     0     75   \n",
       "...      ...   ...   ...   ...   ...   ...   ...   ...   ...   ...    ...   \n",
       "299995     0    13     5     0     4    45     0    19    48     0    159   \n",
       "299996     0    10     0     0     6    33     0    36     4     4    163   \n",
       "299997     0     6    12     0     7    33     2    37    43     0    156   \n",
       "299998     1     7     0     3     1    33     0     1    23     0     25   \n",
       "299999     0     5     2     0     4    33     2    22    55     0    256   \n",
       "\n",
       "        cat11  cat12  cat13  cat14  cat15  cat16  cat17  cat18  \n",
       "0           0      0      0      0      1      3      3      1  \n",
       "1           0      1      0      1      3      1      3      1  \n",
       "2           0      1      0      0      1      3      3      1  \n",
       "3           0      0      0      0      1      3      3      1  \n",
       "4           0      0      0      1      1      1      3      1  \n",
       "...       ...    ...    ...    ...    ...    ...    ...    ...  \n",
       "299995      0      0      0      1      3      1      3      1  \n",
       "299996      0      1      0      1      1      3      3      1  \n",
       "299997      1      0      0      1      3      1      3      3  \n",
       "299998      0      0      0      0      1      0      3      0  \n",
       "299999      0      0      0      0      1      3      3      1  \n",
       "\n",
       "[300000 rows x 19 columns]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat0</th>\n",
       "      <th>cat1</th>\n",
       "      <th>cat2</th>\n",
       "      <th>cat3</th>\n",
       "      <th>cat4</th>\n",
       "      <th>cat5</th>\n",
       "      <th>cat6</th>\n",
       "      <th>cat7</th>\n",
       "      <th>cat8</th>\n",
       "      <th>cat9</th>\n",
       "      <th>...</th>\n",
       "      <th>cont1</th>\n",
       "      <th>cont2</th>\n",
       "      <th>cont3</th>\n",
       "      <th>cont4</th>\n",
       "      <th>cont5</th>\n",
       "      <th>cont6</th>\n",
       "      <th>cont7</th>\n",
       "      <th>cont8</th>\n",
       "      <th>cont9</th>\n",
       "      <th>cont10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>44</td>\n",
       "      <td>54</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.855349</td>\n",
       "      <td>0.759439</td>\n",
       "      <td>0.795549</td>\n",
       "      <td>0.681917</td>\n",
       "      <td>0.621672</td>\n",
       "      <td>0.592184</td>\n",
       "      <td>0.791921</td>\n",
       "      <td>0.815254</td>\n",
       "      <td>0.965006</td>\n",
       "      <td>0.665915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>33</td>\n",
       "      <td>8</td>\n",
       "      <td>48</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.328929</td>\n",
       "      <td>0.386385</td>\n",
       "      <td>0.541366</td>\n",
       "      <td>0.388982</td>\n",
       "      <td>0.357778</td>\n",
       "      <td>0.600044</td>\n",
       "      <td>0.408701</td>\n",
       "      <td>0.399353</td>\n",
       "      <td>0.927406</td>\n",
       "      <td>0.493729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>38</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>0.322749</td>\n",
       "      <td>0.343255</td>\n",
       "      <td>0.616352</td>\n",
       "      <td>0.793687</td>\n",
       "      <td>0.552877</td>\n",
       "      <td>0.352113</td>\n",
       "      <td>0.388835</td>\n",
       "      <td>0.412303</td>\n",
       "      <td>0.292696</td>\n",
       "      <td>0.549452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.707663</td>\n",
       "      <td>0.831147</td>\n",
       "      <td>0.807807</td>\n",
       "      <td>0.800032</td>\n",
       "      <td>0.619147</td>\n",
       "      <td>0.221789</td>\n",
       "      <td>0.897617</td>\n",
       "      <td>0.633669</td>\n",
       "      <td>0.760318</td>\n",
       "      <td>0.934242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>33</td>\n",
       "      <td>2</td>\n",
       "      <td>32</td>\n",
       "      <td>54</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.274514</td>\n",
       "      <td>0.338818</td>\n",
       "      <td>0.277308</td>\n",
       "      <td>0.610578</td>\n",
       "      <td>0.128291</td>\n",
       "      <td>0.578764</td>\n",
       "      <td>0.279167</td>\n",
       "      <td>0.351103</td>\n",
       "      <td>0.357084</td>\n",
       "      <td>0.328960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299995</th>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>48</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500730</td>\n",
       "      <td>0.662428</td>\n",
       "      <td>0.671927</td>\n",
       "      <td>0.390566</td>\n",
       "      <td>0.145840</td>\n",
       "      <td>0.262767</td>\n",
       "      <td>0.514248</td>\n",
       "      <td>0.519340</td>\n",
       "      <td>0.617436</td>\n",
       "      <td>0.688007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299996</th>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.790664</td>\n",
       "      <td>0.821657</td>\n",
       "      <td>0.620356</td>\n",
       "      <td>0.384891</td>\n",
       "      <td>0.735879</td>\n",
       "      <td>0.547731</td>\n",
       "      <td>0.726653</td>\n",
       "      <td>0.470575</td>\n",
       "      <td>0.275743</td>\n",
       "      <td>0.638939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299997</th>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>33</td>\n",
       "      <td>2</td>\n",
       "      <td>37</td>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.522347</td>\n",
       "      <td>0.407037</td>\n",
       "      <td>0.232436</td>\n",
       "      <td>0.832482</td>\n",
       "      <td>0.810663</td>\n",
       "      <td>0.596939</td>\n",
       "      <td>0.308821</td>\n",
       "      <td>0.373997</td>\n",
       "      <td>0.518024</td>\n",
       "      <td>0.452144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299998</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.812891</td>\n",
       "      <td>0.808045</td>\n",
       "      <td>0.630708</td>\n",
       "      <td>0.346898</td>\n",
       "      <td>0.735147</td>\n",
       "      <td>0.563488</td>\n",
       "      <td>0.609836</td>\n",
       "      <td>0.680430</td>\n",
       "      <td>0.318453</td>\n",
       "      <td>0.335822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299999</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>33</td>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.819735</td>\n",
       "      <td>0.775451</td>\n",
       "      <td>0.848696</td>\n",
       "      <td>0.819377</td>\n",
       "      <td>0.355467</td>\n",
       "      <td>0.218153</td>\n",
       "      <td>0.968856</td>\n",
       "      <td>0.823655</td>\n",
       "      <td>0.330515</td>\n",
       "      <td>0.972569</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300000 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        cat0  cat1  cat2  cat3  cat4  cat5  cat6  cat7  cat8  cat9  ...  \\\n",
       "0          0     8     0     1     1    33     0    44    54     0  ...   \n",
       "1          0     8     0     0     4    33     8    48     3     5  ...   \n",
       "2          0    10     0     0     4    33     0    30    38     9  ...   \n",
       "3          0    10     0     2     4    33     0    50     3     5  ...   \n",
       "4          0     8     6     1     4    33     2    32    54     0  ...   \n",
       "...      ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  ...   \n",
       "299995     0    13     5     0     4    45     0    19    48     0  ...   \n",
       "299996     0    10     0     0     6    33     0    36     4     4  ...   \n",
       "299997     0     6    12     0     7    33     2    37    43     0  ...   \n",
       "299998     1     7     0     3     1    33     0     1    23     0  ...   \n",
       "299999     0     5     2     0     4    33     2    22    55     0  ...   \n",
       "\n",
       "           cont1     cont2     cont3     cont4     cont5     cont6     cont7  \\\n",
       "0       0.855349  0.759439  0.795549  0.681917  0.621672  0.592184  0.791921   \n",
       "1       0.328929  0.386385  0.541366  0.388982  0.357778  0.600044  0.408701   \n",
       "2       0.322749  0.343255  0.616352  0.793687  0.552877  0.352113  0.388835   \n",
       "3       0.707663  0.831147  0.807807  0.800032  0.619147  0.221789  0.897617   \n",
       "4       0.274514  0.338818  0.277308  0.610578  0.128291  0.578764  0.279167   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "299995  0.500730  0.662428  0.671927  0.390566  0.145840  0.262767  0.514248   \n",
       "299996  0.790664  0.821657  0.620356  0.384891  0.735879  0.547731  0.726653   \n",
       "299997  0.522347  0.407037  0.232436  0.832482  0.810663  0.596939  0.308821   \n",
       "299998  0.812891  0.808045  0.630708  0.346898  0.735147  0.563488  0.609836   \n",
       "299999  0.819735  0.775451  0.848696  0.819377  0.355467  0.218153  0.968856   \n",
       "\n",
       "           cont8     cont9    cont10  \n",
       "0       0.815254  0.965006  0.665915  \n",
       "1       0.399353  0.927406  0.493729  \n",
       "2       0.412303  0.292696  0.549452  \n",
       "3       0.633669  0.760318  0.934242  \n",
       "4       0.351103  0.357084  0.328960  \n",
       "...          ...       ...       ...  \n",
       "299995  0.519340  0.617436  0.688007  \n",
       "299996  0.470575  0.275743  0.638939  \n",
       "299997  0.373997  0.518024  0.452144  \n",
       "299998  0.680430  0.318453  0.335822  \n",
       "299999  0.823655  0.330515  0.972569  \n",
       "\n",
       "[300000 rows x 30 columns]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = pd.concat([df3,df1[['cont0','cont1','cont2','cont3','cont4','cont5','cont6','cont7','cont8','cont9','cont10']]],axis=1)\n",
    "#X = pd.concat([df3,df1[['cont1','cont2','cont5','cont6','cont8','cont9']]],axis=1)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X.drop(['cont0','cont9','cat3','cont10','cont7','cont8','cat5','cat7','cont3'],axis=1,inplace=True)\n",
    "#X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 300000 entries, 0 to 299999\n",
      "Data columns (total 30 columns):\n",
      " #   Column  Non-Null Count   Dtype\n",
      "---  ------  --------------   -----\n",
      " 0   cat0    300000 non-null  int32\n",
      " 1   cat1    300000 non-null  int32\n",
      " 2   cat2    300000 non-null  int32\n",
      " 3   cat3    300000 non-null  int32\n",
      " 4   cat4    300000 non-null  int32\n",
      " 5   cat5    300000 non-null  int32\n",
      " 6   cat6    300000 non-null  int32\n",
      " 7   cat7    300000 non-null  int32\n",
      " 8   cat8    300000 non-null  int32\n",
      " 9   cat9    300000 non-null  int32\n",
      " 10  cat10   300000 non-null  int32\n",
      " 11  cat11   300000 non-null  int32\n",
      " 12  cat12   300000 non-null  int32\n",
      " 13  cat13   300000 non-null  int32\n",
      " 14  cat14   300000 non-null  int32\n",
      " 15  cat15   300000 non-null  int32\n",
      " 16  cat16   300000 non-null  int32\n",
      " 17  cat17   300000 non-null  int32\n",
      " 18  cat18   300000 non-null  int32\n",
      " 19  cont0   300000 non-null  int32\n",
      " 20  cont1   300000 non-null  int32\n",
      " 21  cont2   300000 non-null  int32\n",
      " 22  cont3   300000 non-null  int32\n",
      " 23  cont4   300000 non-null  int32\n",
      " 24  cont5   300000 non-null  int32\n",
      " 25  cont6   300000 non-null  int32\n",
      " 26  cont7   300000 non-null  int32\n",
      " 27  cont8   300000 non-null  int32\n",
      " 28  cont9   300000 non-null  int32\n",
      " 29  cont10  300000 non-null  int32\n",
      "dtypes: int32(30)\n",
      "memory usage: 34.3 MB\n"
     ]
    }
   ],
   "source": [
    "X=X.astype('int')\n",
    "X.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the features and split the data into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-5.84920296e-01, -3.98542791e-02, -6.34956934e-01,\n",
       "         3.24427962e-01, -2.50936389e+00,  2.60956863e-01,\n",
       "        -5.96674350e-01,  1.47628735e+00,  1.08344194e+00,\n",
       "        -5.82026220e-01,  1.59106764e+00, -3.98252870e-01,\n",
       "        -4.08269723e-01, -1.57791614e-01, -9.34375034e-01,\n",
       "        -5.60252412e-01,  6.59684429e-01,  4.30845702e-01,\n",
       "        -3.83126522e-01, -1.82574490e-03, -6.32468182e-03,\n",
       "        -8.36689311e-03,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00, -2.78795686e-02,\n",
       "        -6.57414028e-02, -5.77359892e-03, -1.08018646e-02],\n",
       "       [-5.84920296e-01, -3.98542791e-02, -6.34956934e-01,\n",
       "        -5.20099298e-01, -5.31621538e-01,  2.60956863e-01,\n",
       "         3.18564049e+00,  1.76448086e+00, -1.60044309e+00,\n",
       "         1.22990528e+00,  2.81671781e-01, -3.98252870e-01,\n",
       "         2.44936115e+00, -1.57791614e-01,  1.07023408e+00,\n",
       "         1.57444910e+00, -1.49142537e+00,  4.30845702e-01,\n",
       "        -3.83126522e-01, -1.82574490e-03, -6.32468182e-03,\n",
       "        -8.36689311e-03,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00, -2.78795686e-02,\n",
       "        -6.57414028e-02, -5.77359892e-03, -1.08018646e-02],\n",
       "       [-5.84920296e-01,  6.11359432e-01, -6.34956934e-01,\n",
       "        -5.20099298e-01, -5.31621538e-01,  2.60956863e-01,\n",
       "        -5.96674350e-01,  4.67610061e-01,  2.41438789e-01,\n",
       "         2.67945048e+00, -9.86805455e-01, -3.98252870e-01,\n",
       "         2.44936115e+00, -1.57791614e-01, -9.34375034e-01,\n",
       "        -5.60252412e-01,  6.59684429e-01,  4.30845702e-01,\n",
       "        -3.83126522e-01, -1.82574490e-03, -6.32468182e-03,\n",
       "        -8.36689311e-03,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00, -2.78795686e-02,\n",
       "        -6.57414028e-02, -5.77359892e-03, -1.08018646e-02],\n",
       "       [-5.84920296e-01,  6.11359432e-01, -6.34956934e-01,\n",
       "         1.16895522e+00, -5.31621538e-01,  2.60956863e-01,\n",
       "        -5.96674350e-01,  1.90857761e+00, -1.60044309e+00,\n",
       "         1.22990528e+00,  1.35919545e+00, -3.98252870e-01,\n",
       "        -4.08269723e-01, -1.57791614e-01, -9.34375034e-01,\n",
       "        -5.60252412e-01,  6.59684429e-01,  4.30845702e-01,\n",
       "        -3.83126522e-01, -1.82574490e-03, -6.32468182e-03,\n",
       "        -8.36689311e-03,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00, -2.78795686e-02,\n",
       "        -6.57414028e-02, -5.77359892e-03, -1.08018646e-02],\n",
       "       [-5.84920296e-01, -3.98542791e-02,  8.15608776e-01,\n",
       "         3.24427962e-01, -5.31621538e-01,  2.60956863e-01,\n",
       "         3.48904361e-01,  6.11706816e-01,  1.08344194e+00,\n",
       "        -5.82026220e-01, -9.04968214e-01, -3.98252870e-01,\n",
       "        -4.08269723e-01, -1.57791614e-01,  1.07023408e+00,\n",
       "        -5.60252412e-01, -1.49142537e+00,  4.30845702e-01,\n",
       "        -3.83126522e-01, -1.82574490e-03, -6.32468182e-03,\n",
       "        -8.36689311e-03,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00, -2.78795686e-02,\n",
       "        -6.57414028e-02, -5.77359892e-03, -1.08018646e-02]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X= preprocessing.StandardScaler().fit(X).transform(X)\n",
    "X[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y = df1['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\taihs\\anaconda3\\envs\\new_env\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:16:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=200, n_jobs=8, num_parallel_tree=1, random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb = XGBClassifier(n_estimators=200)\n",
    "xgb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.06706327, 0.01498194, 0.01170831, 0.00534046, 0.02624277,\n",
       "       0.01102255, 0.00766307, 0.00865909, 0.00895882, 0.00728115,\n",
       "       0.00829969, 0.03372672, 0.01341711, 0.05380396, 0.0325352 ,\n",
       "       0.06184799, 0.5215331 , 0.01997108, 0.06541748, 0.        ,\n",
       "       0.00357563, 0.00081129, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.00248556, 0.0136539 , 0.        , 0.        ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is 0.843\n",
      "roc-auc is 0.882\n"
     ]
    }
   ],
   "source": [
    "y_pred_class_xgb = xgb.predict(X_test)\n",
    "y_pred_prob_xgb = xgb.predict_proba(X_test)\n",
    "\n",
    "print('accuracy is {:.3f}'.format(accuracy_score(y_test,y_pred_class_xgb)))\n",
    "print('roc-auc is {:.3f}'.format(roc_auc_score(y_test,y_pred_prob_xgb[:,1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(n_estimators=200)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_model = RandomForestClassifier(n_estimators=200)\n",
    "rf_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is 0.830\n",
      "roc-auc is 0.860\n"
     ]
    }
   ],
   "source": [
    "y_pred_class_rf = rf_model.predict(X_test)\n",
    "y_pred_prob_rf = rf_model.predict_proba(X_test)\n",
    "\n",
    "print('accuracy is {:.3f}'.format(accuracy_score(y_test,y_pred_class_rf)))\n",
    "print('roc-auc is {:.3f}'.format(roc_auc_score(y_test,y_pred_prob_rf[:,1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.11293020e-02, 6.92415119e-02, 6.13450715e-02, 3.88797469e-02,\n",
       "       5.46821656e-02, 2.05207805e-02, 3.95478411e-02, 1.00856660e-01,\n",
       "       8.88006708e-02, 3.30349249e-02, 9.13018367e-02, 2.31932003e-02,\n",
       "       8.59478594e-03, 8.02489199e-03, 2.77350148e-02, 9.23186914e-02,\n",
       "       1.30679313e-01, 2.88897135e-02, 5.98572466e-02, 0.00000000e+00,\n",
       "       4.86639854e-05, 2.84325805e-05, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 1.54472097e-04, 1.07958061e-03,\n",
       "       2.21017283e-05, 3.33795442e-05])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_model.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cat0</th>\n",
       "      <th>cat1</th>\n",
       "      <th>cat2</th>\n",
       "      <th>cat3</th>\n",
       "      <th>cat4</th>\n",
       "      <th>cat5</th>\n",
       "      <th>cat6</th>\n",
       "      <th>cat7</th>\n",
       "      <th>cat8</th>\n",
       "      <th>...</th>\n",
       "      <th>cont1</th>\n",
       "      <th>cont2</th>\n",
       "      <th>cont3</th>\n",
       "      <th>cont4</th>\n",
       "      <th>cont5</th>\n",
       "      <th>cont6</th>\n",
       "      <th>cont7</th>\n",
       "      <th>cont8</th>\n",
       "      <th>cont9</th>\n",
       "      <th>cont10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>A</td>\n",
       "      <td>F</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>F</td>\n",
       "      <td>BI</td>\n",
       "      <td>A</td>\n",
       "      <td>AH</td>\n",
       "      <td>AX</td>\n",
       "      <td>...</td>\n",
       "      <td>0.735690</td>\n",
       "      <td>0.578366</td>\n",
       "      <td>0.723154</td>\n",
       "      <td>0.228037</td>\n",
       "      <td>0.356227</td>\n",
       "      <td>0.551249</td>\n",
       "      <td>0.655693</td>\n",
       "      <td>0.598331</td>\n",
       "      <td>0.359987</td>\n",
       "      <td>0.947489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>A</td>\n",
       "      <td>H</td>\n",
       "      <td>C</td>\n",
       "      <td>A</td>\n",
       "      <td>E</td>\n",
       "      <td>AB</td>\n",
       "      <td>I</td>\n",
       "      <td>F</td>\n",
       "      <td>N</td>\n",
       "      <td>...</td>\n",
       "      <td>0.313703</td>\n",
       "      <td>0.928885</td>\n",
       "      <td>0.516602</td>\n",
       "      <td>0.600169</td>\n",
       "      <td>0.795224</td>\n",
       "      <td>0.248987</td>\n",
       "      <td>0.654614</td>\n",
       "      <td>0.347944</td>\n",
       "      <td>0.565520</td>\n",
       "      <td>0.388580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>A</td>\n",
       "      <td>N</td>\n",
       "      <td>C</td>\n",
       "      <td>A</td>\n",
       "      <td>F</td>\n",
       "      <td>AB</td>\n",
       "      <td>A</td>\n",
       "      <td>AH</td>\n",
       "      <td>BC</td>\n",
       "      <td>...</td>\n",
       "      <td>0.448201</td>\n",
       "      <td>0.424876</td>\n",
       "      <td>0.344729</td>\n",
       "      <td>0.242073</td>\n",
       "      <td>0.270632</td>\n",
       "      <td>0.746740</td>\n",
       "      <td>0.335590</td>\n",
       "      <td>0.341238</td>\n",
       "      <td>0.252289</td>\n",
       "      <td>0.411592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>B</td>\n",
       "      <td>L</td>\n",
       "      <td>C</td>\n",
       "      <td>A</td>\n",
       "      <td>F</td>\n",
       "      <td>BI</td>\n",
       "      <td>A</td>\n",
       "      <td>E</td>\n",
       "      <td>AX</td>\n",
       "      <td>...</td>\n",
       "      <td>0.666092</td>\n",
       "      <td>0.598943</td>\n",
       "      <td>0.561971</td>\n",
       "      <td>0.806347</td>\n",
       "      <td>0.735983</td>\n",
       "      <td>0.538724</td>\n",
       "      <td>0.381566</td>\n",
       "      <td>0.481660</td>\n",
       "      <td>0.348514</td>\n",
       "      <td>0.325723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>A</td>\n",
       "      <td>F</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>F</td>\n",
       "      <td>BI</td>\n",
       "      <td>A</td>\n",
       "      <td>AH</td>\n",
       "      <td>I</td>\n",
       "      <td>...</td>\n",
       "      <td>0.772229</td>\n",
       "      <td>0.479572</td>\n",
       "      <td>0.767745</td>\n",
       "      <td>0.252454</td>\n",
       "      <td>0.354810</td>\n",
       "      <td>0.178920</td>\n",
       "      <td>0.763479</td>\n",
       "      <td>0.562491</td>\n",
       "      <td>0.466261</td>\n",
       "      <td>0.585781</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id cat0 cat1 cat2 cat3 cat4 cat5 cat6 cat7 cat8  ...     cont1     cont2  \\\n",
       "0   5    A    F    A    A    F   BI    A   AH   AX  ...  0.735690  0.578366   \n",
       "1   6    A    H    C    A    E   AB    I    F    N  ...  0.313703  0.928885   \n",
       "2   8    A    N    C    A    F   AB    A   AH   BC  ...  0.448201  0.424876   \n",
       "3   9    B    L    C    A    F   BI    A    E   AX  ...  0.666092  0.598943   \n",
       "4  11    A    F    A    B    F   BI    A   AH    I  ...  0.772229  0.479572   \n",
       "\n",
       "      cont3     cont4     cont5     cont6     cont7     cont8     cont9  \\\n",
       "0  0.723154  0.228037  0.356227  0.551249  0.655693  0.598331  0.359987   \n",
       "1  0.516602  0.600169  0.795224  0.248987  0.654614  0.347944  0.565520   \n",
       "2  0.344729  0.242073  0.270632  0.746740  0.335590  0.341238  0.252289   \n",
       "3  0.561971  0.806347  0.735983  0.538724  0.381566  0.481660  0.348514   \n",
       "4  0.767745  0.252454  0.354810  0.178920  0.763479  0.562491  0.466261   \n",
       "\n",
       "     cont10  \n",
       "0  0.947489  \n",
       "1  0.388580  \n",
       "2  0.411592  \n",
       "3  0.325723  \n",
       "4  0.585781  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.read_csv('C:/Users/taihs/OneDrive/Documents/tps competition/test.csv')\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "lst = res['Column'].tolist()\n",
    "df3 = pd.DataFrame()\n",
    "for i in lst:\n",
    "    icat = le.fit_transform(df1[i])\n",
    "    df2 = pd.DataFrame({i:icat})\n",
    "    df3 = pd.concat([df3,df2],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat0</th>\n",
       "      <th>cat1</th>\n",
       "      <th>cat2</th>\n",
       "      <th>cat3</th>\n",
       "      <th>cat4</th>\n",
       "      <th>cat5</th>\n",
       "      <th>cat6</th>\n",
       "      <th>cat7</th>\n",
       "      <th>cat8</th>\n",
       "      <th>cat9</th>\n",
       "      <th>cat10</th>\n",
       "      <th>cat11</th>\n",
       "      <th>cat12</th>\n",
       "      <th>cat13</th>\n",
       "      <th>cat14</th>\n",
       "      <th>cat15</th>\n",
       "      <th>cat16</th>\n",
       "      <th>cat17</th>\n",
       "      <th>cat18</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>249</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>31</td>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "      <td>269</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>121</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>162</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>46</td>\n",
       "      <td>0</td>\n",
       "      <td>173</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199995</th>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>75</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199996</th>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>55</td>\n",
       "      <td>4</td>\n",
       "      <td>180</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199997</th>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>171</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199998</th>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>43</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>162</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199999</th>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>33</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>38</td>\n",
       "      <td>7</td>\n",
       "      <td>69</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200000 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        cat0  cat1  cat2  cat3  cat4  cat5  cat6  cat7  cat8  cat9  cat10  \\\n",
       "0          0     5     0     0     5    33     0     8    23     0    249   \n",
       "1          0     7     2     0     4     2     7    31    51     0    269   \n",
       "2          0    13     2     0     5     2     0     8    28     0    121   \n",
       "3          1    11     2     0     5    33     0    30    23     0    162   \n",
       "4          0     5     0     1     5    33     0     8    46     0    173   \n",
       "...      ...   ...   ...   ...   ...   ...   ...   ...   ...   ...    ...   \n",
       "199995     0    13     0     3     5    33     0     6    23     0     75   \n",
       "199996     1     8     0     1     4    33     0    19    55     4    180   \n",
       "199997     0    11     3     0     7    33     0    14    47     0    171   \n",
       "199998     0    10     0     2     5    33     0    43    38     0    162   \n",
       "199999     0    10     0     0     4    33     2    15    38     7     69   \n",
       "\n",
       "        cat11  cat12  cat13  cat14  cat15  cat16  cat17  cat18  \n",
       "0           0      0      0      0      1      3      3      1  \n",
       "1           0      0      0      1      3      1      3      1  \n",
       "2           0      0      0      1      1      3      3      1  \n",
       "3           0      0      0      0      1      3      1      1  \n",
       "4           0      0      0      0      3      3      3      1  \n",
       "...       ...    ...    ...    ...    ...    ...    ...    ...  \n",
       "199995      0      0      0      1      3      1      2      1  \n",
       "199996      0      0      0      1      1      3      3      1  \n",
       "199997      0      0      0      1      3      1      3      1  \n",
       "199998      0      0      0      0      1      3      3      1  \n",
       "199999      0      1      0      0      1      1      1      1  \n",
       "\n",
       "[200000 rows x 19 columns]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat0</th>\n",
       "      <th>cat1</th>\n",
       "      <th>cat2</th>\n",
       "      <th>cat3</th>\n",
       "      <th>cat4</th>\n",
       "      <th>cat5</th>\n",
       "      <th>cat6</th>\n",
       "      <th>cat7</th>\n",
       "      <th>cat8</th>\n",
       "      <th>cat9</th>\n",
       "      <th>...</th>\n",
       "      <th>cont1</th>\n",
       "      <th>cont2</th>\n",
       "      <th>cont3</th>\n",
       "      <th>cont4</th>\n",
       "      <th>cont5</th>\n",
       "      <th>cont6</th>\n",
       "      <th>cont7</th>\n",
       "      <th>cont8</th>\n",
       "      <th>cont9</th>\n",
       "      <th>cont10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.735690</td>\n",
       "      <td>0.578366</td>\n",
       "      <td>0.723154</td>\n",
       "      <td>0.228037</td>\n",
       "      <td>0.356227</td>\n",
       "      <td>0.551249</td>\n",
       "      <td>0.655693</td>\n",
       "      <td>0.598331</td>\n",
       "      <td>0.359987</td>\n",
       "      <td>0.947489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>31</td>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.313703</td>\n",
       "      <td>0.928885</td>\n",
       "      <td>0.516602</td>\n",
       "      <td>0.600169</td>\n",
       "      <td>0.795224</td>\n",
       "      <td>0.248987</td>\n",
       "      <td>0.654614</td>\n",
       "      <td>0.347944</td>\n",
       "      <td>0.565520</td>\n",
       "      <td>0.388580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.448201</td>\n",
       "      <td>0.424876</td>\n",
       "      <td>0.344729</td>\n",
       "      <td>0.242073</td>\n",
       "      <td>0.270632</td>\n",
       "      <td>0.746740</td>\n",
       "      <td>0.335590</td>\n",
       "      <td>0.341238</td>\n",
       "      <td>0.252289</td>\n",
       "      <td>0.411592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.666092</td>\n",
       "      <td>0.598943</td>\n",
       "      <td>0.561971</td>\n",
       "      <td>0.806347</td>\n",
       "      <td>0.735983</td>\n",
       "      <td>0.538724</td>\n",
       "      <td>0.381566</td>\n",
       "      <td>0.481660</td>\n",
       "      <td>0.348514</td>\n",
       "      <td>0.325723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>46</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.772229</td>\n",
       "      <td>0.479572</td>\n",
       "      <td>0.767745</td>\n",
       "      <td>0.252454</td>\n",
       "      <td>0.354810</td>\n",
       "      <td>0.178920</td>\n",
       "      <td>0.763479</td>\n",
       "      <td>0.562491</td>\n",
       "      <td>0.466261</td>\n",
       "      <td>0.585781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199995</th>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.361426</td>\n",
       "      <td>0.351946</td>\n",
       "      <td>0.327670</td>\n",
       "      <td>0.205547</td>\n",
       "      <td>0.679195</td>\n",
       "      <td>0.485967</td>\n",
       "      <td>0.319130</td>\n",
       "      <td>0.520681</td>\n",
       "      <td>0.519545</td>\n",
       "      <td>0.427119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199996</th>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>55</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.551106</td>\n",
       "      <td>0.628843</td>\n",
       "      <td>0.677765</td>\n",
       "      <td>0.624935</td>\n",
       "      <td>0.555306</td>\n",
       "      <td>0.242424</td>\n",
       "      <td>0.700829</td>\n",
       "      <td>0.531728</td>\n",
       "      <td>0.528427</td>\n",
       "      <td>0.922645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199997</th>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.812384</td>\n",
       "      <td>0.803348</td>\n",
       "      <td>0.324762</td>\n",
       "      <td>0.665624</td>\n",
       "      <td>0.488447</td>\n",
       "      <td>0.853213</td>\n",
       "      <td>0.578641</td>\n",
       "      <td>0.811941</td>\n",
       "      <td>0.537106</td>\n",
       "      <td>0.531758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199998</th>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>43</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.811282</td>\n",
       "      <td>0.820635</td>\n",
       "      <td>0.561449</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.555089</td>\n",
       "      <td>0.746532</td>\n",
       "      <td>0.369986</td>\n",
       "      <td>0.438712</td>\n",
       "      <td>0.715524</td>\n",
       "      <td>0.381978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199999</th>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>33</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>38</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>0.336168</td>\n",
       "      <td>0.270483</td>\n",
       "      <td>0.581868</td>\n",
       "      <td>0.218993</td>\n",
       "      <td>0.553284</td>\n",
       "      <td>0.565213</td>\n",
       "      <td>0.378355</td>\n",
       "      <td>0.547927</td>\n",
       "      <td>0.273595</td>\n",
       "      <td>0.448016</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200000 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        cat0  cat1  cat2  cat3  cat4  cat5  cat6  cat7  cat8  cat9  ...  \\\n",
       "0          0     5     0     0     5    33     0     8    23     0  ...   \n",
       "1          0     7     2     0     4     2     7    31    51     0  ...   \n",
       "2          0    13     2     0     5     2     0     8    28     0  ...   \n",
       "3          1    11     2     0     5    33     0    30    23     0  ...   \n",
       "4          0     5     0     1     5    33     0     8    46     0  ...   \n",
       "...      ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  ...   \n",
       "199995     0    13     0     3     5    33     0     6    23     0  ...   \n",
       "199996     1     8     0     1     4    33     0    19    55     4  ...   \n",
       "199997     0    11     3     0     7    33     0    14    47     0  ...   \n",
       "199998     0    10     0     2     5    33     0    43    38     0  ...   \n",
       "199999     0    10     0     0     4    33     2    15    38     7  ...   \n",
       "\n",
       "           cont1     cont2     cont3     cont4     cont5     cont6     cont7  \\\n",
       "0       0.735690  0.578366  0.723154  0.228037  0.356227  0.551249  0.655693   \n",
       "1       0.313703  0.928885  0.516602  0.600169  0.795224  0.248987  0.654614   \n",
       "2       0.448201  0.424876  0.344729  0.242073  0.270632  0.746740  0.335590   \n",
       "3       0.666092  0.598943  0.561971  0.806347  0.735983  0.538724  0.381566   \n",
       "4       0.772229  0.479572  0.767745  0.252454  0.354810  0.178920  0.763479   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "199995  0.361426  0.351946  0.327670  0.205547  0.679195  0.485967  0.319130   \n",
       "199996  0.551106  0.628843  0.677765  0.624935  0.555306  0.242424  0.700829   \n",
       "199997  0.812384  0.803348  0.324762  0.665624  0.488447  0.853213  0.578641   \n",
       "199998  0.811282  0.820635  0.561449  0.797434  0.555089  0.746532  0.369986   \n",
       "199999  0.336168  0.270483  0.581868  0.218993  0.553284  0.565213  0.378355   \n",
       "\n",
       "           cont8     cont9    cont10  \n",
       "0       0.598331  0.359987  0.947489  \n",
       "1       0.347944  0.565520  0.388580  \n",
       "2       0.341238  0.252289  0.411592  \n",
       "3       0.481660  0.348514  0.325723  \n",
       "4       0.562491  0.466261  0.585781  \n",
       "...          ...       ...       ...  \n",
       "199995  0.520681  0.519545  0.427119  \n",
       "199996  0.531728  0.528427  0.922645  \n",
       "199997  0.811941  0.537106  0.531758  \n",
       "199998  0.438712  0.715524  0.381978  \n",
       "199999  0.547927  0.273595  0.448016  \n",
       "\n",
       "[200000 rows x 30 columns]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = pd.concat([df3,df1[['cont0','cont1','cont2','cont3','cont4','cont5','cont6','cont7','cont8','cont9','cont10']]],axis=1)\n",
    "#X = pd.concat([df3,df1[['cont1','cont2','cont5','cont6','cont8','cont9']]],axis=1)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X.drop(['cont0','cont9','cat3','cont10','cont7','cont8','cat5','cat7','cont3'],axis=1,inplace=True)\n",
    "#X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 200000 entries, 0 to 199999\n",
      "Data columns (total 30 columns):\n",
      " #   Column  Non-Null Count   Dtype\n",
      "---  ------  --------------   -----\n",
      " 0   cat0    200000 non-null  int32\n",
      " 1   cat1    200000 non-null  int32\n",
      " 2   cat2    200000 non-null  int32\n",
      " 3   cat3    200000 non-null  int32\n",
      " 4   cat4    200000 non-null  int32\n",
      " 5   cat5    200000 non-null  int32\n",
      " 6   cat6    200000 non-null  int32\n",
      " 7   cat7    200000 non-null  int32\n",
      " 8   cat8    200000 non-null  int32\n",
      " 9   cat9    200000 non-null  int32\n",
      " 10  cat10   200000 non-null  int32\n",
      " 11  cat11   200000 non-null  int32\n",
      " 12  cat12   200000 non-null  int32\n",
      " 13  cat13   200000 non-null  int32\n",
      " 14  cat14   200000 non-null  int32\n",
      " 15  cat15   200000 non-null  int32\n",
      " 16  cat16   200000 non-null  int32\n",
      " 17  cat17   200000 non-null  int32\n",
      " 18  cat18   200000 non-null  int32\n",
      " 19  cont0   200000 non-null  int32\n",
      " 20  cont1   200000 non-null  int32\n",
      " 21  cont2   200000 non-null  int32\n",
      " 22  cont3   200000 non-null  int32\n",
      " 23  cont4   200000 non-null  int32\n",
      " 24  cont5   200000 non-null  int32\n",
      " 25  cont6   200000 non-null  int32\n",
      " 26  cont7   200000 non-null  int32\n",
      " 27  cont8   200000 non-null  int32\n",
      " 28  cont9   200000 non-null  int32\n",
      " 29  cont10  200000 non-null  int32\n",
      "dtypes: int32(30)\n",
      "memory usage: 22.9 MB\n"
     ]
    }
   ],
   "source": [
    "X=X.astype('int')\n",
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-5.84871536e-01, -1.01120305e+00, -6.36230407e-01,\n",
       "        -5.18058919e-01,  1.29465996e-01,  2.65387358e-01,\n",
       "        -5.97737446e-01, -1.11838839e+00, -5.52753344e-01,\n",
       "        -5.86271243e-01,  1.47795544e+00, -3.98550559e-01,\n",
       "        -4.10999678e-01, -1.59865185e-01, -9.35975115e-01,\n",
       "        -5.62749702e-01,  6.59929099e-01,  4.28600036e-01,\n",
       "        -3.85341368e-01, -2.23607357e-03, -6.32468182e-03,\n",
       "        -7.74619908e-03,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00, -2.88216841e-02,\n",
       "        -6.52923950e-02, -7.07124460e-03, -1.11810387e-02],\n",
       "       [-5.84871536e-01, -3.62062827e-01, -1.52758874e-01,\n",
       "        -5.18058919e-01, -5.33136983e-01, -2.17765193e+00,\n",
       "         2.71391909e+00,  5.37757899e-01,  9.20319820e-01,\n",
       "        -5.86271243e-01,  1.75172996e+00, -3.98550559e-01,\n",
       "        -4.10999678e-01, -1.59865185e-01,  1.06840447e+00,\n",
       "         1.56988773e+00, -1.49372835e+00,  4.28600036e-01,\n",
       "        -3.85341368e-01, -2.23607357e-03, -6.32468182e-03,\n",
       "        -7.74619908e-03,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00, -2.88216841e-02,\n",
       "        -6.52923950e-02, -7.07124460e-03, -1.11810387e-02],\n",
       "       [-5.84871536e-01,  1.58535784e+00, -1.52758874e-01,\n",
       "        -5.18058919e-01,  1.29465996e-01, -2.17765193e+00,\n",
       "        -5.97737446e-01, -1.11838839e+00, -2.89704564e-01,\n",
       "        -5.86271243e-01, -2.74201469e-01, -3.98550559e-01,\n",
       "        -4.10999678e-01, -1.59865185e-01,  1.06840447e+00,\n",
       "        -5.62749702e-01,  6.59929099e-01,  4.28600036e-01,\n",
       "        -3.85341368e-01, -2.23607357e-03, -6.32468182e-03,\n",
       "        -7.74619908e-03,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00, -2.88216841e-02,\n",
       "        -6.52923950e-02, -7.07124460e-03, -1.11810387e-02],\n",
       "       [ 1.70977717e+00,  9.36217616e-01, -1.52758874e-01,\n",
       "        -5.18058919e-01,  1.29465996e-01,  2.65387358e-01,\n",
       "        -5.97737446e-01,  4.65751539e-01, -5.52753344e-01,\n",
       "        -5.86271243e-01,  2.87036292e-01, -3.98550559e-01,\n",
       "        -4.10999678e-01, -1.59865185e-01, -9.35975115e-01,\n",
       "        -5.62749702e-01,  6.59929099e-01, -2.74786210e+00,\n",
       "        -3.85341368e-01, -2.23607357e-03, -6.32468182e-03,\n",
       "        -7.74619908e-03,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00, -2.88216841e-02,\n",
       "        -6.52923950e-02, -7.07124460e-03, -1.11810387e-02],\n",
       "       [-5.84871536e-01, -1.01120305e+00, -6.36230407e-01,\n",
       "         3.14271210e-01,  1.29465996e-01,  2.65387358e-01,\n",
       "        -5.97737446e-01, -1.11838839e+00,  6.57271041e-01,\n",
       "        -5.86271243e-01,  4.37612276e-01, -3.98550559e-01,\n",
       "        -4.10999678e-01, -1.59865185e-01, -9.35975115e-01,\n",
       "         1.56988773e+00,  6.59929099e-01,  4.28600036e-01,\n",
       "        -3.85341368e-01, -2.23607357e-03, -6.32468182e-03,\n",
       "        -7.74619908e-03,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00, -2.88216841e-02,\n",
       "        -6.52923950e-02, -7.07124460e-03, -1.11810387e-02]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X= preprocessing.StandardScaler().fit(X).transform(X)\n",
    "X[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred_class_rf = rf_model.predict(X)\n",
    "y_pred_prob_rf = rf_model.predict_proba(X)\n",
    "\n",
    "y_pred_class_xgb = xgb.predict(X)\n",
    "y_pred_prob_xgb = xgb.predict_proba(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.00081129, 0.00248556, 0.00357563,\n",
       "       0.00534046, 0.00728115, 0.00766307, 0.00829969, 0.00865909,\n",
       "       0.00895882, 0.01102255, 0.01170831, 0.01341711, 0.0136539 ,\n",
       "       0.01498194, 0.01997108, 0.02624277, 0.0325352 , 0.03372672,\n",
       "       0.05380396, 0.06184799, 0.06541748, 0.06706327, 0.5215331 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_model.feature_importances_\n",
    "np.sort(xgb.feature_importances_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.16220756, 0.30961773, 0.04213469, ..., 0.69286907, 0.03106131,\n",
       "       0.6059582 ], dtype=float32)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#y_pred_prob_rf[:,1]\n",
    "y_pred_prob_xgb[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>199995</th>\n",
       "      <td>0.871420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199996</th>\n",
       "      <td>0.014474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199997</th>\n",
       "      <td>0.692869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199998</th>\n",
       "      <td>0.031061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199999</th>\n",
       "      <td>0.605958</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          target\n",
       "199995  0.871420\n",
       "199996  0.014474\n",
       "199997  0.692869\n",
       "199998  0.031061\n",
       "199999  0.605958"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#final = pd.DataFrame({'target':y_pred_prob_rf[:,1]})\n",
    "final = pd.DataFrame({'target':y_pred_prob_xgb[:,1]})\n",
    "final.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>199995</th>\n",
       "      <td>499983</td>\n",
       "      <td>0.871420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199996</th>\n",
       "      <td>499984</td>\n",
       "      <td>0.014474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199997</th>\n",
       "      <td>499987</td>\n",
       "      <td>0.692869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199998</th>\n",
       "      <td>499994</td>\n",
       "      <td>0.031061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199999</th>\n",
       "      <td>499998</td>\n",
       "      <td>0.605958</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id    target\n",
       "199995  499983  0.871420\n",
       "199996  499984  0.014474\n",
       "199997  499987  0.692869\n",
       "199998  499994  0.031061\n",
       "199999  499998  0.605958"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final1 = pd.concat([df1['id'],final],axis=1)\n",
    "final1.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "final1.to_csv('sample_submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a Neural Network model with one hidden layer with sigmoid activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 12)                300       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 13        \n",
      "=================================================================\n",
      "Total params: 313\n",
      "Trainable params: 313\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_1 = Sequential()\n",
    "model_1.add(Dense(12,input_shape = (24,),activation = 'sigmoid'))\n",
    "model_1.add(Dense(1,activation='sigmoid'))\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    C:\\Users\\taihs\\anaconda3\\envs\\new_env\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:806 train_function  *\n        return step_function(self, iterator)\n    C:\\Users\\taihs\\anaconda3\\envs\\new_env\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:796 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    C:\\Users\\taihs\\anaconda3\\envs\\new_env\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1211 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    C:\\Users\\taihs\\anaconda3\\envs\\new_env\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2585 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    C:\\Users\\taihs\\anaconda3\\envs\\new_env\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2945 _call_for_each_replica\n        return fn(*args, **kwargs)\n    C:\\Users\\taihs\\anaconda3\\envs\\new_env\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:789 run_step  **\n        outputs = model.train_step(data)\n    C:\\Users\\taihs\\anaconda3\\envs\\new_env\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:747 train_step\n        y_pred = self(x, training=True)\n    C:\\Users\\taihs\\anaconda3\\envs\\new_env\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:975 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs,\n    C:\\Users\\taihs\\anaconda3\\envs\\new_env\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\input_spec.py:212 assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer sequential is incompatible with the layer: expected axis -1 of input shape to have value 24 but received input with shape [32, 30]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-90-13d064fab0d2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel_1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSGD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m.003\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"binary_crossentropy\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"accuracy\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mrun_hist_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\new_env\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\new_env\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\new_env\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\new_env\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    821\u001b[0m       \u001b[1;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    822\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 823\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    824\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    825\u001b[0m       \u001b[1;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\new_env\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[1;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[0;32m    694\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_deleter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFunctionDeleter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lifted_initializer_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    695\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m--> 696\u001b[1;33m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m    697\u001b[0m             *args, **kwds))\n\u001b[0;32m    698\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\new_env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2853\u001b[0m       \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2854\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2855\u001b[1;33m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2856\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2857\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\new_env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   3211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3212\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3213\u001b[1;33m       \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3214\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3215\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\new_env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   3063\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3064\u001b[0m     graph_function = ConcreteFunction(\n\u001b[1;32m-> 3065\u001b[1;33m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[0;32m   3066\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3067\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\new_env\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    984\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    985\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 986\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    987\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    988\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\new_env\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    598\u001b[0m         \u001b[1;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    599\u001b[0m         \u001b[1;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 600\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    601\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mref\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    602\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\new_env\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    971\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    972\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 973\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    974\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    975\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    C:\\Users\\taihs\\anaconda3\\envs\\new_env\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:806 train_function  *\n        return step_function(self, iterator)\n    C:\\Users\\taihs\\anaconda3\\envs\\new_env\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:796 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    C:\\Users\\taihs\\anaconda3\\envs\\new_env\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1211 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    C:\\Users\\taihs\\anaconda3\\envs\\new_env\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2585 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    C:\\Users\\taihs\\anaconda3\\envs\\new_env\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2945 _call_for_each_replica\n        return fn(*args, **kwargs)\n    C:\\Users\\taihs\\anaconda3\\envs\\new_env\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:789 run_step  **\n        outputs = model.train_step(data)\n    C:\\Users\\taihs\\anaconda3\\envs\\new_env\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:747 train_step\n        y_pred = self(x, training=True)\n    C:\\Users\\taihs\\anaconda3\\envs\\new_env\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:975 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs,\n    C:\\Users\\taihs\\anaconda3\\envs\\new_env\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\input_spec.py:212 assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer sequential is incompatible with the layer: expected axis -1 of input shape to have value 24 but received input with shape [32, 30]\n"
     ]
    }
   ],
   "source": [
    "model_1.compile(SGD(lr = .003), \"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "run_hist_1 = model_1.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-167-98292c949c9b>:1: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
      "Instructions for updating:\n",
      "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n"
     ]
    }
   ],
   "source": [
    "y_pred_class_nn_1 = model_1.predict_classes(X_test)\n",
    "y_pred_prob_nn_1 = model_1.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is 0.837\n",
      "roc-auc is 0.869\n"
     ]
    }
   ],
   "source": [
    "print('accuracy is {:.3f}'.format(accuracy_score(y_test,y_pred_class_nn_1)))\n",
    "print('roc-auc is {:.3f}'.format(roc_auc_score(y_test,y_pred_prob_nn_1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1ab8d68e880>"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(run_hist_1.history[\"loss\"],'r', marker='.', label=\"Train Loss\")\n",
    "ax.plot(run_hist_1.history[\"val_loss\"],'b', marker='.', label=\"Validation Loss\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add two hidden layers to the Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 12)                300       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 12)                156       \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 13        \n",
      "=================================================================\n",
      "Total params: 469\n",
      "Trainable params: 469\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_1 = Sequential()\n",
    "model_1.add(Dense(12,input_shape = (24,),activation = 'sigmoid'))\n",
    "model_1.add(Dense(12,activation='sigmoid'))\n",
    "model_1.add(Dense(1,activation='sigmoid'))\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "7500/7500 [==============================] - 4s 562us/step - loss: 0.5581 - accuracy: 0.7268 - val_loss: 0.5198 - val_accuracy: 0.7362\n",
      "Epoch 2/50\n",
      "7500/7500 [==============================] - 4s 530us/step - loss: 0.4736 - accuracy: 0.7633 - val_loss: 0.4299 - val_accuracy: 0.8266\n",
      "Epoch 3/50\n",
      "7500/7500 [==============================] - 4s 536us/step - loss: 0.4084 - accuracy: 0.8319 - val_loss: 0.3982 - val_accuracy: 0.8311\n",
      "Epoch 4/50\n",
      "7500/7500 [==============================] - 4s 536us/step - loss: 0.3932 - accuracy: 0.8326 - val_loss: 0.3925 - val_accuracy: 0.8310\n",
      "Epoch 5/50\n",
      "7500/7500 [==============================] - 4s 533us/step - loss: 0.3893 - accuracy: 0.8332 - val_loss: 0.3901 - val_accuracy: 0.8325\n",
      "Epoch 6/50\n",
      "7500/7500 [==============================] - 4s 535us/step - loss: 0.3872 - accuracy: 0.8339 - val_loss: 0.3884 - val_accuracy: 0.8328\n",
      "Epoch 7/50\n",
      "7500/7500 [==============================] - 4s 564us/step - loss: 0.3857 - accuracy: 0.8345 - val_loss: 0.3872 - val_accuracy: 0.8328\n",
      "Epoch 8/50\n",
      "7500/7500 [==============================] - 4s 564us/step - loss: 0.3846 - accuracy: 0.8350 - val_loss: 0.3863 - val_accuracy: 0.8331\n",
      "Epoch 9/50\n",
      "7500/7500 [==============================] - 4s 556us/step - loss: 0.3837 - accuracy: 0.8351 - val_loss: 0.3855 - val_accuracy: 0.8338\n",
      "Epoch 10/50\n",
      "7500/7500 [==============================] - 4s 544us/step - loss: 0.3831 - accuracy: 0.8353 - val_loss: 0.3849 - val_accuracy: 0.8340\n",
      "Epoch 11/50\n",
      "7500/7500 [==============================] - 4s 552us/step - loss: 0.3825 - accuracy: 0.8356 - val_loss: 0.3844 - val_accuracy: 0.8345\n",
      "Epoch 12/50\n",
      "7500/7500 [==============================] - 4s 549us/step - loss: 0.3820 - accuracy: 0.8356 - val_loss: 0.3839 - val_accuracy: 0.8347\n",
      "Epoch 13/50\n",
      "7500/7500 [==============================] - 4s 551us/step - loss: 0.3816 - accuracy: 0.8358 - val_loss: 0.3835 - val_accuracy: 0.8355\n",
      "Epoch 14/50\n",
      "7500/7500 [==============================] - 4s 582us/step - loss: 0.3813 - accuracy: 0.8358 - val_loss: 0.3832 - val_accuracy: 0.8354\n",
      "Epoch 15/50\n",
      "7500/7500 [==============================] - 4s 545us/step - loss: 0.3810 - accuracy: 0.8362 - val_loss: 0.3829 - val_accuracy: 0.8357\n",
      "Epoch 16/50\n",
      "7500/7500 [==============================] - 4s 551us/step - loss: 0.3807 - accuracy: 0.8362 - val_loss: 0.3826 - val_accuracy: 0.8356\n",
      "Epoch 17/50\n",
      "7500/7500 [==============================] - 4s 547us/step - loss: 0.3805 - accuracy: 0.8363 - val_loss: 0.3824 - val_accuracy: 0.8359\n",
      "Epoch 18/50\n",
      "7500/7500 [==============================] - 4s 544us/step - loss: 0.3803 - accuracy: 0.8364 - val_loss: 0.3822 - val_accuracy: 0.8359\n",
      "Epoch 19/50\n",
      "7500/7500 [==============================] - 4s 550us/step - loss: 0.3801 - accuracy: 0.8364 - val_loss: 0.3820 - val_accuracy: 0.8360\n",
      "Epoch 20/50\n",
      "7500/7500 [==============================] - 4s 551us/step - loss: 0.3799 - accuracy: 0.8366 - val_loss: 0.3818 - val_accuracy: 0.8360\n",
      "Epoch 21/50\n",
      "7500/7500 [==============================] - 4s 547us/step - loss: 0.3797 - accuracy: 0.8366 - val_loss: 0.3816 - val_accuracy: 0.8363\n",
      "Epoch 22/50\n",
      "7500/7500 [==============================] - 4s 546us/step - loss: 0.3795 - accuracy: 0.8369 - val_loss: 0.3814 - val_accuracy: 0.8363\n",
      "Epoch 23/50\n",
      "7500/7500 [==============================] - 4s 552us/step - loss: 0.3794 - accuracy: 0.8369 - val_loss: 0.3813 - val_accuracy: 0.8363\n",
      "Epoch 24/50\n",
      "7500/7500 [==============================] - 4s 559us/step - loss: 0.3792 - accuracy: 0.8371 - val_loss: 0.3812 - val_accuracy: 0.8368\n",
      "Epoch 25/50\n",
      "7500/7500 [==============================] - 4s 547us/step - loss: 0.3791 - accuracy: 0.8370 - val_loss: 0.3810 - val_accuracy: 0.8365\n",
      "Epoch 26/50\n",
      "7500/7500 [==============================] - 4s 555us/step - loss: 0.3789 - accuracy: 0.8372 - val_loss: 0.3809 - val_accuracy: 0.8367\n",
      "Epoch 27/50\n",
      "7500/7500 [==============================] - 4s 555us/step - loss: 0.3788 - accuracy: 0.8373 - val_loss: 0.3808 - val_accuracy: 0.8368\n",
      "Epoch 28/50\n",
      "7500/7500 [==============================] - 4s 551us/step - loss: 0.3787 - accuracy: 0.8375 - val_loss: 0.3806 - val_accuracy: 0.8369\n",
      "Epoch 29/50\n",
      "7500/7500 [==============================] - 4s 548us/step - loss: 0.3786 - accuracy: 0.8374 - val_loss: 0.3805 - val_accuracy: 0.8371\n",
      "Epoch 30/50\n",
      "7500/7500 [==============================] - 4s 553us/step - loss: 0.3785 - accuracy: 0.8376 - val_loss: 0.3804 - val_accuracy: 0.8371\n",
      "Epoch 31/50\n",
      "7500/7500 [==============================] - 5s 622us/step - loss: 0.3783 - accuracy: 0.8378 - val_loss: 0.3803 - val_accuracy: 0.8371\n",
      "Epoch 32/50\n",
      "7500/7500 [==============================] - 4s 535us/step - loss: 0.3782 - accuracy: 0.8376 - val_loss: 0.3802 - val_accuracy: 0.8373\n",
      "Epoch 33/50\n",
      "7500/7500 [==============================] - 4s 535us/step - loss: 0.3781 - accuracy: 0.8377 - val_loss: 0.3801 - val_accuracy: 0.8373\n",
      "Epoch 34/50\n",
      "7500/7500 [==============================] - 4s 537us/step - loss: 0.3780 - accuracy: 0.8379 - val_loss: 0.3800 - val_accuracy: 0.8370\n",
      "Epoch 35/50\n",
      "7500/7500 [==============================] - 4s 534us/step - loss: 0.3779 - accuracy: 0.8379 - val_loss: 0.3799 - val_accuracy: 0.8374\n",
      "Epoch 36/50\n",
      "7500/7500 [==============================] - 4s 534us/step - loss: 0.3778 - accuracy: 0.8379 - val_loss: 0.3798 - val_accuracy: 0.8369\n",
      "Epoch 37/50\n",
      "7500/7500 [==============================] - 4s 538us/step - loss: 0.3777 - accuracy: 0.8380 - val_loss: 0.3797 - val_accuracy: 0.8376\n",
      "Epoch 38/50\n",
      "7500/7500 [==============================] - 4s 538us/step - loss: 0.3777 - accuracy: 0.8380 - val_loss: 0.3797 - val_accuracy: 0.8371\n",
      "Epoch 39/50\n",
      "7500/7500 [==============================] - 4s 535us/step - loss: 0.3776 - accuracy: 0.8379 - val_loss: 0.3796 - val_accuracy: 0.8379\n",
      "Epoch 40/50\n",
      "7500/7500 [==============================] - 4s 570us/step - loss: 0.3775 - accuracy: 0.8381 - val_loss: 0.3795 - val_accuracy: 0.8378\n",
      "Epoch 41/50\n",
      "7500/7500 [==============================] - 4s 548us/step - loss: 0.3774 - accuracy: 0.8382 - val_loss: 0.3794 - val_accuracy: 0.8375\n",
      "Epoch 42/50\n",
      "7500/7500 [==============================] - 4s 537us/step - loss: 0.3773 - accuracy: 0.8381 - val_loss: 0.3793 - val_accuracy: 0.8379\n",
      "Epoch 43/50\n",
      "7500/7500 [==============================] - 4s 564us/step - loss: 0.3772 - accuracy: 0.8383 - val_loss: 0.3792 - val_accuracy: 0.8381\n",
      "Epoch 44/50\n",
      "7500/7500 [==============================] - 4s 538us/step - loss: 0.3772 - accuracy: 0.8384 - val_loss: 0.3791 - val_accuracy: 0.8378\n",
      "Epoch 45/50\n",
      "7500/7500 [==============================] - 4s 539us/step - loss: 0.3771 - accuracy: 0.8383 - val_loss: 0.3791 - val_accuracy: 0.8376\n",
      "Epoch 46/50\n",
      "7500/7500 [==============================] - 4s 537us/step - loss: 0.3770 - accuracy: 0.8383 - val_loss: 0.3790 - val_accuracy: 0.8380\n",
      "Epoch 47/50\n",
      "7500/7500 [==============================] - 4s 537us/step - loss: 0.3769 - accuracy: 0.8384 - val_loss: 0.3789 - val_accuracy: 0.8379\n",
      "Epoch 48/50\n",
      "7500/7500 [==============================] - 4s 539us/step - loss: 0.3769 - accuracy: 0.8384 - val_loss: 0.3788 - val_accuracy: 0.8380\n",
      "Epoch 49/50\n",
      "7500/7500 [==============================] - 4s 538us/step - loss: 0.3768 - accuracy: 0.8386 - val_loss: 0.3788 - val_accuracy: 0.8380\n",
      "Epoch 50/50\n",
      "7500/7500 [==============================] - 4s 539us/step - loss: 0.3767 - accuracy: 0.8386 - val_loss: 0.3787 - val_accuracy: 0.8381\n"
     ]
    }
   ],
   "source": [
    "model_1.compile(SGD(lr = .003), \"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "run_hist_1 = model_1.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is 0.838\n",
      "roc-auc is 0.869\n"
     ]
    }
   ],
   "source": [
    "y_pred_class_nn_1 = model_1.predict_classes(X_test)\n",
    "y_pred_prob_nn_1 = model_1.predict(X_test)\n",
    "\n",
    "print('accuracy is {:.3f}'.format(accuracy_score(y_test,y_pred_class_nn_1)))\n",
    "print('roc-auc is {:.3f}'.format(roc_auc_score(y_test,y_pred_prob_nn_1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1ab8f906e50>"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD4CAYAAADlwTGnAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAvSElEQVR4nO3deXwV1f3/8dcnOwJCWKVhCQguLCFgAAMogbgg+gUVUKgLSH9atWCprZX6sGqlFLW41GrLFxW1daEoQikiCPkKqKRKQEARKYgoEYqAsikQknx+f5xJcm+4JDcbl2Q+z8cjj3vnzMy9Z1jmnXNm5hxRVYwxxvhPVKQrYIwxJjIsAIwxxqcsAIwxxqcsAIwxxqcsAIwxxqdiIl2BimjWrJkmJydHuhrGGFOrrF69eo+qNi9dXqsCIDk5mZycnEhXwxhjahUR+TJUuXUBGWOMT1kAGGOMT1kAGGOMT9WqawDGmJPj2LFj5ObmcuTIkUhXxVRAQkICrVu3JjY2NqztLQCMMcfJzc2lYcOGJCcnIyKRro4Jg6qyd+9ecnNzad++fVj7WBeQMeY4R44coWnTpnbyr0VEhKZNm1ao1eaPAMjOhqlT3asxJix28q99Kvp3Vve7gLKzISMDjh2DhATIyoL09EjXyhhjIq7utwCWLXMnf1XIy3PLxphT2t69e0lNTSU1NZUzzjiDpKSk4uW8vLwy983JyeGOO+6o0PclJyezZ8+eqlS5Vqr7LYCMDIiOhvx8iItzy8aYU1rTpk1Zu3YtAA888AANGjTgV7/6VfH6/Px8YmJCn77S0tJIS0s7GdWs9ep+CyA9HX72M/f+jTes+8eYmlLD19rGjh3LnXfeycCBA7n77rv58MMP6du3Lz169KBv375s2rQJgGXLlnHFFVcALjzGjRtHRkYGHTp04Mknnwz7+7788ksyMzNJSUkhMzOTr776CoDXXnuNrl270r17dy688EIANmzYQO/evUlNTSUlJYXNmzdX89HXjLrfAgDo2dO9duwY2XoYUxtNnAjeb+MntH8/rF8PhYUQFQUpKdCo0Ym3T02FJ56ocFX+85//sHTpUqKjozlw4AArVqwgJiaGpUuXcs899zBnzpzj9vnss8945513OHjwIGeffTa33XZbWPfJjx8/nhtvvJExY8Ywc+ZM7rjjDubNm8eDDz7I4sWLSUpKYt++fQBMnz6dn//851x33XXk5eVRUFBQ4WOLBH8EQJMm7vW77yJbD2Pqqv373ckf3Ov+/WUHQCWNHDmS6Oho7yv3M2bMGDZv3oyIcOzYsZD7XH755cTHxxMfH0+LFi3YtWsXrVu3Lve7srOzeeONNwC44YYb+PWvfw1Av379GDt2LNdccw1XX301AOnp6UyZMoXc3FyuvvpqOnXqVB2HW+P8EQCJie7VAsCYigvnN/XsbMjMdDdaxMXByy/XSHdr/fr1i9//9re/ZeDAgcydO5dt27aRcYLre/Hx8cXvo6Ojyc/Pr9R3F91iOX36dD744APefPNNUlNTWbt2LT/+8Y/p06cPb775JpdeeinPPvssgwYNqtT3nExhXQMQkcEisklEtojIpBDrM0Rkv4is9X7uC1i3TUQ+9spzAsqbiMgSEdnsvSZWzyGFUBQA335bY19hjK+lp7tbrCdPPmm3Wu/fv5+kpCQAXnjhhWr//L59+zJr1iwAXn75Zfr37w/A559/Tp8+fXjwwQdp1qwZ27dvZ+vWrXTo0IE77riDoUOHsn79+mqvT00otwUgItHA08DFQC6wSkTmq+qnpTZ9V1WvOMHHDFTV0vdYTQKyVPUhL1QmAXdXrPphshaAMTUvPf2k3mTx61//mjFjxvDYY49Vy2/bKSkpREW534mvueYannzyScaNG8cf//hHmjdvzvPPPw/AXXfdxebNm1FVMjMz6d69Ow899BAvvfQSsbGxnHHGGdx3331lfdUpQ1S17A1E0oEHVPVSb/k3AKo6NWCbDOBXoQJARLYBaaUDQEQ2ARmqulNEWgHLVPXssuqSlpamlZoQ5sgRqFcPpkyBe+6p+P7G+MzGjRs599xzI10NUwmh/u5EZLWqHndvbDhdQEnA9oDlXK+stHQRWScib4lIl4ByBd4WkdUicktAeUtV3QngvbYI9eUicouI5IhIzu7du8OobggJCS4ArAvIGGOKhXMRONTgEqWbDWuAdqp6SESGAPOAosvg/VR1h4i0AJaIyGequiLcCqrqDGAGuBZAuPsdJzHRuoCMMSZAOC2AXKBNwHJrYEfgBqp6QFUPee8XArEi0sxb3uG9fgPMBXp7u+3yun7wXr+pwnGUr0kTCwBjjAkQTgCsAjqJSHsRiQNGAfMDNxCRM8S7R0pEenufu1dE6otIQ6+8PnAJ8Im323xgjPd+DPDPqh5MmawFYIwxQcrtAlLVfBEZDywGooGZqrpBRG711k8HRgC3iUg+cBgYpaoqIi2BuV42xACvqOoi76MfAmaLyE+Ar4CR1XxswRITYdu2Gv0KY4ypTcJ6EMzr1llYqmx6wPungKdC7LcV6H6Cz9wLZFakslWSmAgffXTSvs4YY051dX8wuCJ2DcCYWiMjI4PFixcHlT3xxBPcfvvtZe5TdJv4kCFDisfpCfTAAw8wbdq0Mr973rx5fPppyWNO9913H0uXLq1A7UMLHKTuVOGfAEhMhEOH3NwAxphT2ujRo4ufwi0ya9YsRo8eHdb+CxcupHHjxpX67tIB8OCDD3LRRRdV6rNOdf4KALBWgDE1pDpHgx4xYgQLFizg6NGjAGzbto0dO3bQv39/brvtNtLS0ujSpQv3339/yP0DJ3iZMmUKZ599NhdddFHxkNEAzzzzDL169aJ79+4MHz6cH374gZUrVzJ//nzuuusuUlNT+fzzzxk7diyvv/46AFlZWfTo0YNu3boxbty44volJydz//3307NnT7p168Znn30W9rG++uqrdOvWja5du3L33W4whIKCAsaOHUvXrl3p1q0bjz/+OABPPvkknTt3JiUlhVGjRlXwT/V4/hgMDoIDoEXIZ86MMSFEYjTopk2b0rt3bxYtWsSwYcOYNWsW1157LSLClClTaNKkCQUFBWRmZrJ+/XpSUlJCfs7q1auZNWsWH330Efn5+fTs2ZPzzjsPgKuvvpqbb74ZgHvvvZfnnnuOCRMmMHToUK644gpGjBgR9FlHjhxh7NixZGVlcdZZZ3HjjTfy17/+lYkTJwLQrFkz1qxZw1/+8hemTZvGs88+W/YfGrBjxw7uvvtuVq9eTWJiIpdccgnz5s2jTZs2fP3113zyibtpsqg766GHHuKLL74gPj4+ZBdXRfmnBWBDQhtTY0KNBl1Vgd1Agd0/s2fPpmfPnvTo0YMNGzYEddeU9u6773LVVVdx2mmncfrppzN06NDidZ988gkXXHAB3bp14+WXX2bDhg1l1mfTpk20b9+es846C4AxY8awYkXJM61FQ0Ofd955bAvzjsNVq1aRkZFB8+bNiYmJ4brrrmPFihV06NCBrVu3MmHCBBYtWsTpp58OuPGKrrvuOl566aUTzohWEf5rAdhwEMZUSKRGg77yyiu58847WbNmDYcPH6Znz5588cUXTJs2jVWrVpGYmMjYsWM5cuRImZ9TNIxzaWPHjmXevHl0796dF154gWXlzBde3rhpRcNOV2TI6RN9ZmJiIuvWrWPx4sU8/fTTzJ49m5kzZ/Lmm2+yYsUK5s+fz+TJk9mwYUOVgsA/LQC7BmBMjamJ0aAbNGhARkYG48aNK/7t/8CBA9SvX59GjRqxa9cu3nrrrTI/48ILL2Tu3LkcPnyYgwcP8q9//at43cGDB2nVqhXHjh3j5ZdfLi5v2LAhBw8ePO6zzjnnHLZt28aWLVsA+Pvf/86AAQOqdIx9+vRh+fLl7Nmzh4KCAl599VUGDBjAnj17KCwsZPjw4UyePJk1a9ZQWFjI9u3bGThwII888gj79u3j0KFDVfp+/7QArAvImBpVE6NBjx49mquvvrq4K6h79+706NGDLl260KFDB/r161fm/j179uTaa68lNTWVdu3accEFFxSvmzx5Mn369KFdu3Z069at+KQ/atQobr75Zp588snii78ACQkJPP/884wcOZL8/Hx69erFrbfeWqHjycrKCpqN7LXXXmPq1KkMHDgQVWXIkCEMGzaMdevWcdNNN1Ho9atNnTqVgoICrr/+evbv34+q8otf/KLSdzoVKXc46FNJpYeDBnf7Z1wcPPgg/Pa31VsxY+oYGw669qru4aDrhthYaNDArgEYY4zHPwEANiCcMcYE8FcA2HAQxoStNnUPG6eif2f+CgBrARgTloSEBPbu3WshUIuoKnv37iUhISHsffxzFxC4APjPfyJdC2NOea1btyY3N5dKT8NqIiIhISHoLqPy+C8ArAVgTLliY2Np3759pKthapi/uoDsGoAxxhTzVwAkJsLhw1DOo+PGGOMH/gsAsFaAMcYQZgCIyGAR2SQiW0RkUoj1GSKyX0TWej/3eeVtROQdEdkoIhtE5OcB+zwgIl8H7DOk+g7rBGw4CGOMKVbuRWARiQaeBi4GcoFVIjJfVUuPwfquqpae7ywf+KWqrhGRhsBqEVkSsO/jqlr2/GzVyVoAxhhTLJwWQG9gi6puVdU8YBYwLJwPV9WdqrrGe38Q2AgkVbayVWZDQhtjTLFwAiAJ2B6wnEvok3i6iKwTkbdEpEvplSKSDPQAPggoHi8i60VkpogkhvpyEblFRHJEJKfK9yRbC8AYY4qFEwChZlMo/XjgGqCdqnYH/gzMC/oAkQbAHGCiqh7wiv8KnAmkAjuBR0N9uarOUNU0VU1r3rx5GNUtg10DMMaYYuEEQC7QJmC5NbAjcANVPaCqh7z3C4FYEWkGICKxuJP/y6r6RsA+u1S1QFULgWdwXU01q2iSUgsAY4wJKwBWAZ1EpL2IxAGjgPmBG4jIGeLNuyYivb3P3euVPQdsVNXHSu3TKmDxKuCTyh9GmKKjXQjYNQBjjCn/LiBVzReR8cBiIBqYqaobRORWb/10YARwm4jkA4eBUaqqItIfuAH4WETWeh95j9dKeEREUnHdSduAn1brkQXIzoZlyyAjA9JtOAhjjAF8MCNYdjYMGOAmBKtXD7LajCW90x5YsKCGammMMacW384ItmwZ5Oe793l5sKzgAusCMsYYfBAAGRmu6x/clMAZbbdaF5AxxuCDAEhPhwkT3PvXX4f0jrstAIwxBh8EAECPHu61UydKhoSuRdc+jDGmJvgiAIoeAN63z1vIy4MffohklYwxJuJ8EQCNG7vX777DhoMwxhiPrwJg3z5sOAhjjPH4LwCsBWCMMYBPAuC4awBgzwIYY3zPFwFw2mkQE2MtAGOMCeSLABBx3UDffYddAzDGGI8vAgBcAOzbBzRsCFFR1gVkjPE9/wVAVFRAc8AYY/zLNwGQmOgFQNGCBYAxxud8EwDFLQAoGQ7CGGN8zFcBUHzOT0y0awDGGN/zVQBYF5AxxpTwTQAkJsLRo3DkCNYFZIwxhBkAIjJYRDaJyBYRmRRifYaI7BeRtd7PfeXtKyJNRGSJiGz2XhOr55BCO244CBsS2hjjc+UGgIhEA08DlwGdgdEi0jnEpu+qaqr382AY+04CslS1E5DlLdeY40YELSiAgwdr8iuNMeaUFk4LoDewRVW3qmoeMAsYFubnl7XvMOBF7/2LwJVh17oSbEA4Y4wJFk4AJAHbA5ZzvbLS0kVknYi8JSJdwti3paruBPBeW1So5hVkQ0IbY0ywmDC2kRBlpTvP1wDtVPWQiAwB5gGdwty37C8XuQW4BaBt27YV2TVI0IigrawFYIwx4bQAcoE2AcutgR2BG6jqAVU95L1fCMSKSLNy9t0lIq0AvNdvQn25qs5Q1TRVTWvevHkY1Q0tZBeQPQtgjPGxcAJgFdBJRNqLSBwwCpgfuIGInCEi4r3v7X3u3nL2nQ+M8d6PAf5Z1YMpi00LaYwxwcrtAlLVfBEZDywGooGZqrpBRG711k8HRgC3iUg+cBgYpaoKhNzX++iHgNki8hPgK2BkNR9bkIQEiI+3awDGGFMknGsARd06C0uVTQ94/xTwVLj7euV7gcyKVLaqigeEq1/fzRBjXUDGGB/zzZPAEDAchIgNB2GM8T3fBUDQgHAWAMYYH/NdANiQ0MYY4/gqAI6bFMauARhjfMxXAWBDQhtjTAlfBoAq1gVkjPE93wVAfj58/z0l/UGFhRGulTHGRIbvAgAChoNQhf37I1gjY4yJHF8FQNCAcDYchDHG53wVADYktDHGlPBvANiIoMYYn/NlANiIoMYY47MAsGsAxhhTwlcB0KiRe7VrAMYY47MAiI11I0Hv2wfUq+cmCLBrAMYYn/JVAICNCGqMMUV8FwBBA8LZcBDGGB/zXQDYgHDGGONYANg1AGOMT4UVACIyWEQ2icgWEZlUxna9RKRAREZ4y2eLyNqAnwMiMtFb94CIfB2wbki1HFE5rAVgjDFOuZPCi0g08DRwMZALrBKR+ar6aYjtHgYWF5Wp6iYgNWD918DcgN0eV9VpVTyGCgm6CGzXAIwxPhZOC6A3sEVVt6pqHjALGBZiuwnAHOCbE3xOJvC5qn5ZqZpWk8RENwBoYSFw6BAcOADvvhvJKhljTESEEwBJwPaA5VyvrJiIJAFXAdPL+JxRwKulysaLyHoRmSkiiaF2EpFbRCRHRHJ2794dRnXL1rixGwX6YNaH8Le/ucJLLoHs7Cp/tjHG1CbhBICEKNNSy08Ad6tqQcgPEIkDhgKvBRT/FTgT10W0E3g01L6qOkNV01Q1rXnz5mFUt2zFA8ItWQUFXnWPHYNly6r82cYYU5uUew0A9xt/m4Dl1sCOUtukAbNEBKAZMERE8lV1nrf+MmCNqu4q2iHwvYg8AyyocO0roXhAuC79aRcbC0ePQkwMZGScjK83xphTRjgtgFVAJxFp7/0mPwqYH7iBqrZX1WRVTQZeB24POPkDjKZU94+ItApYvAr4pOLVr7jiAeHadYcZM9zCffdBevrJ+HpjjDlllNsCUNV8ERmPu7snGpipqhtE5FZvfVn9/ojIabg7iH5aatUjIpKK607aFmJ9jQiaE+Dii91CYsjLD8YYU6eF0wWEqi4EFpYqC3niV9WxpZZ/AJqG2O6GsGtZjYICoOiawq5dJ9jaGGPqLl8+CQxeAMTEQNOm8M2J7lw1xpi6y3cB0KgRiAQ8/9WihQWAMcaXfBcAUVFw+ukBw0G0bGldQMYYX/JdAECp8YCsBWCM8SkLgJYtLQCMMb5kAdCihVs4ejRyFTLGmAjwZQAEjQLdooV7rYZxhowxpjbxZQAc1wUE1g1kjPEdC4CiFoDdCWSM8RnfBsDBg5CfT0kAWAvAGOMzvgyAoqF/9u+npAvIWgDGGJ/xZQAEDQdRvz7Uq2ctAGOM71gAiNjDYMYYX7IAABsOwhjjS74MgOJJYfZ5BdYCMMb4kC8DoHhaSBsR1BjjY74OgOPGAyosjFCNjDHm5PNlADRo4IaFDuoCys8PKDDGmLovrAAQkcEisklEtojIpDK26yUiBSIyIqBsm4h8LCJrRSQnoLyJiCwRkc3e60mbmFfEhoMwxphyA0BEooGngcuAzsBoEel8gu0exk0eX9pAVU1V1bSAsklAlqp2ArK85ZMmMdGGgzDG+Fs4LYDewBZV3aqqecAsYFiI7SYAc4Bwf40eBrzovX8RuDLM/apF48YhRgS1FoAxxkfCCYAkYHvAcq5XVkxEkoCrgOkh9lfgbRFZLSK3BJS3VNWdAN5ri1BfLiK3iEiOiOTsrsYhm60LyBjjd+EEgIQo01LLTwB3q2pBiG37qWpPXBfSz0TkwopUUFVnqGqaqqY1b968IruWKSgAmjZ1FwasC8gY4yMxYWyTC7QJWG4N7Ci1TRowS0QAmgFDRCRfVeep6g4AVf1GRObiupRWALtEpJWq7hSRVoTfdVQtgq4BREdDs2bWAjDG+Eo4LYBVQCcRaS8iccAoYH7gBqraXlWTVTUZeB24XVXniUh9EWkIICL1gUuAT7zd5gNjvPdjgH9W+WgqIOgaANjcwMYY3ym3BaCq+SIyHnd3TzQwU1U3iMit3vpQ/f5FWgJzvZZBDPCKqi7y1j0EzBaRnwBfASMrfxgV17gxHD7spgKOj8ddCLYuIGOMj4TTBYSqLgQWlioLeeJX1bEB77cC3U+w3V4gM9yKVreip4H37/duAmrRAnJyytrFGGPqFF8+CQwhBoSzEUGNMT7j2wA4bjygFi3cPJGHD0eoRsYYc3L5PgCKLwQXPQtQjc8aGGPMqcz3AWDDQRhj/MoCYJ9XYMNBGGN8xrcBEPIiMFgAGGN8w7cBkJAAcXEB1wCKhpmwLiBjjE/4NgCOmxOgfn33Yy0AY4xP+DYAoFQAgA0HYYzxFV8HQNCAcGDDQRhjfMXXAXBcC6BFC2sBGGN8w/cBYCOCGmP8yvcBcFwLYPduKCyMUI2MMebksQDYB1o0v1mLFlBQAN9+G8FaGWPMyeHrADhwAPLyYPlyr6DoYTC7EGyM8QHfBkB2Njz7rHt/2WVu2YaDMMb4iW8DYNky19sDrhWwbBk2HIQxxld8GwAZGW4oCHBzwmdkYCOCGmN8JawAEJHBIrJJRLaIyKQytuslIgUiMsJbbiMi74jIRhHZICI/D9j2ARH5WkTWej9Dqn444UtPh6wsNybQkCFumSZNXBpYC8AY4wPlzgksItHA08DFQC6wSkTmq+qnIbZ7GDd5fJF84JequkZEGgKrRWRJwL6Pq+q06jiQyujbF/r0CTjfR0W5QeEsAIwxPhBOC6A3sEVVt6pqHjALGBZiuwnAHKD47KmqO1V1jff+ILARSKpyratRSgp8/HHArf82HIQxxifCCYAkYHvAci6lTuIikgRcBUw/0YeISDLQA/ggoHi8iKwXkZkikniC/W4RkRwRydldA9M1pqTAoUPwxRdegQ0HYYzxiXACQEKUaanlJ4C7VbUg5AeINMC1Diaq6gGv+K/AmUAqsBN4NNS+qjpDVdNUNa150Zj91Sglxb2uX+8V2HAQxhifCCcAcoE2AcutgR2ltkkDZonINmAE8BcRuRJARGJxJ/+XVfWNoh1UdZeqFqhqIfAMrqvppOvSxc0NUBwA1gVkjPGJci8CA6uATiLSHvgaGAX8OHADVW1f9F5EXgAWqOo8ERHgOWCjqj4WuI+ItFLVnd7iVcAnlT6KKqhfHzp2LBUA33/vfurXj0SVjDHmpCg3AFQ1X0TG4+7uiQZmquoGEbnVW3/Cfn+gH3AD8LGIrPXK7lHVhcAjIpKK607aBvy0sgdRVSkpsG6dt1D0MNju3RYAxpg6LZwWAN4Je2GpspAnflUdG/D+PUJfQ0BVbwi7ljUsJQXeeMNdDG4Q+DBYcnJE62WMMTXJt08CB0pJcSOCbtiADQdhjPENCwCge3f3un49NhyEMcY3LACAdu2gYcNSAWAtAGNMHWcBgBsBols3LwASEuD00y0AjDF1ngWAJyXFBYAq9iyAMcYXLAA8KSluesjcXGw4CGOML1gAeIKGhLDhIIwxPmAB4Ona1b2uW4d1ARljfMECwNOokXvuq/hOoD17SuaMNMaYOsgCIEDRhWBatnRXg/fujXSVjDGmxlgABEhJgU2b4EjjM1zBlCmQnR3ZShljTA2xAAiQkuJmBvv0Y6/r56mnIDPTQsAYUydZAAQovhNonTffTWEh5OXBsmURq5MxxtQUC4AAHTu6B4HXN7rAzRIDEBcHGRkRrZcxxtQEC4AA0dHudtD1e34EEye6whkzID09ovUyxpiaYAFQStHkMPqbeyA2FtaujXSVjDGmRlgAlJKS4h4B2FXQDP7nf+Cll+DYsUhXyxhjqp0FQClBcwOMHeueCF68OJJVMsaYGhFWAIjIYBHZJCJbRGRSGdv1EpECERlR3r4i0kRElojIZu81sWqHUj26dXOv69cDgwe7p4JfeCGSVTLGmBpRbgCISDTwNHAZ0BkYLSKdT7Ddw7jJ48PZdxKQpaqdgCxvOeKaNoWkJG9MoNhYuP56mD/fngo2xtQ54bQAegNbVHWrquYBs4BhIbabAMwBvglz32HAi977F4ErK179mlE8JATAmDHuGsCrr0a0TsYYU93CCYAkYHvAcq5XVkxEkoCrgOkV2Lelqu4E8F5bhF/tmpWS4iaI//3vIfv7FOjZ07qBjDF1TjgBICHKtNTyE8Ddqlp6+Mxw9i37y0VuEZEcEcnZvXt3RXattHr13ECg99/vjQQxYBKsXg0ff3xSvt8YY06GcAIgF2gTsNwa2FFqmzRglohsA0YAfxGRK8vZd5eItALwXkPOwKKqM1Q1TVXTmjdvHkZ1q+7AAfdaPBLEaUPc9QBrBRhj6pBwAmAV0ElE2otIHDAKmB+4gaq2V9VkVU0GXgduV9V55ew7HxjjvR8D/LOqB1NdRoxw5/si/S+tb88EGGPqnHIDQFXzgfG4u3s2ArNVdYOI3Coit1ZmX2/1Q8DFIrIZuNhbPiWkp8Py5XDZZa4r6M9/hmPX3+SmiVy0KNLVM8aYaiGqFeqSj6i0tDTNyck5qd/52GPwy1/C8KsKefW9NsRecD7MmXNS62CMMVUhIqtVNa10uT0JXI4774THH4c5c6O4ttFb5M1bCPfea3MEGGNqPQuAMEycCH/6E8zdksLFhYv4/RTIzviNhYAxplazAAjTHXfAnf0+YAUD+C2TGZD3NksfXRvpahljTKVZAFRAs25nEEUBIBwjjsvnjOOOAWvZvKmQ7GyYOtUaBcaY2iMm0hWoTTJubEf88wXk5RUQEwMDG61n+oru/PmcKKJEQZX4eCXrnWibQ8YYc8qzFkAFpKdD1jvRTJ4SzTvLo3nrmzS+emwOg6LeoVChkCgOH41izDWH+fOfYetWrGVgjDll2W2g1SD7uqfIfGUcR4kjCuVHMd/wVb4b8khEQSE2RnnplShGjHDTDWdnu7nmMzJsxkljTM060W2g1gVUDdLHn0fW60NYdqwfGVErSG/8GVv2NOIX0X9mQcGlQBR5+cI110CTJtCpkxtaqLDQzTn/9ttwwQUln2fhYIw5GawFUF0Cz9q9e8PSpWT/7CUyP/9f8oglhnwmNnmJvWens2Dz2fx3TwxFY+VFRcFZZ8E550CDBvCPf7gnkOPiICsL+vY9/issGIwx4TpRC8ACoCZlZ5Od8RuWHetLhqwgvdU2+PprsjmfTLI4ShwxFHBt2lYOnXEmm7bGsWmTO/kXiY+Hrl0hMdGd/IuC4dlnYcgQV/7vf1swGGNOzAIgUkr/2v7ll3DnnWS/sYNlZJDBMtL5t9s2OZl3G1/BJWsfIY9Yoilk6IX7OJjQglWr4LvvlNIjbMfFufHpVCE6Gq6+2k1f0KoVfPstfPEFXHSRm90yLi50lcqqrjGm9rMAOJVkZ7uJBvLy3LCjU6fC0aOwZg1kZZG9t1NwOCQnk11vEJkb/0weccSQz71XbqB+z7N54+36vPceuGBQ4uKEvLzQX3v66a6L6b//ddcfigKjSxfXktizBx5+GPLzXVi88goMGgQNG8IHH4QOhhMFhgWJMacOC4BTTVlnzqJwiI6GG26AH36A5cvJ3tH2uFZDdswFZOYvIo9Y4jhG1sUPkzKwKfe/dxGPv3UOheqeURiUqXTuHMXy5bBuXVFLQomPF44eDb/aItC5M/zoR67l8e67JWEybhycey7s2uUG0cvPd/n21FNw/vkufOrXh08+cYc5cGDVwsRCxpjwWADUJqHObIHBEBMDv/sdnHYazJ5N9nv5JcEQvQoKCoqvMxQHAxeR3nwL2dH9yfzvSyXlg6fRK6M+++udQdZXnbjxTz05lh9FTIzyy/F5NEuK5823onjn/xT1QqNjR6FZM/ecwzfflIRJ6AngytawofsRgR07XFdWVBSkpblurEOH3B9FUciMHu3uotq1C2bMKAmZBx6A1FQ3m9uWLW7ytvR0Fzzx8SU/a9bAe+9VPWQqE0oWWCZSLADqgvKCIS4Oli51v6L/7ndk/+lDlumF7gL0gDh3q9H775O9oWFJYER96M6uRV/B+ce3MuIGkJm3sCQ0zp1AepcDZO9sR+b7DxaXL7rmebplNGX5traMfqw3x/KFmBiY8qtvads+hkP5Ccx9K54FbwqqgojSp4/QpQusWgXr15eESevWQpMmLhT27Ckpj4qSwOpWSVGLBNxUD6olLZymTeHw4ZLbdaOj3bWUpCTYuxfefNNdkC9qpJ15pqvrM8+4UIqJgUmT3GfFxbmwvPde12qKjYWnn3YhFxvr5p/OyXG3Avft64IqLs79mSxfXj0hczICzpy6LADqsnCCISvLrQsVGN26uSvG06bBX/7iznhRUe7Kce/esGQJ2e8XsowBLhjafO1+bc/NJftA5+MvZhM6SIrKg1omzUeT3mob2Qe7kvnFMyXl599Leuphsrc0J3PppJLy0c/Ra1BDlq8Qrvj7NRwjlliO8b83/ZuzBvyI5/7VgplvNKZQhShRrhpWwEWXRHM0T1i0CBYv1uLwOf98oVs3d/Jds6YkZDp2FJKS3AX0r74qKU9MFOrXh+++g++/r1rLp6Lq14eEBBdS331XElbt2rlrOzExLrA2bSr56+vVC5o1g4MH4f33SwLriiugTRsXZK+9VlJ+443u83Jz3eynRUE2YYK7RXn79pJrRDEx8Ic/uH86sbFueeNGWLvWfW+PHu4zo6Nh/XoXZn36uHLx/rg++siVX3gh9O/vji8+/sR3tZ2qwVcbwtICwI8q8688nNA4Uflrr7mLAH/6k+v4LzoTDR/uriYfOgSLFpGd9YMXJstJT/kekpPh00/J3tKsJDQaf+bOLPv2kX3svLBD5viur0y3Lj6ebOlL5pEFJeta3UD6GV+Qve/c4PBJ/RXp5+4je1srMrMnl5RfNo30Pkr22npkzhtfXL70hr/R66JGvLusgCueH04eMcSSz99vfpeuF7ciT2NZtakhP3vwDI4VCLHRyu/vPcyZZwqz58cz+/Xo4sAacrkwYAAsWQJLlpSEVXq6kJrqTpirVrnQEZQuXYWOHd1JedMm2Lw5uBXVooVrmfz3vyXlDRoIsbHur+PYsZMTYJXVuLHr1isogN27S4KvfXv3O0hMjLtEFhh8553ngu/AAfdPtKgFN3AgtGzpftd5++3gGyHatnXdiq++6r4rJgZ+8hP3T/Orr4JbdhMmQMeO7v0XX8Cjj5asu/9+d1PF5s3BLb4nn3RdlEUtvtWr3X+htLSSoPzoI3ezRf/+Liyjokp+Vq1yIT5oUOXC5EQBgKrWmp/zzjtPTQ1buVL1D39wr5UtX7lStV491eho9xrOunDLFyxQ3bZN9ZVXVOPjVaOi3Otjj6nOnat67bW6knT9A5N0pfRVvfRS1fvvV737btX09JJ1pKt26aJ6xRWqHTvqSs73ys9XbdFC9cwzVRs1Ci6PilI35B/B5V5ZWeUnWreS87Ue32s0eVqP73Vl3ADVJk10ZcNLgstbXa2amqor240KLu/8E9WhQ1WHD9eVve4IXpfxG9WJE3XlxfcFl1/5sOof/6grr3k8qPz9m2ZowSuzdMXNLwaU/6Bv3jpft8/5QF+b8pkmxB7TaMnX+Jh8fWbyDn3vH7m6bNZOHTfqkEZJoYJqlBTqddce09dePaajri0MKh85UvXvf1cdOVKDyi+/XHXqVNVBg1QFVy4Uap8+qjffrHreeap45VCoXbq4w778ctVOnYLXtW2r2quXaqtWweVNm6p26KDauHFweXy8+6cV8Nd7Sv5ERR3/3ylcQI6GOKdG/KRekR8LgFrkRIFR1rpTPXxWrlQtKFBdtqykPCFBdc4c1U2b3JktMJSeeMIF1htvqI4aVXKGiYpywfPoo6qXXhocSv36qY4fr9qr1/FhNXSo6llnBQdJUpJqaqpq586qTZsGr6tXT/X001VjYyscWJUpDwoZb/1x5fUGqTZrdnzAtbxStXNnXfmj4cHlZ16vmpGhK8+5Kbi8x+3uz/T663Vl+p3B6wbdo3rnnccH39CpqlOn6srh04LLr3tK9bnndOVNM4LKl93ykv7w+puadevs4ED82Zu6c0GO5i74SOf8cUtxKCbE5uvfp/1X1yz4Wp/9416Nj83XKCnQ+Nh8fXzqD7rg9cN6/Y/zg4JvxAjV555TveoqVSkViI89pjptmurgwSXroqMK9Q9/qPh/xyoFADAY2ARsASaFWD8MWA+sBXKA/l752V5Z0c8BYKK37gHg64B1Q8qrhwWACUukwqe88qqGT1nlFdnnnXdUDxxQXbTIBVhRkP3jH6obNqi++GJwkD31lOqSJapjxwaH2LXXqv7tb+4MduWVwYF16aWqkyerDhoUXJ6ernr77ccHXLduqsOHuxAIDJkOHVQvvFC1bdvjW2lnneXWl26pxcerNmigGhNT48FX0X2OC8SofqoJCboybkBw+emXqrZpo9q+va5sPjR43f+uL/OffyiVDgAgGvgc6ADEAeuAzqW2aUDJ9YQU4LMTfM5/gXZaEgC/Ku/7A38sAEytVtMhU9PfUdOtq8qUV2SfFStUDx9WXbo0uAX3z3+qfvml6muvBQfi88+rfvih6owZwYH4+OOqCxeq/utfqtdfHxyKI0eqzpxZ9Ct9Sflll6k+/LDqJZcEB9+AAap33aXav39weVqaC9wbblBNSSkJk6h+WpkmQFUCIB1YHLD8G+A35Wy/MUT5JcD7AcsWAMbUNqdi6yqS3x3JgKuAEwVAuXcBicgIYLCq/j9v+Qagj6qOL7XdVcBUoAVwuapml1o/E1ijqk95yw8AY71uoRzgl6r6XYjvvwW4BaBt27bnffnll2XW1xhjTqpa8BRhpW8DFZGRwKWlAqC3qk44wfYXAvep6kUBZXHADqCLqu7yyloCe3A3Uk8GWqnquLLqYreBGmNMxZ0oAMKZEjIXaBOw3Bp3Mg9JVVcAZ4pIs4Diy3C//e8K2G6XqhaoaiHwDNA7jLoYY4ypJuEEwCqgk4i0936THwXMD9xARDqKuOf7RKQn7mLx3oBNRgOvltqnVcDiVcAnFa++McaYyip3SkhVzReR8cBi3J08M1V1g4jc6q2fDgwHbhSRY8Bh4FrvwgMichpwMfDTUh/9iIik4rqAtoVYb4wxpgbZUBDGGFPHVeUagDHGmDrIAsAYY3yqVnUBichuoLIPAjTD3XbqN3bc/uPXY7fjPrF2qtq8dGGtCoCqEJGcUH1gdZ0dt//49djtuCvOuoCMMcanLACMMcan/BQAMyJdgQix4/Yfvx67HXcF+eYagDHGmGB+agEYY4wJYAFgjDE+5YsAEJHBIrJJRLaIyKRI16emiMhMEflGRD4JKGsiIktEZLP3mhjJOtYEEWkjIu+IyEYR2SAiP/fK6/Sxi0iCiHwoIuu84/6dV16nj7uIiESLyEcissBbrvPHLSLbRORjEVkrIjleWaWPu84HgIhEA0/jhqTuDIwWkc6RrVWNeQE3f3OgSUCWqnYCsrzluiYfN6HQucD5wM+8v+O6fuxHgUGq2h1IBQaLyPnU/eMu8nNgY8CyX457oKqmBtz7X+njrvMBgJtnYIuqblXVPGAWbhL7Osebi+HbUsXDgBe99y8CV57MOp0MqrpTVdd47w/iTgpJ1PFj92b7O+Qtxno/Sh0/bgARaQ1cDjwbUFznj/sEKn3cfgiAJGB7wHKuV+YXLVV1J7gTJW7KzjpLRJKBHsAH+ODYvW6QtcA3wBJV9cVxA08AvwYKA8r8cNwKvC0iq73pcqEKx13ufAB1gIQos3tf6yARaQDMASaq6gFvjqI6TVULgFQRaQzMFZGuEa5SjRORK4BvVHW1iGREuDonWz9V3SEiLYAlIvJZVT7MDy2ACk1pWQftKpp9zXv9JsL1qREiEos7+b+sqm94xb44dgBV3Qcsw10DquvH3Q8YKiLbcF26g0TkJer+caOqO7zXb4C5uC7uSh+3HwKg3Ckt67j5wBjv/RjgnxGsS43wpiN9Dtioqo8FrKrTxy4izb3f/BGResBFwGfU8eNW1d+oamtVTcb9f/4/Vb2eOn7cIlJfRBoWvQcuwU2lW+nj9sWTwCIyBNdnWDSl5ZTI1qhmiMirQAZueNhdwP3APGA20Bb4ChipqqUvFNdqItIfeBf4mJI+4Xtw1wHq7LGLSAruol807pe52ar6oIg0pQ4fdyCvC+hXqnpFXT9uEemA+60fXPf9K6o6pSrH7YsAMMYYczw/dAEZY4wJwQLAGGN8ygLAGGN8ygLAGGN8ygLAGGN8ygLAGGN8ygLAGGN86v8D0CM03w5C76gAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(run_hist_1.history[\"loss\"],'r', marker='.', label=\"Train Loss\")\n",
    "ax.plot(run_hist_1.history[\"val_loss\"],'b', marker='.', label=\"Validation Loss\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change to two hidden layers with activation relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_7 (Dense)              (None, 12)                300       \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 12)                156       \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 13        \n",
      "=================================================================\n",
      "Total params: 469\n",
      "Trainable params: 469\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_1 = Sequential()\n",
    "model_1.add(Dense(12,input_shape = (24,),activation = 'relu'))\n",
    "model_1.add(Dense(12,activation='relu'))\n",
    "model_1.add(Dense(1,activation='sigmoid'))\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "   1/7500 [..............................] - ETA: 0s - loss: 0.7998 - accuracy: 0.3750WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_train_batch_end` time: 0.0010s). Check your callbacks.\n",
      "7500/7500 [==============================] - 4s 581us/step - loss: 0.4311 - accuracy: 0.8112 - val_loss: 0.3932 - val_accuracy: 0.8306\n",
      "Epoch 2/50\n",
      "7500/7500 [==============================] - 4s 532us/step - loss: 0.3861 - accuracy: 0.8327 - val_loss: 0.3854 - val_accuracy: 0.8332\n",
      "Epoch 3/50\n",
      "7500/7500 [==============================] - 4s 534us/step - loss: 0.3815 - accuracy: 0.8349 - val_loss: 0.3825 - val_accuracy: 0.8354\n",
      "Epoch 4/50\n",
      "7500/7500 [==============================] - 4s 535us/step - loss: 0.3794 - accuracy: 0.8361 - val_loss: 0.3810 - val_accuracy: 0.8361\n",
      "Epoch 5/50\n",
      "7500/7500 [==============================] - 4s 534us/step - loss: 0.3783 - accuracy: 0.8368 - val_loss: 0.3802 - val_accuracy: 0.8361\n",
      "Epoch 6/50\n",
      "7500/7500 [==============================] - 4s 537us/step - loss: 0.3776 - accuracy: 0.8372 - val_loss: 0.3796 - val_accuracy: 0.8360\n",
      "Epoch 7/50\n",
      "7500/7500 [==============================] - 4s 541us/step - loss: 0.3771 - accuracy: 0.8374 - val_loss: 0.3792 - val_accuracy: 0.8365\n",
      "Epoch 8/50\n",
      "7500/7500 [==============================] - 4s 536us/step - loss: 0.3767 - accuracy: 0.8376 - val_loss: 0.3788 - val_accuracy: 0.8365\n",
      "Epoch 9/50\n",
      "7500/7500 [==============================] - 4s 549us/step - loss: 0.3763 - accuracy: 0.8376 - val_loss: 0.3785 - val_accuracy: 0.8366\n",
      "Epoch 10/50\n",
      "7500/7500 [==============================] - 4s 586us/step - loss: 0.3761 - accuracy: 0.8378 - val_loss: 0.3783 - val_accuracy: 0.8367\n",
      "Epoch 11/50\n",
      "7500/7500 [==============================] - 4s 536us/step - loss: 0.3758 - accuracy: 0.8381 - val_loss: 0.3782 - val_accuracy: 0.8369\n",
      "Epoch 12/50\n",
      "7500/7500 [==============================] - 4s 536us/step - loss: 0.3756 - accuracy: 0.8382 - val_loss: 0.3779 - val_accuracy: 0.8366\n",
      "Epoch 13/50\n",
      "7500/7500 [==============================] - 4s 536us/step - loss: 0.3754 - accuracy: 0.8382 - val_loss: 0.3778 - val_accuracy: 0.8369\n",
      "Epoch 14/50\n",
      "7500/7500 [==============================] - 4s 536us/step - loss: 0.3752 - accuracy: 0.8382 - val_loss: 0.3777 - val_accuracy: 0.8370\n",
      "Epoch 15/50\n",
      "7500/7500 [==============================] - 4s 539us/step - loss: 0.3751 - accuracy: 0.8384 - val_loss: 0.3775 - val_accuracy: 0.8371\n",
      "Epoch 16/50\n",
      "7500/7500 [==============================] - 4s 534us/step - loss: 0.3749 - accuracy: 0.8383 - val_loss: 0.3775 - val_accuracy: 0.8370\n",
      "Epoch 17/50\n",
      "7500/7500 [==============================] - 4s 536us/step - loss: 0.3747 - accuracy: 0.8384 - val_loss: 0.3773 - val_accuracy: 0.8370\n",
      "Epoch 18/50\n",
      "7500/7500 [==============================] - 4s 536us/step - loss: 0.3745 - accuracy: 0.8386 - val_loss: 0.3772 - val_accuracy: 0.8371\n",
      "Epoch 19/50\n",
      "7500/7500 [==============================] - 4s 537us/step - loss: 0.3744 - accuracy: 0.8387 - val_loss: 0.3772 - val_accuracy: 0.8369\n",
      "Epoch 20/50\n",
      "7500/7500 [==============================] - 4s 534us/step - loss: 0.3742 - accuracy: 0.8387 - val_loss: 0.3770 - val_accuracy: 0.8371\n",
      "Epoch 21/50\n",
      "7500/7500 [==============================] - 4s 534us/step - loss: 0.3741 - accuracy: 0.8388 - val_loss: 0.3769 - val_accuracy: 0.8371\n",
      "Epoch 22/50\n",
      "7500/7500 [==============================] - 4s 535us/step - loss: 0.3740 - accuracy: 0.8388 - val_loss: 0.3771 - val_accuracy: 0.8374\n",
      "Epoch 23/50\n",
      "7500/7500 [==============================] - 4s 535us/step - loss: 0.3739 - accuracy: 0.8389 - val_loss: 0.3767 - val_accuracy: 0.8369\n",
      "Epoch 24/50\n",
      "7500/7500 [==============================] - 4s 535us/step - loss: 0.3737 - accuracy: 0.8389 - val_loss: 0.3766 - val_accuracy: 0.8371\n",
      "Epoch 25/50\n",
      "7500/7500 [==============================] - 4s 536us/step - loss: 0.3736 - accuracy: 0.8391 - val_loss: 0.3766 - val_accuracy: 0.8371\n",
      "Epoch 26/50\n",
      "7500/7500 [==============================] - 4s 564us/step - loss: 0.3735 - accuracy: 0.8392 - val_loss: 0.3764 - val_accuracy: 0.8378\n",
      "Epoch 27/50\n",
      "7500/7500 [==============================] - 4s 542us/step - loss: 0.3734 - accuracy: 0.8392 - val_loss: 0.3765 - val_accuracy: 0.8374\n",
      "Epoch 28/50\n",
      "7500/7500 [==============================] - 4s 534us/step - loss: 0.3733 - accuracy: 0.8394 - val_loss: 0.3763 - val_accuracy: 0.8373\n",
      "Epoch 29/50\n",
      "7500/7500 [==============================] - 4s 539us/step - loss: 0.3732 - accuracy: 0.8393 - val_loss: 0.3761 - val_accuracy: 0.8375\n",
      "Epoch 30/50\n",
      "7500/7500 [==============================] - 4s 537us/step - loss: 0.3731 - accuracy: 0.8394 - val_loss: 0.3760 - val_accuracy: 0.8372\n",
      "Epoch 31/50\n",
      "7500/7500 [==============================] - 4s 535us/step - loss: 0.3730 - accuracy: 0.8393 - val_loss: 0.3760 - val_accuracy: 0.8378\n",
      "Epoch 32/50\n",
      "7500/7500 [==============================] - 4s 535us/step - loss: 0.3729 - accuracy: 0.8394 - val_loss: 0.3761 - val_accuracy: 0.8375\n",
      "Epoch 33/50\n",
      "7500/7500 [==============================] - 4s 535us/step - loss: 0.3728 - accuracy: 0.8394 - val_loss: 0.3759 - val_accuracy: 0.8379\n",
      "Epoch 34/50\n",
      "7500/7500 [==============================] - 4s 536us/step - loss: 0.3728 - accuracy: 0.8394 - val_loss: 0.3758 - val_accuracy: 0.8377\n",
      "Epoch 35/50\n",
      "7500/7500 [==============================] - 4s 535us/step - loss: 0.3727 - accuracy: 0.8394 - val_loss: 0.3757 - val_accuracy: 0.8379\n",
      "Epoch 36/50\n",
      "7500/7500 [==============================] - 4s 537us/step - loss: 0.3726 - accuracy: 0.8394 - val_loss: 0.3757 - val_accuracy: 0.8378\n",
      "Epoch 37/50\n",
      "7500/7500 [==============================] - 4s 539us/step - loss: 0.3725 - accuracy: 0.8396 - val_loss: 0.3759 - val_accuracy: 0.8379\n",
      "Epoch 38/50\n",
      "7500/7500 [==============================] - 4s 538us/step - loss: 0.3724 - accuracy: 0.8397 - val_loss: 0.3757 - val_accuracy: 0.8380\n",
      "Epoch 39/50\n",
      "7500/7500 [==============================] - 4s 558us/step - loss: 0.3724 - accuracy: 0.8396 - val_loss: 0.3761 - val_accuracy: 0.8376\n",
      "Epoch 40/50\n",
      "7500/7500 [==============================] - 4s 566us/step - loss: 0.3723 - accuracy: 0.8397 - val_loss: 0.3755 - val_accuracy: 0.8384\n",
      "Epoch 41/50\n",
      "7500/7500 [==============================] - 4s 537us/step - loss: 0.3722 - accuracy: 0.8396 - val_loss: 0.3756 - val_accuracy: 0.8379\n",
      "Epoch 42/50\n",
      "7500/7500 [==============================] - 5s 641us/step - loss: 0.3721 - accuracy: 0.8396 - val_loss: 0.3757 - val_accuracy: 0.8384\n",
      "Epoch 43/50\n",
      "7500/7500 [==============================] - 4s 545us/step - loss: 0.3721 - accuracy: 0.8398 - val_loss: 0.3753 - val_accuracy: 0.8382\n",
      "Epoch 44/50\n",
      "7500/7500 [==============================] - 4s 539us/step - loss: 0.3720 - accuracy: 0.8398 - val_loss: 0.3756 - val_accuracy: 0.8385\n",
      "Epoch 45/50\n",
      "7500/7500 [==============================] - 4s 540us/step - loss: 0.3720 - accuracy: 0.8399 - val_loss: 0.3753 - val_accuracy: 0.8385\n",
      "Epoch 46/50\n",
      "7500/7500 [==============================] - 4s 538us/step - loss: 0.3719 - accuracy: 0.8400 - val_loss: 0.3752 - val_accuracy: 0.8386\n",
      "Epoch 47/50\n",
      "7500/7500 [==============================] - 4s 537us/step - loss: 0.3718 - accuracy: 0.8400 - val_loss: 0.3752 - val_accuracy: 0.8384\n",
      "Epoch 48/50\n",
      "7500/7500 [==============================] - 4s 536us/step - loss: 0.3718 - accuracy: 0.8400 - val_loss: 0.3751 - val_accuracy: 0.8381\n",
      "Epoch 49/50\n",
      "7500/7500 [==============================] - 4s 542us/step - loss: 0.3717 - accuracy: 0.8401 - val_loss: 0.3750 - val_accuracy: 0.8384\n",
      "Epoch 50/50\n",
      "7500/7500 [==============================] - 4s 538us/step - loss: 0.3717 - accuracy: 0.8401 - val_loss: 0.3750 - val_accuracy: 0.8383\n"
     ]
    }
   ],
   "source": [
    "model_1.compile(SGD(lr = .003), \"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "run_hist_1 = model_1.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is 0.838\n",
      "roc-auc is 0.872\n"
     ]
    }
   ],
   "source": [
    "y_pred_class_nn_1 = model_1.predict_classes(X_test)\n",
    "y_pred_prob_nn_1 = model_1.predict(X_test)\n",
    "\n",
    "print('accuracy is {:.3f}'.format(accuracy_score(y_test,y_pred_class_nn_1)))\n",
    "print('roc-auc is {:.3f}'.format(roc_auc_score(y_test,y_pred_prob_nn_1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1ab91ba9e50>"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(run_hist_1.history[\"loss\"],'r', marker='.', label=\"Train Loss\")\n",
    "ax.plot(run_hist_1.history[\"val_loss\"],'b', marker='.', label=\"Validation Loss\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change to epochs 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "7424/7500 [============================>.] - ETA: 0s - loss: 0.3809 - accuracy: 0.8361WARNING:tensorflow:Callbacks method `on_test_batch_begin` is slow compared to the batch time (batch time: 0.0000s vs `on_test_batch_begin` time: 0.0010s). Check your callbacks.\n",
      "7500/7500 [==============================] - 4s 549us/step - loss: 0.3810 - accuracy: 0.8360 - val_loss: 0.3859 - val_accuracy: 0.8334\n",
      "Epoch 2/500\n",
      "7500/7500 [==============================] - 4s 551us/step - loss: 0.3807 - accuracy: 0.8361 - val_loss: 0.3809 - val_accuracy: 0.8354\n",
      "Epoch 3/500\n",
      "7500/7500 [==============================] - 6s 742us/step - loss: 0.3807 - accuracy: 0.8364 - val_loss: 0.3799 - val_accuracy: 0.8360\n",
      "Epoch 4/500\n",
      "7500/7500 [==============================] - 4s 559us/step - loss: 0.3809 - accuracy: 0.8364 - val_loss: 0.3798 - val_accuracy: 0.8357\n",
      "Epoch 5/500\n",
      "7500/7500 [==============================] - 4s 546us/step - loss: 0.3809 - accuracy: 0.8363 - val_loss: 0.3863 - val_accuracy: 0.8321\n",
      "Epoch 6/500\n",
      "7500/7500 [==============================] - 4s 544us/step - loss: 0.3809 - accuracy: 0.8359 - val_loss: 0.3809 - val_accuracy: 0.8365\n",
      "Epoch 7/500\n",
      "7500/7500 [==============================] - 4s 542us/step - loss: 0.3805 - accuracy: 0.8366 - val_loss: 0.3806 - val_accuracy: 0.8360\n",
      "Epoch 8/500\n",
      "7500/7500 [==============================] - 4s 544us/step - loss: 0.3807 - accuracy: 0.8362 - val_loss: 0.3789 - val_accuracy: 0.8361\n",
      "Epoch 9/500\n",
      "7500/7500 [==============================] - 4s 544us/step - loss: 0.3804 - accuracy: 0.8361 - val_loss: 0.3868 - val_accuracy: 0.8340\n",
      "Epoch 10/500\n",
      "7500/7500 [==============================] - 4s 543us/step - loss: 0.3806 - accuracy: 0.8363 - val_loss: 0.3809 - val_accuracy: 0.8367\n",
      "Epoch 11/500\n",
      "7500/7500 [==============================] - 4s 563us/step - loss: 0.3807 - accuracy: 0.8369 - val_loss: 0.3790 - val_accuracy: 0.8369\n",
      "Epoch 12/500\n",
      "7500/7500 [==============================] - 4s 558us/step - loss: 0.3807 - accuracy: 0.8360 - val_loss: 0.3895 - val_accuracy: 0.8336\n",
      "Epoch 13/500\n",
      "7500/7500 [==============================] - 4s 543us/step - loss: 0.3806 - accuracy: 0.8364 - val_loss: 0.3840 - val_accuracy: 0.8349\n",
      "Epoch 14/500\n",
      "7500/7500 [==============================] - 4s 544us/step - loss: 0.3804 - accuracy: 0.8361 - val_loss: 0.3797 - val_accuracy: 0.8365\n",
      "Epoch 15/500\n",
      "7500/7500 [==============================] - 4s 544us/step - loss: 0.3806 - accuracy: 0.8363 - val_loss: 0.3926 - val_accuracy: 0.8288\n",
      "Epoch 16/500\n",
      "7500/7500 [==============================] - 4s 553us/step - loss: 0.3808 - accuracy: 0.8360 - val_loss: 0.3790 - val_accuracy: 0.8370\n",
      "Epoch 17/500\n",
      "7500/7500 [==============================] - 4s 582us/step - loss: 0.3805 - accuracy: 0.8361 - val_loss: 0.3856 - val_accuracy: 0.8333\n",
      "Epoch 18/500\n",
      "7500/7500 [==============================] - 4s 545us/step - loss: 0.3804 - accuracy: 0.8369 - val_loss: 0.3804 - val_accuracy: 0.8372\n",
      "Epoch 19/500\n",
      "7500/7500 [==============================] - 4s 548us/step - loss: 0.3804 - accuracy: 0.8361 - val_loss: 0.3940 - val_accuracy: 0.8317\n",
      "Epoch 20/500\n",
      "7500/7500 [==============================] - 4s 551us/step - loss: 0.3804 - accuracy: 0.8362 - val_loss: 0.3782 - val_accuracy: 0.8363\n",
      "Epoch 21/500\n",
      "7500/7500 [==============================] - 4s 545us/step - loss: 0.3806 - accuracy: 0.8359 - val_loss: 0.3811 - val_accuracy: 0.8363\n",
      "Epoch 22/500\n",
      "7500/7500 [==============================] - 4s 543us/step - loss: 0.3805 - accuracy: 0.8364 - val_loss: 0.3784 - val_accuracy: 0.8374\n",
      "Epoch 23/500\n",
      "7500/7500 [==============================] - 4s 545us/step - loss: 0.3804 - accuracy: 0.8364 - val_loss: 0.3859 - val_accuracy: 0.8318\n",
      "Epoch 24/500\n",
      "7500/7500 [==============================] - 4s 545us/step - loss: 0.3802 - accuracy: 0.8365 - val_loss: 0.3807 - val_accuracy: 0.8361\n",
      "Epoch 25/500\n",
      "7500/7500 [==============================] - 4s 542us/step - loss: 0.3804 - accuracy: 0.8361 - val_loss: 0.3792 - val_accuracy: 0.8369\n",
      "Epoch 26/500\n",
      "7500/7500 [==============================] - 4s 542us/step - loss: 0.3804 - accuracy: 0.8361 - val_loss: 0.3807 - val_accuracy: 0.8362\n",
      "Epoch 27/500\n",
      "7500/7500 [==============================] - 4s 567us/step - loss: 0.3805 - accuracy: 0.8362 - val_loss: 0.3787 - val_accuracy: 0.8364\n",
      "Epoch 28/500\n",
      "7500/7500 [==============================] - 4s 558us/step - loss: 0.3804 - accuracy: 0.8361 - val_loss: 0.3788 - val_accuracy: 0.8361\n",
      "Epoch 29/500\n",
      "7500/7500 [==============================] - 4s 544us/step - loss: 0.3800 - accuracy: 0.8363 - val_loss: 0.4103 - val_accuracy: 0.8199\n",
      "Epoch 30/500\n",
      "7500/7500 [==============================] - 4s 541us/step - loss: 0.3802 - accuracy: 0.8364 - val_loss: 0.3796 - val_accuracy: 0.8363\n",
      "Epoch 31/500\n",
      "7500/7500 [==============================] - 4s 542us/step - loss: 0.3804 - accuracy: 0.8363 - val_loss: 0.3789 - val_accuracy: 0.8365\n",
      "Epoch 32/500\n",
      "7500/7500 [==============================] - 4s 543us/step - loss: 0.3803 - accuracy: 0.8366 - val_loss: 0.3829 - val_accuracy: 0.8345\n",
      "Epoch 33/500\n",
      "7500/7500 [==============================] - 4s 543us/step - loss: 0.3802 - accuracy: 0.8366 - val_loss: 0.3918 - val_accuracy: 0.8321\n",
      "Epoch 34/500\n",
      "7500/7500 [==============================] - 4s 552us/step - loss: 0.3804 - accuracy: 0.8363 - val_loss: 0.3829 - val_accuracy: 0.8355\n",
      "Epoch 35/500\n",
      "7500/7500 [==============================] - 4s 545us/step - loss: 0.3801 - accuracy: 0.8365 - val_loss: 0.3832 - val_accuracy: 0.8337\n",
      "Epoch 36/500\n",
      "7500/7500 [==============================] - 4s 546us/step - loss: 0.3802 - accuracy: 0.8361 - val_loss: 0.3782 - val_accuracy: 0.8371\n",
      "Epoch 37/500\n",
      "7500/7500 [==============================] - 4s 543us/step - loss: 0.3800 - accuracy: 0.8362 - val_loss: 0.3782 - val_accuracy: 0.8360\n",
      "Epoch 38/500\n",
      "7500/7500 [==============================] - 4s 540us/step - loss: 0.3801 - accuracy: 0.8363 - val_loss: 0.3776 - val_accuracy: 0.8372\n",
      "Epoch 39/500\n",
      "7500/7500 [==============================] - 4s 543us/step - loss: 0.3803 - accuracy: 0.8365 - val_loss: 0.3827 - val_accuracy: 0.8345\n",
      "Epoch 40/500\n",
      "7500/7500 [==============================] - 4s 542us/step - loss: 0.3799 - accuracy: 0.8368 - val_loss: 0.3896 - val_accuracy: 0.8295\n",
      "Epoch 41/500\n",
      "7500/7500 [==============================] - 4s 543us/step - loss: 0.3801 - accuracy: 0.8363 - val_loss: 0.3869 - val_accuracy: 0.8321\n",
      "Epoch 42/500\n",
      "7500/7500 [==============================] - 4s 540us/step - loss: 0.3802 - accuracy: 0.8362 - val_loss: 0.3812 - val_accuracy: 0.8359\n",
      "Epoch 43/500\n",
      "7500/7500 [==============================] - 4s 555us/step - loss: 0.3802 - accuracy: 0.8361 - val_loss: 0.3785 - val_accuracy: 0.8368\n",
      "Epoch 44/500\n",
      "7500/7500 [==============================] - 4s 563us/step - loss: 0.3800 - accuracy: 0.8367 - val_loss: 0.3802 - val_accuracy: 0.8358\n",
      "Epoch 45/500\n",
      "7500/7500 [==============================] - 4s 550us/step - loss: 0.3798 - accuracy: 0.8367 - val_loss: 0.3842 - val_accuracy: 0.8343\n",
      "Epoch 46/500\n",
      "7500/7500 [==============================] - 4s 552us/step - loss: 0.3798 - accuracy: 0.8368 - val_loss: 0.3790 - val_accuracy: 0.8368\n",
      "Epoch 47/500\n",
      "7500/7500 [==============================] - 4s 540us/step - loss: 0.3801 - accuracy: 0.8367 - val_loss: 0.3806 - val_accuracy: 0.8357\n",
      "Epoch 48/500\n",
      "7500/7500 [==============================] - 4s 539us/step - loss: 0.3799 - accuracy: 0.8367 - val_loss: 0.3782 - val_accuracy: 0.8371\n",
      "Epoch 49/500\n",
      "7500/7500 [==============================] - 4s 541us/step - loss: 0.3798 - accuracy: 0.8364 - val_loss: 0.3780 - val_accuracy: 0.8370\n",
      "Epoch 50/500\n",
      "7500/7500 [==============================] - 4s 539us/step - loss: 0.3799 - accuracy: 0.8365 - val_loss: 0.3862 - val_accuracy: 0.8334\n",
      "Epoch 51/500\n",
      "7500/7500 [==============================] - 4s 540us/step - loss: 0.3800 - accuracy: 0.8367 - val_loss: 0.3793 - val_accuracy: 0.8368\n",
      "Epoch 52/500\n",
      "7500/7500 [==============================] - 4s 540us/step - loss: 0.3801 - accuracy: 0.8369 - val_loss: 0.3778 - val_accuracy: 0.8374\n",
      "Epoch 53/500\n",
      "7500/7500 [==============================] - 4s 541us/step - loss: 0.3798 - accuracy: 0.8366 - val_loss: 0.3830 - val_accuracy: 0.8347\n",
      "Epoch 54/500\n",
      "7500/7500 [==============================] - 4s 539us/step - loss: 0.3797 - accuracy: 0.8370 - val_loss: 0.3855 - val_accuracy: 0.8332\n",
      "Epoch 55/500\n",
      "7500/7500 [==============================] - 4s 545us/step - loss: 0.3799 - accuracy: 0.8369 - val_loss: 0.3793 - val_accuracy: 0.8373\n",
      "Epoch 56/500\n",
      "7500/7500 [==============================] - 4s 542us/step - loss: 0.3797 - accuracy: 0.8362 - val_loss: 0.3777 - val_accuracy: 0.8377\n",
      "Epoch 57/500\n",
      "7500/7500 [==============================] - 4s 541us/step - loss: 0.3798 - accuracy: 0.8363 - val_loss: 0.3847 - val_accuracy: 0.8363\n",
      "Epoch 58/500\n",
      "7500/7500 [==============================] - 4s 539us/step - loss: 0.3798 - accuracy: 0.8362 - val_loss: 0.3786 - val_accuracy: 0.8371\n",
      "Epoch 59/500\n",
      "7500/7500 [==============================] - 4s 549us/step - loss: 0.3798 - accuracy: 0.8367 - val_loss: 0.3776 - val_accuracy: 0.8377\n",
      "Epoch 60/500\n",
      "7500/7500 [==============================] - 4s 565us/step - loss: 0.3795 - accuracy: 0.8367 - val_loss: 0.3890 - val_accuracy: 0.8316\n",
      "Epoch 61/500\n",
      "7500/7500 [==============================] - 4s 566us/step - loss: 0.3799 - accuracy: 0.8364 - val_loss: 0.3811 - val_accuracy: 0.8354\n",
      "Epoch 62/500\n",
      "7500/7500 [==============================] - 4s 542us/step - loss: 0.3798 - accuracy: 0.8364 - val_loss: 0.3783 - val_accuracy: 0.8377\n",
      "Epoch 63/500\n",
      "7500/7500 [==============================] - 4s 543us/step - loss: 0.3797 - accuracy: 0.8368 - val_loss: 0.3779 - val_accuracy: 0.8372\n",
      "Epoch 64/500\n",
      "7500/7500 [==============================] - 4s 543us/step - loss: 0.3796 - accuracy: 0.8367 - val_loss: 0.3863 - val_accuracy: 0.8320\n",
      "Epoch 65/500\n",
      "7500/7500 [==============================] - 4s 541us/step - loss: 0.3795 - accuracy: 0.8372 - val_loss: 0.3786 - val_accuracy: 0.8369\n",
      "Epoch 66/500\n",
      "7500/7500 [==============================] - 4s 541us/step - loss: 0.3796 - accuracy: 0.8369 - val_loss: 0.3786 - val_accuracy: 0.8372\n",
      "Epoch 67/500\n",
      "7500/7500 [==============================] - 4s 539us/step - loss: 0.3797 - accuracy: 0.8369 - val_loss: 0.3933 - val_accuracy: 0.8293\n",
      "Epoch 68/500\n",
      "7500/7500 [==============================] - 4s 541us/step - loss: 0.3798 - accuracy: 0.8367 - val_loss: 0.3803 - val_accuracy: 0.8363\n",
      "Epoch 69/500\n",
      "7500/7500 [==============================] - 4s 539us/step - loss: 0.3793 - accuracy: 0.8370 - val_loss: 0.3986 - val_accuracy: 0.8298\n",
      "Epoch 70/500\n",
      "7500/7500 [==============================] - 4s 539us/step - loss: 0.3795 - accuracy: 0.8367 - val_loss: 0.3787 - val_accuracy: 0.8372\n",
      "Epoch 71/500\n",
      "7500/7500 [==============================] - 4s 541us/step - loss: 0.3795 - accuracy: 0.8368 - val_loss: 0.3972 - val_accuracy: 0.8322\n",
      "Epoch 72/500\n",
      "7500/7500 [==============================] - 4s 542us/step - loss: 0.3796 - accuracy: 0.8367 - val_loss: 0.3781 - val_accuracy: 0.8374\n",
      "Epoch 73/500\n",
      "7500/7500 [==============================] - 4s 537us/step - loss: 0.3796 - accuracy: 0.8367 - val_loss: 0.3802 - val_accuracy: 0.8357\n",
      "Epoch 74/500\n",
      "7500/7500 [==============================] - 4s 539us/step - loss: 0.3795 - accuracy: 0.8367 - val_loss: 0.3778 - val_accuracy: 0.8374\n",
      "Epoch 75/500\n",
      "7500/7500 [==============================] - 4s 566us/step - loss: 0.3795 - accuracy: 0.8369 - val_loss: 0.3802 - val_accuracy: 0.8362\n",
      "Epoch 76/500\n",
      "7500/7500 [==============================] - 4s 588us/step - loss: 0.3794 - accuracy: 0.8368 - val_loss: 0.3805 - val_accuracy: 0.8357\n",
      "Epoch 77/500\n",
      "7500/7500 [==============================] - 4s 545us/step - loss: 0.3794 - accuracy: 0.8369 - val_loss: 0.3827 - val_accuracy: 0.8346\n",
      "Epoch 78/500\n",
      "7500/7500 [==============================] - 4s 539us/step - loss: 0.3795 - accuracy: 0.8369 - val_loss: 0.3789 - val_accuracy: 0.8369\n",
      "Epoch 79/500\n",
      "7500/7500 [==============================] - 4s 546us/step - loss: 0.3795 - accuracy: 0.8370 - val_loss: 0.3787 - val_accuracy: 0.8368\n",
      "Epoch 80/500\n",
      "7500/7500 [==============================] - 4s 542us/step - loss: 0.3796 - accuracy: 0.8365 - val_loss: 0.3782 - val_accuracy: 0.8376\n",
      "Epoch 81/500\n",
      "7500/7500 [==============================] - 4s 538us/step - loss: 0.3795 - accuracy: 0.8365 - val_loss: 0.3816 - val_accuracy: 0.8356\n",
      "Epoch 82/500\n",
      "7500/7500 [==============================] - 4s 562us/step - loss: 0.3795 - accuracy: 0.8364 - val_loss: 0.3810 - val_accuracy: 0.8364\n",
      "Epoch 83/500\n",
      "7500/7500 [==============================] - 4s 544us/step - loss: 0.3795 - accuracy: 0.8368 - val_loss: 0.3792 - val_accuracy: 0.8372\n",
      "Epoch 84/500\n",
      "7500/7500 [==============================] - 4s 543us/step - loss: 0.3794 - accuracy: 0.8368 - val_loss: 0.3779 - val_accuracy: 0.8367\n",
      "Epoch 85/500\n",
      "7500/7500 [==============================] - 4s 541us/step - loss: 0.3792 - accuracy: 0.8368 - val_loss: 0.3886 - val_accuracy: 0.8324\n",
      "Epoch 86/500\n",
      "7500/7500 [==============================] - 4s 548us/step - loss: 0.3793 - accuracy: 0.8365 - val_loss: 0.4133 - val_accuracy: 0.8196\n",
      "Epoch 87/500\n",
      "7500/7500 [==============================] - 4s 544us/step - loss: 0.3792 - accuracy: 0.8367 - val_loss: 0.3780 - val_accuracy: 0.8365\n",
      "Epoch 88/500\n",
      "7500/7500 [==============================] - 4s 545us/step - loss: 0.3793 - accuracy: 0.8365 - val_loss: 0.3798 - val_accuracy: 0.8360\n",
      "Epoch 89/500\n",
      "7500/7500 [==============================] - 4s 540us/step - loss: 0.3792 - accuracy: 0.8368 - val_loss: 0.3892 - val_accuracy: 0.8309\n",
      "Epoch 90/500\n",
      "7500/7500 [==============================] - 4s 544us/step - loss: 0.3793 - accuracy: 0.8367 - val_loss: 0.3819 - val_accuracy: 0.8357\n",
      "Epoch 91/500\n",
      "7500/7500 [==============================] - 4s 566us/step - loss: 0.3791 - accuracy: 0.8367 - val_loss: 0.3785 - val_accuracy: 0.8366\n",
      "Epoch 92/500\n",
      "7500/7500 [==============================] - 4s 569us/step - loss: 0.3792 - accuracy: 0.8369 - val_loss: 0.3792 - val_accuracy: 0.8376\n",
      "Epoch 93/500\n",
      "7500/7500 [==============================] - 4s 547us/step - loss: 0.3790 - accuracy: 0.8369 - val_loss: 0.4199 - val_accuracy: 0.8155\n",
      "Epoch 94/500\n",
      "7500/7500 [==============================] - 4s 544us/step - loss: 0.3790 - accuracy: 0.8367 - val_loss: 0.3780 - val_accuracy: 0.8375\n",
      "Epoch 95/500\n",
      "7500/7500 [==============================] - 4s 540us/step - loss: 0.3793 - accuracy: 0.8368 - val_loss: 0.3878 - val_accuracy: 0.8339\n",
      "Epoch 96/500\n",
      "7500/7500 [==============================] - 4s 539us/step - loss: 0.3791 - accuracy: 0.8368 - val_loss: 0.3795 - val_accuracy: 0.8363\n",
      "Epoch 97/500\n",
      "7500/7500 [==============================] - 4s 541us/step - loss: 0.3790 - accuracy: 0.8372 - val_loss: 0.3781 - val_accuracy: 0.8375\n",
      "Epoch 98/500\n",
      "7500/7500 [==============================] - 4s 539us/step - loss: 0.3793 - accuracy: 0.8371 - val_loss: 0.3809 - val_accuracy: 0.8362\n",
      "Epoch 99/500\n",
      "7500/7500 [==============================] - 5s 604us/step - loss: 0.3792 - accuracy: 0.8365 - val_loss: 0.3775 - val_accuracy: 0.8374\n",
      "Epoch 100/500\n",
      "7500/7500 [==============================] - 5s 657us/step - loss: 0.3792 - accuracy: 0.8366 - val_loss: 0.3962 - val_accuracy: 0.8274\n",
      "Epoch 101/500\n",
      "7500/7500 [==============================] - 5s 617us/step - loss: 0.3792 - accuracy: 0.8370 - val_loss: 0.3866 - val_accuracy: 0.8341\n",
      "Epoch 102/500\n",
      "7500/7500 [==============================] - 4s 556us/step - loss: 0.3791 - accuracy: 0.8370 - val_loss: 0.3783 - val_accuracy: 0.8372\n",
      "Epoch 103/500\n",
      "7500/7500 [==============================] - 4s 542us/step - loss: 0.3792 - accuracy: 0.8368 - val_loss: 0.3893 - val_accuracy: 0.8329\n",
      "Epoch 104/500\n",
      "7500/7500 [==============================] - 4s 562us/step - loss: 0.3790 - accuracy: 0.8367 - val_loss: 0.3793 - val_accuracy: 0.8376\n",
      "Epoch 105/500\n",
      "7500/7500 [==============================] - 4s 540us/step - loss: 0.3790 - accuracy: 0.8367 - val_loss: 0.3786 - val_accuracy: 0.8370\n",
      "Epoch 106/500\n",
      "7500/7500 [==============================] - 4s 542us/step - loss: 0.3788 - accuracy: 0.8372 - val_loss: 0.3821 - val_accuracy: 0.8369\n",
      "Epoch 107/500\n",
      "7500/7500 [==============================] - 4s 567us/step - loss: 0.3792 - accuracy: 0.8367 - val_loss: 0.3783 - val_accuracy: 0.8372\n",
      "Epoch 108/500\n",
      "7500/7500 [==============================] - 4s 552us/step - loss: 0.3791 - accuracy: 0.8368 - val_loss: 0.3779 - val_accuracy: 0.8373\n",
      "Epoch 109/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 4s 541us/step - loss: 0.3791 - accuracy: 0.8368 - val_loss: 0.3872 - val_accuracy: 0.8328\n",
      "Epoch 110/500\n",
      "7500/7500 [==============================] - 4s 540us/step - loss: 0.3793 - accuracy: 0.8372 - val_loss: 0.3788 - val_accuracy: 0.8366\n",
      "Epoch 111/500\n",
      "7500/7500 [==============================] - 4s 544us/step - loss: 0.3789 - accuracy: 0.8371 - val_loss: 0.3773 - val_accuracy: 0.8376\n",
      "Epoch 112/500\n",
      "7500/7500 [==============================] - 4s 538us/step - loss: 0.3791 - accuracy: 0.8368 - val_loss: 0.3807 - val_accuracy: 0.8375\n",
      "Epoch 113/500\n",
      "7500/7500 [==============================] - 4s 538us/step - loss: 0.3788 - accuracy: 0.8370 - val_loss: 0.3810 - val_accuracy: 0.8354\n",
      "Epoch 114/500\n",
      "7500/7500 [==============================] - 4s 541us/step - loss: 0.3789 - accuracy: 0.8370 - val_loss: 0.3831 - val_accuracy: 0.8353\n",
      "Epoch 115/500\n",
      "7500/7500 [==============================] - 4s 546us/step - loss: 0.3789 - accuracy: 0.8374 - val_loss: 0.3793 - val_accuracy: 0.8369\n",
      "Epoch 116/500\n",
      "7500/7500 [==============================] - 4s 537us/step - loss: 0.3788 - accuracy: 0.8373 - val_loss: 0.3887 - val_accuracy: 0.8322\n",
      "Epoch 117/500\n",
      "7500/7500 [==============================] - 4s 537us/step - loss: 0.3789 - accuracy: 0.8374 - val_loss: 0.3785 - val_accuracy: 0.8367\n",
      "Epoch 118/500\n",
      "7500/7500 [==============================] - 4s 540us/step - loss: 0.3790 - accuracy: 0.8373 - val_loss: 0.3810 - val_accuracy: 0.8360\n",
      "Epoch 119/500\n",
      "7500/7500 [==============================] - 4s 540us/step - loss: 0.3788 - accuracy: 0.8369 - val_loss: 0.3834 - val_accuracy: 0.8349\n",
      "Epoch 120/500\n",
      "7500/7500 [==============================] - 4s 538us/step - loss: 0.3787 - accuracy: 0.8374 - val_loss: 0.3783 - val_accuracy: 0.8369\n",
      "Epoch 121/500\n",
      "7500/7500 [==============================] - 4s 538us/step - loss: 0.3789 - accuracy: 0.8371 - val_loss: 0.3776 - val_accuracy: 0.8375\n",
      "Epoch 122/500\n",
      "7500/7500 [==============================] - 4s 540us/step - loss: 0.3790 - accuracy: 0.8373 - val_loss: 0.3789 - val_accuracy: 0.8366\n",
      "Epoch 123/500\n",
      "7500/7500 [==============================] - 4s 565us/step - loss: 0.3787 - accuracy: 0.8372 - val_loss: 0.3768 - val_accuracy: 0.8380\n",
      "Epoch 124/500\n",
      "7500/7500 [==============================] - 4s 566us/step - loss: 0.3789 - accuracy: 0.8370 - val_loss: 0.3832 - val_accuracy: 0.8356\n",
      "Epoch 125/500\n",
      "7500/7500 [==============================] - 4s 541us/step - loss: 0.3787 - accuracy: 0.8370 - val_loss: 0.3777 - val_accuracy: 0.8371\n",
      "Epoch 126/500\n",
      "7500/7500 [==============================] - 4s 541us/step - loss: 0.3790 - accuracy: 0.8372 - val_loss: 0.3784 - val_accuracy: 0.8376\n",
      "Epoch 127/500\n",
      "7500/7500 [==============================] - 4s 540us/step - loss: 0.3789 - accuracy: 0.8371 - val_loss: 0.3860 - val_accuracy: 0.8327\n",
      "Epoch 128/500\n",
      "7500/7500 [==============================] - 4s 539us/step - loss: 0.3787 - accuracy: 0.8373 - val_loss: 0.3868 - val_accuracy: 0.8331\n",
      "Epoch 129/500\n",
      "7500/7500 [==============================] - 4s 543us/step - loss: 0.3786 - accuracy: 0.8373 - val_loss: 0.3860 - val_accuracy: 0.8326\n",
      "Epoch 130/500\n",
      "7500/7500 [==============================] - 4s 541us/step - loss: 0.3786 - accuracy: 0.8373 - val_loss: 0.3785 - val_accuracy: 0.8375\n",
      "Epoch 131/500\n",
      "7500/7500 [==============================] - 4s 542us/step - loss: 0.3787 - accuracy: 0.8375 - val_loss: 0.3787 - val_accuracy: 0.8363\n",
      "Epoch 132/500\n",
      "7500/7500 [==============================] - 4s 545us/step - loss: 0.3788 - accuracy: 0.8370 - val_loss: 0.3828 - val_accuracy: 0.8354\n",
      "Epoch 133/500\n",
      "7500/7500 [==============================] - 4s 554us/step - loss: 0.3786 - accuracy: 0.8375 - val_loss: 0.3784 - val_accuracy: 0.8369\n",
      "Epoch 134/500\n",
      "7500/7500 [==============================] - 4s 561us/step - loss: 0.3786 - accuracy: 0.8367 - val_loss: 0.3788 - val_accuracy: 0.8366\n",
      "Epoch 135/500\n",
      "7500/7500 [==============================] - 4s 588us/step - loss: 0.3786 - accuracy: 0.8374 - val_loss: 0.3897 - val_accuracy: 0.8336\n",
      "Epoch 136/500\n",
      "7500/7500 [==============================] - 4s 597us/step - loss: 0.3788 - accuracy: 0.8368 - val_loss: 0.3838 - val_accuracy: 0.8333\n",
      "Epoch 137/500\n",
      "7500/7500 [==============================] - 4s 584us/step - loss: 0.3788 - accuracy: 0.8369 - val_loss: 0.3777 - val_accuracy: 0.8373\n",
      "Epoch 138/500\n",
      "7500/7500 [==============================] - 4s 548us/step - loss: 0.3786 - accuracy: 0.8371 - val_loss: 0.3770 - val_accuracy: 0.8377\n",
      "Epoch 139/500\n",
      "7500/7500 [==============================] - 4s 591us/step - loss: 0.3786 - accuracy: 0.8372 - val_loss: 0.3848 - val_accuracy: 0.8335\n",
      "Epoch 140/500\n",
      "7500/7500 [==============================] - 4s 596us/step - loss: 0.3787 - accuracy: 0.8373 - val_loss: 0.3790 - val_accuracy: 0.8363\n",
      "Epoch 141/500\n",
      "7500/7500 [==============================] - 4s 560us/step - loss: 0.3786 - accuracy: 0.8368 - val_loss: 0.3793 - val_accuracy: 0.8370\n",
      "Epoch 142/500\n",
      "7500/7500 [==============================] - 4s 598us/step - loss: 0.3787 - accuracy: 0.8373 - val_loss: 0.3811 - val_accuracy: 0.8361\n",
      "Epoch 143/500\n",
      "7500/7500 [==============================] - 7s 933us/step - loss: 0.3786 - accuracy: 0.8367 - val_loss: 0.3844 - val_accuracy: 0.8343\n",
      "Epoch 144/500\n",
      "7500/7500 [==============================] - 4s 543us/step - loss: 0.3786 - accuracy: 0.8372 - val_loss: 0.3784 - val_accuracy: 0.8367\n",
      "Epoch 145/500\n",
      "7500/7500 [==============================] - 5s 671us/step - loss: 0.3786 - accuracy: 0.8370 - val_loss: 0.3780 - val_accuracy: 0.8369\n",
      "Epoch 146/500\n",
      "7500/7500 [==============================] - 7s 966us/step - loss: 0.3785 - accuracy: 0.8375 - val_loss: 0.3781 - val_accuracy: 0.8377\n",
      "Epoch 147/500\n",
      "7500/7500 [==============================] - 5s 611us/step - loss: 0.3788 - accuracy: 0.8373 - val_loss: 0.3786 - val_accuracy: 0.8367\n",
      "Epoch 148/500\n",
      "7500/7500 [==============================] - 4s 594us/step - loss: 0.3785 - accuracy: 0.8370 - val_loss: 0.3809 - val_accuracy: 0.8354\n",
      "Epoch 149/500\n",
      "7500/7500 [==============================] - 4s 563us/step - loss: 0.3785 - accuracy: 0.8368 - val_loss: 0.3823 - val_accuracy: 0.8351\n",
      "Epoch 150/500\n",
      "7500/7500 [==============================] - 4s 569us/step - loss: 0.3785 - accuracy: 0.8369 - val_loss: 0.3848 - val_accuracy: 0.8334\n",
      "Epoch 151/500\n",
      "7500/7500 [==============================] - 4s 563us/step - loss: 0.3786 - accuracy: 0.8371 - val_loss: 0.3783 - val_accuracy: 0.8373\n",
      "Epoch 152/500\n",
      "7500/7500 [==============================] - 4s 544us/step - loss: 0.3786 - accuracy: 0.8373 - val_loss: 0.3778 - val_accuracy: 0.8374\n",
      "Epoch 153/500\n",
      "7500/7500 [==============================] - 5s 667us/step - loss: 0.3785 - accuracy: 0.8368 - val_loss: 0.3807 - val_accuracy: 0.8362\n",
      "Epoch 154/500\n",
      "7500/7500 [==============================] - 4s 577us/step - loss: 0.3784 - accuracy: 0.8373 - val_loss: 0.3825 - val_accuracy: 0.8357\n",
      "Epoch 155/500\n",
      "7500/7500 [==============================] - 4s 559us/step - loss: 0.3787 - accuracy: 0.8368 - val_loss: 0.3817 - val_accuracy: 0.8351\n",
      "Epoch 156/500\n",
      "7500/7500 [==============================] - 4s 559us/step - loss: 0.3785 - accuracy: 0.8371 - val_loss: 0.3809 - val_accuracy: 0.8356\n",
      "Epoch 157/500\n",
      "7500/7500 [==============================] - 4s 535us/step - loss: 0.3782 - accuracy: 0.8373 - val_loss: 0.3784 - val_accuracy: 0.8368\n",
      "Epoch 158/500\n",
      "7500/7500 [==============================] - 4s 545us/step - loss: 0.3785 - accuracy: 0.8373 - val_loss: 0.3780 - val_accuracy: 0.8378\n",
      "Epoch 159/500\n",
      "7500/7500 [==============================] - 4s 539us/step - loss: 0.3785 - accuracy: 0.8367 - val_loss: 0.3768 - val_accuracy: 0.8381\n",
      "Epoch 160/500\n",
      "7500/7500 [==============================] - 4s 564us/step - loss: 0.3784 - accuracy: 0.8367 - val_loss: 0.3780 - val_accuracy: 0.8375\n",
      "Epoch 161/500\n",
      "7500/7500 [==============================] - 4s 544us/step - loss: 0.3783 - accuracy: 0.8375 - val_loss: 0.3818 - val_accuracy: 0.8357\n",
      "Epoch 162/500\n",
      "7500/7500 [==============================] - 4s 568us/step - loss: 0.3783 - accuracy: 0.8371 - val_loss: 0.3908 - val_accuracy: 0.8322\n",
      "Epoch 163/500\n",
      "7500/7500 [==============================] - 6s 847us/step - loss: 0.3783 - accuracy: 0.8374 - val_loss: 0.3787 - val_accuracy: 0.8360\n",
      "Epoch 164/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 5s 704us/step - loss: 0.3783 - accuracy: 0.8373 - val_loss: 0.3794 - val_accuracy: 0.8366\n",
      "Epoch 165/500\n",
      "7500/7500 [==============================] - 4s 579us/step - loss: 0.3783 - accuracy: 0.8368 - val_loss: 0.3808 - val_accuracy: 0.8357\n",
      "Epoch 166/500\n",
      "7500/7500 [==============================] - 4s 553us/step - loss: 0.3786 - accuracy: 0.8371 - val_loss: 0.3774 - val_accuracy: 0.8375\n",
      "Epoch 167/500\n",
      "7500/7500 [==============================] - 4s 536us/step - loss: 0.3785 - accuracy: 0.8371 - val_loss: 0.3764 - val_accuracy: 0.8378\n",
      "Epoch 168/500\n",
      "7500/7500 [==============================] - 4s 588us/step - loss: 0.3783 - accuracy: 0.8374 - val_loss: 0.3770 - val_accuracy: 0.8375\n",
      "Epoch 169/500\n",
      "7500/7500 [==============================] - 4s 587us/step - loss: 0.3785 - accuracy: 0.8369 - val_loss: 0.3766 - val_accuracy: 0.8381\n",
      "Epoch 170/500\n",
      "7500/7500 [==============================] - 4s 556us/step - loss: 0.3785 - accuracy: 0.8374 - val_loss: 0.3785 - val_accuracy: 0.8368\n",
      "Epoch 171/500\n",
      "7500/7500 [==============================] - 5s 627us/step - loss: 0.3783 - accuracy: 0.8373 - val_loss: 0.3775 - val_accuracy: 0.8372\n",
      "Epoch 172/500\n",
      "7500/7500 [==============================] - 4s 544us/step - loss: 0.3784 - accuracy: 0.8373 - val_loss: 0.3807 - val_accuracy: 0.8363\n",
      "Epoch 173/500\n",
      "7500/7500 [==============================] - 4s 545us/step - loss: 0.3783 - accuracy: 0.8373 - val_loss: 0.3801 - val_accuracy: 0.8362\n",
      "Epoch 174/500\n",
      "7500/7500 [==============================] - 4s 541us/step - loss: 0.3782 - accuracy: 0.8372 - val_loss: 0.3887 - val_accuracy: 0.8314\n",
      "Epoch 175/500\n",
      "7500/7500 [==============================] - 6s 843us/step - loss: 0.3782 - accuracy: 0.8370 - val_loss: 0.3833 - val_accuracy: 0.8353\n",
      "Epoch 176/500\n",
      "7500/7500 [==============================] - 4s 584us/step - loss: 0.3784 - accuracy: 0.8372 - val_loss: 0.3769 - val_accuracy: 0.8376\n",
      "Epoch 177/500\n",
      "7500/7500 [==============================] - 6s 755us/step - loss: 0.3784 - accuracy: 0.8374 - val_loss: 0.3778 - val_accuracy: 0.8373\n",
      "Epoch 178/500\n",
      "7500/7500 [==============================] - 11s 1ms/step - loss: 0.3783 - accuracy: 0.8372 - val_loss: 0.3773 - val_accuracy: 0.8379\n",
      "Epoch 179/500\n",
      "7500/7500 [==============================] - 17s 2ms/step - loss: 0.3783 - accuracy: 0.8368 - val_loss: 0.3876 - val_accuracy: 0.8331\n",
      "Epoch 180/500\n",
      "7500/7500 [==============================] - 12s 2ms/step - loss: 0.3783 - accuracy: 0.8371 - val_loss: 0.3776 - val_accuracy: 0.8367\n",
      "Epoch 181/500\n",
      "7500/7500 [==============================] - 7s 956us/step - loss: 0.3781 - accuracy: 0.8374 - val_loss: 0.3835 - val_accuracy: 0.8343\n",
      "Epoch 182/500\n",
      "7500/7500 [==============================] - 4s 593us/step - loss: 0.3781 - accuracy: 0.8371 - val_loss: 0.3805 - val_accuracy: 0.8360\n",
      "Epoch 183/500\n",
      "7500/7500 [==============================] - 4s 550us/step - loss: 0.3780 - accuracy: 0.8377 - val_loss: 0.3786 - val_accuracy: 0.8375\n",
      "Epoch 184/500\n",
      "7500/7500 [==============================] - 4s 570us/step - loss: 0.3783 - accuracy: 0.8372 - val_loss: 0.3791 - val_accuracy: 0.8363\n",
      "Epoch 185/500\n",
      "7500/7500 [==============================] - 4s 521us/step - loss: 0.3782 - accuracy: 0.8374 - val_loss: 0.3774 - val_accuracy: 0.8380\n",
      "Epoch 186/500\n",
      "7500/7500 [==============================] - 4s 546us/step - loss: 0.3782 - accuracy: 0.8372 - val_loss: 0.3770 - val_accuracy: 0.8379\n",
      "Epoch 187/500\n",
      "7500/7500 [==============================] - 4s 528us/step - loss: 0.3781 - accuracy: 0.8375 - val_loss: 0.3788 - val_accuracy: 0.8370\n",
      "Epoch 188/500\n",
      "7500/7500 [==============================] - 4s 518us/step - loss: 0.3782 - accuracy: 0.8370 - val_loss: 0.3779 - val_accuracy: 0.8376\n",
      "Epoch 189/500\n",
      "7500/7500 [==============================] - 4s 543us/step - loss: 0.3783 - accuracy: 0.8369 - val_loss: 0.3769 - val_accuracy: 0.8378\n",
      "Epoch 190/500\n",
      "7500/7500 [==============================] - 4s 517us/step - loss: 0.3781 - accuracy: 0.8374 - val_loss: 0.3781 - val_accuracy: 0.8374\n",
      "Epoch 191/500\n",
      "7500/7500 [==============================] - 4s 546us/step - loss: 0.3782 - accuracy: 0.8370 - val_loss: 0.3782 - val_accuracy: 0.8375\n",
      "Epoch 192/500\n",
      "7500/7500 [==============================] - 4s 592us/step - loss: 0.3782 - accuracy: 0.8377 - val_loss: 0.3824 - val_accuracy: 0.8356\n",
      "Epoch 193/500\n",
      "7500/7500 [==============================] - 4s 547us/step - loss: 0.3782 - accuracy: 0.8376 - val_loss: 0.3777 - val_accuracy: 0.8377\n",
      "Epoch 194/500\n",
      "7500/7500 [==============================] - 4s 529us/step - loss: 0.3782 - accuracy: 0.8373 - val_loss: 0.3790 - val_accuracy: 0.8375\n",
      "Epoch 195/500\n",
      "7500/7500 [==============================] - 4s 546us/step - loss: 0.3780 - accuracy: 0.8373 - val_loss: 0.3785 - val_accuracy: 0.8375\n",
      "Epoch 196/500\n",
      "7500/7500 [==============================] - 4s 556us/step - loss: 0.3782 - accuracy: 0.8370 - val_loss: 0.3776 - val_accuracy: 0.8371\n",
      "Epoch 197/500\n",
      "7500/7500 [==============================] - 4s 544us/step - loss: 0.3778 - accuracy: 0.8372 - val_loss: 0.3808 - val_accuracy: 0.8359\n",
      "Epoch 198/500\n",
      "7500/7500 [==============================] - 4s 521us/step - loss: 0.3782 - accuracy: 0.8377 - val_loss: 0.3922 - val_accuracy: 0.8312\n",
      "Epoch 199/500\n",
      "7500/7500 [==============================] - 4s 546us/step - loss: 0.3781 - accuracy: 0.8373 - val_loss: 0.3781 - val_accuracy: 0.8377\n",
      "Epoch 200/500\n",
      "7500/7500 [==============================] - 4s 531us/step - loss: 0.3782 - accuracy: 0.8374 - val_loss: 0.3801 - val_accuracy: 0.8363\n",
      "Epoch 201/500\n",
      "7500/7500 [==============================] - 4s 532us/step - loss: 0.3780 - accuracy: 0.8371 - val_loss: 0.3835 - val_accuracy: 0.8348\n",
      "Epoch 202/500\n",
      "7500/7500 [==============================] - 4s 540us/step - loss: 0.3781 - accuracy: 0.8373 - val_loss: 0.3781 - val_accuracy: 0.8373\n",
      "Epoch 203/500\n",
      "7500/7500 [==============================] - 4s 549us/step - loss: 0.3781 - accuracy: 0.8373 - val_loss: 0.3785 - val_accuracy: 0.8374\n",
      "Epoch 204/500\n",
      "7500/7500 [==============================] - 4s 541us/step - loss: 0.3779 - accuracy: 0.8374 - val_loss: 0.3812 - val_accuracy: 0.8347\n",
      "Epoch 205/500\n",
      "7500/7500 [==============================] - 4s 553us/step - loss: 0.3777 - accuracy: 0.8376 - val_loss: 0.3813 - val_accuracy: 0.8362\n",
      "Epoch 206/500\n",
      "7500/7500 [==============================] - 4s 541us/step - loss: 0.3779 - accuracy: 0.8375 - val_loss: 0.3861 - val_accuracy: 0.8335\n",
      "Epoch 207/500\n",
      "7500/7500 [==============================] - 4s 537us/step - loss: 0.3779 - accuracy: 0.8371 - val_loss: 0.3809 - val_accuracy: 0.8352\n",
      "Epoch 208/500\n",
      "7500/7500 [==============================] - 4s 560us/step - loss: 0.3779 - accuracy: 0.8371 - val_loss: 0.3771 - val_accuracy: 0.8379\n",
      "Epoch 209/500\n",
      "7500/7500 [==============================] - 4s 556us/step - loss: 0.3781 - accuracy: 0.8373 - val_loss: 0.3782 - val_accuracy: 0.8376\n",
      "Epoch 210/500\n",
      "7500/7500 [==============================] - 4s 536us/step - loss: 0.3780 - accuracy: 0.8370 - val_loss: 0.3782 - val_accuracy: 0.8373\n",
      "Epoch 211/500\n",
      "7500/7500 [==============================] - 4s 528us/step - loss: 0.3779 - accuracy: 0.8373 - val_loss: 0.3782 - val_accuracy: 0.8367\n",
      "Epoch 212/500\n",
      "7500/7500 [==============================] - 4s 519us/step - loss: 0.3781 - accuracy: 0.8370 - val_loss: 0.3814 - val_accuracy: 0.8353\n",
      "Epoch 213/500\n",
      "7500/7500 [==============================] - 5s 665us/step - loss: 0.3780 - accuracy: 0.8371 - val_loss: 0.3850 - val_accuracy: 0.8328\n",
      "Epoch 214/500\n",
      "7500/7500 [==============================] - 4s 564us/step - loss: 0.3779 - accuracy: 0.8374 - val_loss: 0.3892 - val_accuracy: 0.8313\n",
      "Epoch 215/500\n",
      "7500/7500 [==============================] - 4s 531us/step - loss: 0.3779 - accuracy: 0.8373 - val_loss: 0.3831 - val_accuracy: 0.8346\n",
      "Epoch 216/500\n",
      "7500/7500 [==============================] - 4s 534us/step - loss: 0.3779 - accuracy: 0.8372 - val_loss: 0.3784 - val_accuracy: 0.8368\n",
      "Epoch 217/500\n",
      "7500/7500 [==============================] - 4s 525us/step - loss: 0.3780 - accuracy: 0.8374 - val_loss: 0.3771 - val_accuracy: 0.8374\n",
      "Epoch 218/500\n",
      "7500/7500 [==============================] - 4s 529us/step - loss: 0.3779 - accuracy: 0.8373 - val_loss: 0.3781 - val_accuracy: 0.8372\n",
      "Epoch 219/500\n",
      "7500/7500 [==============================] - 4s 540us/step - loss: 0.3778 - accuracy: 0.8373 - val_loss: 0.3773 - val_accuracy: 0.8381\n",
      "Epoch 220/500\n",
      "7500/7500 [==============================] - 4s 534us/step - loss: 0.3779 - accuracy: 0.8378 - val_loss: 0.3771 - val_accuracy: 0.8380\n",
      "Epoch 221/500\n",
      "7500/7500 [==============================] - 4s 521us/step - loss: 0.3778 - accuracy: 0.8374 - val_loss: 0.3818 - val_accuracy: 0.8357\n",
      "Epoch 222/500\n",
      "7500/7500 [==============================] - 4s 513us/step - loss: 0.3780 - accuracy: 0.8372 - val_loss: 0.3778 - val_accuracy: 0.8370\n",
      "Epoch 223/500\n",
      "7500/7500 [==============================] - 4s 516us/step - loss: 0.3777 - accuracy: 0.8371 - val_loss: 0.3855 - val_accuracy: 0.8338\n",
      "Epoch 224/500\n",
      "7500/7500 [==============================] - 4s 550us/step - loss: 0.3779 - accuracy: 0.8375 - val_loss: 0.3777 - val_accuracy: 0.8382\n",
      "Epoch 225/500\n",
      "7500/7500 [==============================] - 4s 522us/step - loss: 0.3778 - accuracy: 0.8377 - val_loss: 0.3811 - val_accuracy: 0.8366\n",
      "Epoch 226/500\n",
      "7500/7500 [==============================] - 4s 522us/step - loss: 0.3778 - accuracy: 0.8379 - val_loss: 0.3774 - val_accuracy: 0.8376\n",
      "Epoch 227/500\n",
      "7500/7500 [==============================] - 4s 524us/step - loss: 0.3780 - accuracy: 0.8371 - val_loss: 0.3777 - val_accuracy: 0.8371\n",
      "Epoch 228/500\n",
      "7500/7500 [==============================] - 4s 524us/step - loss: 0.3779 - accuracy: 0.8370 - val_loss: 0.3806 - val_accuracy: 0.8360\n",
      "Epoch 229/500\n",
      "7500/7500 [==============================] - 4s 515us/step - loss: 0.3778 - accuracy: 0.8377 - val_loss: 0.3778 - val_accuracy: 0.8366\n",
      "Epoch 230/500\n",
      "7500/7500 [==============================] - 4s 529us/step - loss: 0.3779 - accuracy: 0.8373 - val_loss: 0.3774 - val_accuracy: 0.8369\n",
      "Epoch 231/500\n",
      "7500/7500 [==============================] - 4s 514us/step - loss: 0.3780 - accuracy: 0.8373 - val_loss: 0.3776 - val_accuracy: 0.8372\n",
      "Epoch 232/500\n",
      "7500/7500 [==============================] - 4s 513us/step - loss: 0.3776 - accuracy: 0.8374 - val_loss: 0.3788 - val_accuracy: 0.8376\n",
      "Epoch 233/500\n",
      "7500/7500 [==============================] - 4s 512us/step - loss: 0.3778 - accuracy: 0.8375 - val_loss: 0.3769 - val_accuracy: 0.8378\n",
      "Epoch 234/500\n",
      "7500/7500 [==============================] - 4s 513us/step - loss: 0.3779 - accuracy: 0.8377 - val_loss: 0.3786 - val_accuracy: 0.8376\n",
      "Epoch 235/500\n",
      "7500/7500 [==============================] - 4s 513us/step - loss: 0.3776 - accuracy: 0.8380 - val_loss: 0.4013 - val_accuracy: 0.8253\n",
      "Epoch 236/500\n",
      "7500/7500 [==============================] - 4s 546us/step - loss: 0.3779 - accuracy: 0.8373 - val_loss: 0.3776 - val_accuracy: 0.8369\n",
      "Epoch 237/500\n",
      "7500/7500 [==============================] - 4s 529us/step - loss: 0.3779 - accuracy: 0.8376 - val_loss: 0.3769 - val_accuracy: 0.8374\n",
      "Epoch 238/500\n",
      "7500/7500 [==============================] - 4s 530us/step - loss: 0.3778 - accuracy: 0.8372 - val_loss: 0.3784 - val_accuracy: 0.8368\n",
      "Epoch 239/500\n",
      "7500/7500 [==============================] - 4s 537us/step - loss: 0.3778 - accuracy: 0.8375 - val_loss: 0.3769 - val_accuracy: 0.8373\n",
      "Epoch 240/500\n",
      "7500/7500 [==============================] - 4s 535us/step - loss: 0.3777 - accuracy: 0.8378 - val_loss: 0.3768 - val_accuracy: 0.8376\n",
      "Epoch 241/500\n",
      "7500/7500 [==============================] - 4s 538us/step - loss: 0.3778 - accuracy: 0.8373 - val_loss: 0.3766 - val_accuracy: 0.8378\n",
      "Epoch 242/500\n",
      "7500/7500 [==============================] - 4s 518us/step - loss: 0.3777 - accuracy: 0.8375 - val_loss: 0.3791 - val_accuracy: 0.8367\n",
      "Epoch 243/500\n",
      "7500/7500 [==============================] - 4s 531us/step - loss: 0.3777 - accuracy: 0.8374 - val_loss: 0.3776 - val_accuracy: 0.8374\n",
      "Epoch 244/500\n",
      "7500/7500 [==============================] - 4s 527us/step - loss: 0.3777 - accuracy: 0.8373 - val_loss: 0.3775 - val_accuracy: 0.8369\n",
      "Epoch 245/500\n",
      "7500/7500 [==============================] - 4s 524us/step - loss: 0.3777 - accuracy: 0.8372 - val_loss: 0.3885 - val_accuracy: 0.8325\n",
      "Epoch 246/500\n",
      "7500/7500 [==============================] - 4s 523us/step - loss: 0.3778 - accuracy: 0.8373 - val_loss: 0.3789 - val_accuracy: 0.8364\n",
      "Epoch 247/500\n",
      "7500/7500 [==============================] - 4s 537us/step - loss: 0.3777 - accuracy: 0.8375 - val_loss: 0.3771 - val_accuracy: 0.8376\n",
      "Epoch 248/500\n",
      "7500/7500 [==============================] - 4s 538us/step - loss: 0.3777 - accuracy: 0.8374 - val_loss: 0.3775 - val_accuracy: 0.8369\n",
      "Epoch 249/500\n",
      "7500/7500 [==============================] - 4s 554us/step - loss: 0.3777 - accuracy: 0.8373 - val_loss: 0.3777 - val_accuracy: 0.8376\n",
      "Epoch 250/500\n",
      "7500/7500 [==============================] - 5s 604us/step - loss: 0.3775 - accuracy: 0.8374 - val_loss: 0.4049 - val_accuracy: 0.8238\n",
      "Epoch 251/500\n",
      "7500/7500 [==============================] - 8s 1ms/step - loss: 0.3775 - accuracy: 0.8376 - val_loss: 0.3774 - val_accuracy: 0.8373\n",
      "Epoch 252/500\n",
      "7500/7500 [==============================] - 9s 1ms/step - loss: 0.3777 - accuracy: 0.8375 - val_loss: 0.3770 - val_accuracy: 0.8375\n",
      "Epoch 253/500\n",
      "7500/7500 [==============================] - 4s 527us/step - loss: 0.3775 - accuracy: 0.8370 - val_loss: 0.3813 - val_accuracy: 0.8354\n",
      "Epoch 254/500\n",
      "7500/7500 [==============================] - 6s 793us/step - loss: 0.3777 - accuracy: 0.8372 - val_loss: 0.3796 - val_accuracy: 0.8363\n",
      "Epoch 255/500\n",
      "7500/7500 [==============================] - 7s 930us/step - loss: 0.3776 - accuracy: 0.8373 - val_loss: 0.3798 - val_accuracy: 0.8354\n",
      "Epoch 256/500\n",
      "7500/7500 [==============================] - 8s 1ms/step - loss: 0.3777 - accuracy: 0.8374 - val_loss: 0.3776 - val_accuracy: 0.8377\n",
      "Epoch 257/500\n",
      "7500/7500 [==============================] - 5s 647us/step - loss: 0.3777 - accuracy: 0.8373 - val_loss: 0.3803 - val_accuracy: 0.8359\n",
      "Epoch 258/500\n",
      "7500/7500 [==============================] - 12s 2ms/step - loss: 0.3777 - accuracy: 0.8374 - val_loss: 0.3780 - val_accuracy: 0.8374\n",
      "Epoch 259/500\n",
      "7500/7500 [==============================] - 8s 1ms/step - loss: 0.3777 - accuracy: 0.8375 - val_loss: 0.3770 - val_accuracy: 0.8372\n",
      "Epoch 260/500\n",
      "7500/7500 [==============================] - 6s 859us/step - loss: 0.3777 - accuracy: 0.8376 - val_loss: 0.3795 - val_accuracy: 0.8363\n",
      "Epoch 261/500\n",
      "7500/7500 [==============================] - 6s 846us/step - loss: 0.3777 - accuracy: 0.8378 - val_loss: 0.3767 - val_accuracy: 0.8380\n",
      "Epoch 262/500\n",
      "7500/7500 [==============================] - 6s 774us/step - loss: 0.3777 - accuracy: 0.8372 - val_loss: 0.3767 - val_accuracy: 0.8379\n",
      "Epoch 263/500\n",
      "7500/7500 [==============================] - 5s 609us/step - loss: 0.3775 - accuracy: 0.8377 - val_loss: 0.3775 - val_accuracy: 0.8379\n",
      "Epoch 264/500\n",
      "7500/7500 [==============================] - 5s 709us/step - loss: 0.3776 - accuracy: 0.8376 - val_loss: 0.3765 - val_accuracy: 0.8384\n",
      "Epoch 265/500\n",
      "7500/7500 [==============================] - 4s 579us/step - loss: 0.3775 - accuracy: 0.8380 - val_loss: 0.3767 - val_accuracy: 0.8381\n",
      "Epoch 266/500\n",
      "7500/7500 [==============================] - 5s 657us/step - loss: 0.3775 - accuracy: 0.8377 - val_loss: 0.3773 - val_accuracy: 0.8377\n",
      "Epoch 267/500\n",
      "7500/7500 [==============================] - 4s 540us/step - loss: 0.3776 - accuracy: 0.8382 - val_loss: 0.3766 - val_accuracy: 0.8379\n",
      "Epoch 268/500\n",
      "7500/7500 [==============================] - 4s 544us/step - loss: 0.3777 - accuracy: 0.8376 - val_loss: 0.3785 - val_accuracy: 0.8369\n",
      "Epoch 269/500\n",
      "7500/7500 [==============================] - 4s 542us/step - loss: 0.3775 - accuracy: 0.8373 - val_loss: 0.3775 - val_accuracy: 0.8373\n",
      "Epoch 270/500\n",
      "7500/7500 [==============================] - 4s 542us/step - loss: 0.3776 - accuracy: 0.8373 - val_loss: 0.3773 - val_accuracy: 0.8375\n",
      "Epoch 271/500\n",
      "7500/7500 [==============================] - 4s 542us/step - loss: 0.3776 - accuracy: 0.8375 - val_loss: 0.3804 - val_accuracy: 0.8369\n",
      "Epoch 272/500\n",
      "7500/7500 [==============================] - 4s 546us/step - loss: 0.3775 - accuracy: 0.8374 - val_loss: 0.3906 - val_accuracy: 0.8302\n",
      "Epoch 273/500\n",
      "7500/7500 [==============================] - 4s 543us/step - loss: 0.3773 - accuracy: 0.8378 - val_loss: 0.3780 - val_accuracy: 0.8374\n",
      "Epoch 274/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 4s 542us/step - loss: 0.3776 - accuracy: 0.8373 - val_loss: 0.3798 - val_accuracy: 0.8370\n",
      "Epoch 275/500\n",
      "7500/7500 [==============================] - 4s 540us/step - loss: 0.3776 - accuracy: 0.8379 - val_loss: 0.3784 - val_accuracy: 0.8374\n",
      "Epoch 276/500\n",
      "7500/7500 [==============================] - 4s 556us/step - loss: 0.3776 - accuracy: 0.8376 - val_loss: 0.3798 - val_accuracy: 0.8367\n",
      "Epoch 277/500\n",
      "7500/7500 [==============================] - 4s 551us/step - loss: 0.3776 - accuracy: 0.8375 - val_loss: 0.3781 - val_accuracy: 0.8378\n",
      "Epoch 278/500\n",
      "7500/7500 [==============================] - 4s 540us/step - loss: 0.3776 - accuracy: 0.8377 - val_loss: 0.3800 - val_accuracy: 0.8370\n",
      "Epoch 279/500\n",
      "7500/7500 [==============================] - 4s 566us/step - loss: 0.3776 - accuracy: 0.8374 - val_loss: 0.3785 - val_accuracy: 0.8368\n",
      "Epoch 280/500\n",
      "7500/7500 [==============================] - 4s 552us/step - loss: 0.3776 - accuracy: 0.8374 - val_loss: 0.3770 - val_accuracy: 0.8374\n",
      "Epoch 281/500\n",
      "7500/7500 [==============================] - 4s 541us/step - loss: 0.3774 - accuracy: 0.8379 - val_loss: 0.3773 - val_accuracy: 0.8378\n",
      "Epoch 282/500\n",
      "7500/7500 [==============================] - 4s 541us/step - loss: 0.3776 - accuracy: 0.8375 - val_loss: 0.3772 - val_accuracy: 0.8379\n",
      "Epoch 283/500\n",
      "7500/7500 [==============================] - 4s 544us/step - loss: 0.3775 - accuracy: 0.8372 - val_loss: 0.3851 - val_accuracy: 0.8335\n",
      "Epoch 284/500\n",
      "7500/7500 [==============================] - 4s 549us/step - loss: 0.3774 - accuracy: 0.8378 - val_loss: 0.3774 - val_accuracy: 0.8375\n",
      "Epoch 285/500\n",
      "7500/7500 [==============================] - 4s 543us/step - loss: 0.3775 - accuracy: 0.8376 - val_loss: 0.3802 - val_accuracy: 0.8370\n",
      "Epoch 286/500\n",
      "7500/7500 [==============================] - 4s 547us/step - loss: 0.3774 - accuracy: 0.8373 - val_loss: 0.4053 - val_accuracy: 0.8263\n",
      "Epoch 287/500\n",
      "7500/7500 [==============================] - 4s 550us/step - loss: 0.3772 - accuracy: 0.8376 - val_loss: 0.3765 - val_accuracy: 0.8380\n",
      "Epoch 288/500\n",
      "7500/7500 [==============================] - 4s 554us/step - loss: 0.3775 - accuracy: 0.8374 - val_loss: 0.3792 - val_accuracy: 0.8364\n",
      "Epoch 289/500\n",
      "7500/7500 [==============================] - 4s 562us/step - loss: 0.3776 - accuracy: 0.8372 - val_loss: 0.3767 - val_accuracy: 0.8382\n",
      "Epoch 290/500\n",
      "7500/7500 [==============================] - 4s 547us/step - loss: 0.3773 - accuracy: 0.8379 - val_loss: 0.3782 - val_accuracy: 0.8366\n",
      "Epoch 291/500\n",
      "7500/7500 [==============================] - 4s 545us/step - loss: 0.3775 - accuracy: 0.8375 - val_loss: 0.3768 - val_accuracy: 0.8377\n",
      "Epoch 292/500\n",
      "7500/7500 [==============================] - 4s 544us/step - loss: 0.3776 - accuracy: 0.8377 - val_loss: 0.3802 - val_accuracy: 0.8359\n",
      "Epoch 293/500\n",
      "7500/7500 [==============================] - 4s 545us/step - loss: 0.3773 - accuracy: 0.8374 - val_loss: 0.3765 - val_accuracy: 0.8382\n",
      "Epoch 294/500\n",
      "7500/7500 [==============================] - 4s 542us/step - loss: 0.3773 - accuracy: 0.8378 - val_loss: 0.3784 - val_accuracy: 0.8374\n",
      "Epoch 295/500\n",
      "7500/7500 [==============================] - 4s 567us/step - loss: 0.3774 - accuracy: 0.8373 - val_loss: 0.3773 - val_accuracy: 0.8372\n",
      "Epoch 296/500\n",
      "7500/7500 [==============================] - 4s 557us/step - loss: 0.3772 - accuracy: 0.8374 - val_loss: 0.3815 - val_accuracy: 0.8349\n",
      "Epoch 297/500\n",
      "7500/7500 [==============================] - 4s 543us/step - loss: 0.3774 - accuracy: 0.8375 - val_loss: 0.3844 - val_accuracy: 0.8335\n",
      "Epoch 298/500\n",
      "7500/7500 [==============================] - 4s 541us/step - loss: 0.3772 - accuracy: 0.8373 - val_loss: 0.3801 - val_accuracy: 0.8364\n",
      "Epoch 299/500\n",
      "7500/7500 [==============================] - 4s 542us/step - loss: 0.3774 - accuracy: 0.8371 - val_loss: 0.3821 - val_accuracy: 0.8349\n",
      "Epoch 300/500\n",
      "7500/7500 [==============================] - 4s 542us/step - loss: 0.3773 - accuracy: 0.8380 - val_loss: 0.3787 - val_accuracy: 0.8368\n",
      "Epoch 301/500\n",
      "7500/7500 [==============================] - 4s 541us/step - loss: 0.3775 - accuracy: 0.8373 - val_loss: 0.3771 - val_accuracy: 0.8378\n",
      "Epoch 302/500\n",
      "7500/7500 [==============================] - 4s 542us/step - loss: 0.3773 - accuracy: 0.8373 - val_loss: 0.3793 - val_accuracy: 0.8366\n",
      "Epoch 303/500\n",
      "7500/7500 [==============================] - 4s 542us/step - loss: 0.3773 - accuracy: 0.8380 - val_loss: 0.3781 - val_accuracy: 0.8371\n",
      "Epoch 304/500\n",
      "7500/7500 [==============================] - 4s 541us/step - loss: 0.3774 - accuracy: 0.8376 - val_loss: 0.3767 - val_accuracy: 0.8378\n",
      "Epoch 305/500\n",
      "7500/7500 [==============================] - 4s 540us/step - loss: 0.3772 - accuracy: 0.8379 - val_loss: 0.3801 - val_accuracy: 0.8354\n",
      "Epoch 306/500\n",
      "7500/7500 [==============================] - 4s 544us/step - loss: 0.3772 - accuracy: 0.8375 - val_loss: 0.3767 - val_accuracy: 0.8380\n",
      "Epoch 307/500\n",
      "7500/7500 [==============================] - 4s 542us/step - loss: 0.3772 - accuracy: 0.8374 - val_loss: 0.3775 - val_accuracy: 0.8375\n",
      "Epoch 308/500\n",
      "7500/7500 [==============================] - 4s 545us/step - loss: 0.3773 - accuracy: 0.8374 - val_loss: 0.3770 - val_accuracy: 0.8374\n",
      "Epoch 309/500\n",
      "7500/7500 [==============================] - 4s 541us/step - loss: 0.3772 - accuracy: 0.8375 - val_loss: 0.3817 - val_accuracy: 0.8356\n",
      "Epoch 310/500\n",
      "7500/7500 [==============================] - 4s 542us/step - loss: 0.3774 - accuracy: 0.8381 - val_loss: 0.3821 - val_accuracy: 0.8354\n",
      "Epoch 311/500\n",
      "7500/7500 [==============================] - 4s 581us/step - loss: 0.3775 - accuracy: 0.8375 - val_loss: 0.3824 - val_accuracy: 0.8354\n",
      "Epoch 312/500\n",
      "7500/7500 [==============================] - 4s 556us/step - loss: 0.3774 - accuracy: 0.8375 - val_loss: 0.3811 - val_accuracy: 0.8364\n",
      "Epoch 313/500\n",
      "7500/7500 [==============================] - 4s 539us/step - loss: 0.3772 - accuracy: 0.8382 - val_loss: 0.3793 - val_accuracy: 0.8363\n",
      "Epoch 314/500\n",
      "7500/7500 [==============================] - 4s 544us/step - loss: 0.3773 - accuracy: 0.8375 - val_loss: 0.3815 - val_accuracy: 0.8360\n",
      "Epoch 315/500\n",
      "7500/7500 [==============================] - 4s 543us/step - loss: 0.3771 - accuracy: 0.8379 - val_loss: 0.3790 - val_accuracy: 0.8371\n",
      "Epoch 316/500\n",
      "7500/7500 [==============================] - 4s 540us/step - loss: 0.3773 - accuracy: 0.8373 - val_loss: 0.3786 - val_accuracy: 0.8371\n",
      "Epoch 317/500\n",
      "7500/7500 [==============================] - 4s 567us/step - loss: 0.3774 - accuracy: 0.8375 - val_loss: 0.3760 - val_accuracy: 0.8379\n",
      "Epoch 318/500\n",
      "7500/7500 [==============================] - 4s 539us/step - loss: 0.3770 - accuracy: 0.8377 - val_loss: 0.3788 - val_accuracy: 0.8370\n",
      "Epoch 319/500\n",
      "7500/7500 [==============================] - 4s 542us/step - loss: 0.3772 - accuracy: 0.8375 - val_loss: 0.3850 - val_accuracy: 0.8330\n",
      "Epoch 320/500\n",
      "7500/7500 [==============================] - 4s 541us/step - loss: 0.3772 - accuracy: 0.8378 - val_loss: 0.3769 - val_accuracy: 0.8370\n",
      "Epoch 321/500\n",
      "7500/7500 [==============================] - 4s 540us/step - loss: 0.3773 - accuracy: 0.8376 - val_loss: 0.3782 - val_accuracy: 0.8374\n",
      "Epoch 322/500\n",
      "7500/7500 [==============================] - 4s 540us/step - loss: 0.3773 - accuracy: 0.8374 - val_loss: 0.3789 - val_accuracy: 0.8372\n",
      "Epoch 323/500\n",
      "7500/7500 [==============================] - 4s 575us/step - loss: 0.3772 - accuracy: 0.8374 - val_loss: 0.3758 - val_accuracy: 0.8381\n",
      "Epoch 324/500\n",
      "7500/7500 [==============================] - 4s 540us/step - loss: 0.3771 - accuracy: 0.8377 - val_loss: 0.3821 - val_accuracy: 0.8342\n",
      "Epoch 325/500\n",
      "7500/7500 [==============================] - 4s 563us/step - loss: 0.3773 - accuracy: 0.8379 - val_loss: 0.3785 - val_accuracy: 0.8372\n",
      "Epoch 326/500\n",
      "7500/7500 [==============================] - 4s 543us/step - loss: 0.3772 - accuracy: 0.8376 - val_loss: 0.3771 - val_accuracy: 0.8375\n",
      "Epoch 327/500\n",
      "7500/7500 [==============================] - 4s 556us/step - loss: 0.3774 - accuracy: 0.8376 - val_loss: 0.3766 - val_accuracy: 0.8377\n",
      "Epoch 328/500\n",
      "7500/7500 [==============================] - 4s 570us/step - loss: 0.3772 - accuracy: 0.8377 - val_loss: 0.3767 - val_accuracy: 0.8380\n",
      "Epoch 329/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 4s 542us/step - loss: 0.3770 - accuracy: 0.8376 - val_loss: 0.3784 - val_accuracy: 0.8377\n",
      "Epoch 330/500\n",
      "7500/7500 [==============================] - 4s 539us/step - loss: 0.3772 - accuracy: 0.8375 - val_loss: 0.3787 - val_accuracy: 0.8375\n",
      "Epoch 331/500\n",
      "7500/7500 [==============================] - 4s 559us/step - loss: 0.3772 - accuracy: 0.8374 - val_loss: 0.3757 - val_accuracy: 0.8381\n",
      "Epoch 332/500\n",
      "7500/7500 [==============================] - 4s 548us/step - loss: 0.3772 - accuracy: 0.8379 - val_loss: 0.3774 - val_accuracy: 0.8373\n",
      "Epoch 333/500\n",
      "7500/7500 [==============================] - 4s 544us/step - loss: 0.3770 - accuracy: 0.8378 - val_loss: 0.3777 - val_accuracy: 0.8367\n",
      "Epoch 334/500\n",
      "7500/7500 [==============================] - 47s 6ms/step - loss: 0.3773 - accuracy: 0.8377 - val_loss: 0.3804 - val_accuracy: 0.8360\n",
      "Epoch 335/500\n",
      "7500/7500 [==============================] - 8s 1ms/step - loss: 0.3772 - accuracy: 0.8378 - val_loss: 0.3765 - val_accuracy: 0.8378\n",
      "Epoch 336/500\n",
      "7500/7500 [==============================] - 4s 590us/step - loss: 0.3772 - accuracy: 0.8375 - val_loss: 0.3792 - val_accuracy: 0.8361\n",
      "Epoch 337/500\n",
      "7500/7500 [==============================] - 5s 652us/step - loss: 0.3772 - accuracy: 0.8373 - val_loss: 0.3778 - val_accuracy: 0.8374\n",
      "Epoch 338/500\n",
      "7500/7500 [==============================] - 5s 606us/step - loss: 0.3771 - accuracy: 0.8373 - val_loss: 0.3776 - val_accuracy: 0.8376\n",
      "Epoch 339/500\n",
      "7500/7500 [==============================] - 4s 586us/step - loss: 0.3770 - accuracy: 0.8377 - val_loss: 0.3792 - val_accuracy: 0.8362\n",
      "Epoch 340/500\n",
      "7500/7500 [==============================] - 5s 601us/step - loss: 0.3769 - accuracy: 0.8380 - val_loss: 0.3782 - val_accuracy: 0.8374\n",
      "Epoch 341/500\n",
      "7500/7500 [==============================] - 5s 718us/step - loss: 0.3771 - accuracy: 0.8376 - val_loss: 0.3773 - val_accuracy: 0.8372\n",
      "Epoch 342/500\n",
      "7500/7500 [==============================] - 5s 664us/step - loss: 0.3771 - accuracy: 0.8376 - val_loss: 0.4040 - val_accuracy: 0.8240\n",
      "Epoch 343/500\n",
      "7500/7500 [==============================] - 5s 726us/step - loss: 0.3771 - accuracy: 0.8376 - val_loss: 0.3785 - val_accuracy: 0.8357\n",
      "Epoch 344/500\n",
      "7500/7500 [==============================] - 5s 641us/step - loss: 0.3770 - accuracy: 0.8380 - val_loss: 0.3796 - val_accuracy: 0.8360\n",
      "Epoch 345/500\n",
      "7500/7500 [==============================] - 5s 643us/step - loss: 0.3773 - accuracy: 0.8372 - val_loss: 0.3775 - val_accuracy: 0.8377\n",
      "Epoch 346/500\n",
      "7500/7500 [==============================] - 6s 804us/step - loss: 0.3770 - accuracy: 0.8378 - val_loss: 0.3768 - val_accuracy: 0.8378\n",
      "Epoch 347/500\n",
      "7500/7500 [==============================] - 6s 765us/step - loss: 0.3770 - accuracy: 0.8373 - val_loss: 0.3769 - val_accuracy: 0.8382\n",
      "Epoch 348/500\n",
      "7500/7500 [==============================] - 5s 695us/step - loss: 0.3768 - accuracy: 0.8375 - val_loss: 0.3781 - val_accuracy: 0.8375\n",
      "Epoch 349/500\n",
      "7500/7500 [==============================] - 5s 694us/step - loss: 0.3769 - accuracy: 0.8381 - val_loss: 0.3761 - val_accuracy: 0.8380\n",
      "Epoch 350/500\n",
      "7500/7500 [==============================] - 5s 683us/step - loss: 0.3771 - accuracy: 0.8375 - val_loss: 0.3823 - val_accuracy: 0.8350\n",
      "Epoch 351/500\n",
      "7500/7500 [==============================] - 6s 750us/step - loss: 0.3773 - accuracy: 0.8375 - val_loss: 0.3778 - val_accuracy: 0.8368\n",
      "Epoch 352/500\n",
      "7500/7500 [==============================] - 6s 787us/step - loss: 0.3770 - accuracy: 0.8376 - val_loss: 0.3776 - val_accuracy: 0.8372\n",
      "Epoch 353/500\n",
      "7500/7500 [==============================] - 7s 966us/step - loss: 0.3771 - accuracy: 0.8378 - val_loss: 0.3781 - val_accuracy: 0.8369\n",
      "Epoch 354/500\n",
      "7500/7500 [==============================] - 8s 1ms/step - loss: 0.3772 - accuracy: 0.8377 - val_loss: 0.3881 - val_accuracy: 0.8328\n",
      "Epoch 355/500\n",
      "7500/7500 [==============================] - 7s 880us/step - loss: 0.3770 - accuracy: 0.8378 - val_loss: 0.3787 - val_accuracy: 0.8366\n",
      "Epoch 356/500\n",
      "7500/7500 [==============================] - 5s 685us/step - loss: 0.3770 - accuracy: 0.8377 - val_loss: 0.3864 - val_accuracy: 0.8334\n",
      "Epoch 357/500\n",
      "7500/7500 [==============================] - 5s 646us/step - loss: 0.3769 - accuracy: 0.8376 - val_loss: 0.3784 - val_accuracy: 0.8371\n",
      "Epoch 358/500\n",
      "7500/7500 [==============================] - 5s 670us/step - loss: 0.3771 - accuracy: 0.8379 - val_loss: 0.3815 - val_accuracy: 0.8356\n",
      "Epoch 359/500\n",
      "7500/7500 [==============================] - 5s 625us/step - loss: 0.3771 - accuracy: 0.8379 - val_loss: 0.3760 - val_accuracy: 0.8382\n",
      "Epoch 360/500\n",
      "7500/7500 [==============================] - 5s 607us/step - loss: 0.3768 - accuracy: 0.8379 - val_loss: 0.3796 - val_accuracy: 0.8361\n",
      "Epoch 361/500\n",
      "7500/7500 [==============================] - 5s 646us/step - loss: 0.3771 - accuracy: 0.8380 - val_loss: 0.3788 - val_accuracy: 0.8364\n",
      "Epoch 362/500\n",
      "7500/7500 [==============================] - 5s 612us/step - loss: 0.3771 - accuracy: 0.8376 - val_loss: 0.3781 - val_accuracy: 0.8377\n",
      "Epoch 363/500\n",
      "7500/7500 [==============================] - 5s 606us/step - loss: 0.3769 - accuracy: 0.8375 - val_loss: 0.3796 - val_accuracy: 0.8367\n",
      "Epoch 364/500\n",
      "7500/7500 [==============================] - 4s 598us/step - loss: 0.3769 - accuracy: 0.8379 - val_loss: 0.3908 - val_accuracy: 0.8317\n",
      "Epoch 365/500\n",
      "7500/7500 [==============================] - 5s 614us/step - loss: 0.3770 - accuracy: 0.8379 - val_loss: 0.3769 - val_accuracy: 0.8379\n",
      "Epoch 366/500\n",
      "7500/7500 [==============================] - 5s 614us/step - loss: 0.3769 - accuracy: 0.8381 - val_loss: 0.3766 - val_accuracy: 0.8382\n",
      "Epoch 367/500\n",
      "7500/7500 [==============================] - 4s 576us/step - loss: 0.3771 - accuracy: 0.8373 - val_loss: 0.3764 - val_accuracy: 0.8375\n",
      "Epoch 368/500\n",
      "7500/7500 [==============================] - 4s 565us/step - loss: 0.3768 - accuracy: 0.8377 - val_loss: 0.3774 - val_accuracy: 0.8378\n",
      "Epoch 369/500\n",
      "7500/7500 [==============================] - 4s 575us/step - loss: 0.3772 - accuracy: 0.8377 - val_loss: 0.3846 - val_accuracy: 0.8344\n",
      "Epoch 370/500\n",
      "7500/7500 [==============================] - 4s 568us/step - loss: 0.3768 - accuracy: 0.8376 - val_loss: 0.3769 - val_accuracy: 0.8382\n",
      "Epoch 371/500\n",
      "7500/7500 [==============================] - 4s 554us/step - loss: 0.3770 - accuracy: 0.8376 - val_loss: 0.3853 - val_accuracy: 0.8329\n",
      "Epoch 372/500\n",
      "7500/7500 [==============================] - 5s 651us/step - loss: 0.3769 - accuracy: 0.8375 - val_loss: 0.3772 - val_accuracy: 0.8373\n",
      "Epoch 373/500\n",
      "7500/7500 [==============================] - 5s 619us/step - loss: 0.3769 - accuracy: 0.8377 - val_loss: 0.3772 - val_accuracy: 0.8370\n",
      "Epoch 374/500\n",
      "7500/7500 [==============================] - 4s 592us/step - loss: 0.3768 - accuracy: 0.8374 - val_loss: 0.3791 - val_accuracy: 0.8367\n",
      "Epoch 375/500\n",
      "7500/7500 [==============================] - 5s 601us/step - loss: 0.3768 - accuracy: 0.8378 - val_loss: 0.3785 - val_accuracy: 0.8371\n",
      "Epoch 376/500\n",
      "7500/7500 [==============================] - 5s 618us/step - loss: 0.3768 - accuracy: 0.8380 - val_loss: 0.3774 - val_accuracy: 0.8377\n",
      "Epoch 377/500\n",
      "7500/7500 [==============================] - 5s 604us/step - loss: 0.3769 - accuracy: 0.8377 - val_loss: 0.3814 - val_accuracy: 0.8354\n",
      "Epoch 378/500\n",
      "7500/7500 [==============================] - 5s 632us/step - loss: 0.3769 - accuracy: 0.8378 - val_loss: 0.3815 - val_accuracy: 0.8360\n",
      "Epoch 379/500\n",
      "7500/7500 [==============================] - 5s 607us/step - loss: 0.3769 - accuracy: 0.8377 - val_loss: 0.3771 - val_accuracy: 0.8378\n",
      "Epoch 380/500\n",
      "7500/7500 [==============================] - 5s 657us/step - loss: 0.3769 - accuracy: 0.8380 - val_loss: 0.3773 - val_accuracy: 0.8371\n",
      "Epoch 381/500\n",
      "7500/7500 [==============================] - 5s 645us/step - loss: 0.3769 - accuracy: 0.8377 - val_loss: 0.3770 - val_accuracy: 0.8371\n",
      "Epoch 382/500\n",
      "7500/7500 [==============================] - 5s 717us/step - loss: 0.3769 - accuracy: 0.8377 - val_loss: 0.3831 - val_accuracy: 0.8348\n",
      "Epoch 383/500\n",
      "7500/7500 [==============================] - 5s 622us/step - loss: 0.3768 - accuracy: 0.8379 - val_loss: 0.3775 - val_accuracy: 0.8379\n",
      "Epoch 384/500\n",
      "7500/7500 [==============================] - 4s 591us/step - loss: 0.3771 - accuracy: 0.8375 - val_loss: 0.3766 - val_accuracy: 0.8385\n",
      "Epoch 385/500\n",
      "7500/7500 [==============================] - 4s 591us/step - loss: 0.3768 - accuracy: 0.8379 - val_loss: 0.3774 - val_accuracy: 0.8372\n",
      "Epoch 386/500\n",
      "7500/7500 [==============================] - 5s 684us/step - loss: 0.3768 - accuracy: 0.8376 - val_loss: 0.3804 - val_accuracy: 0.8358\n",
      "Epoch 387/500\n",
      "7500/7500 [==============================] - 5s 656us/step - loss: 0.3767 - accuracy: 0.8378 - val_loss: 0.3763 - val_accuracy: 0.8382\n",
      "Epoch 388/500\n",
      "7500/7500 [==============================] - 5s 679us/step - loss: 0.3768 - accuracy: 0.8378 - val_loss: 0.3792 - val_accuracy: 0.8361\n",
      "Epoch 389/500\n",
      "7500/7500 [==============================] - 5s 690us/step - loss: 0.3769 - accuracy: 0.8378 - val_loss: 0.3788 - val_accuracy: 0.8371\n",
      "Epoch 390/500\n",
      "7500/7500 [==============================] - 5s 696us/step - loss: 0.3768 - accuracy: 0.8375 - val_loss: 0.3765 - val_accuracy: 0.8381\n",
      "Epoch 391/500\n",
      "7500/7500 [==============================] - 5s 650us/step - loss: 0.3768 - accuracy: 0.8375 - val_loss: 0.3788 - val_accuracy: 0.8363\n",
      "Epoch 392/500\n",
      "7500/7500 [==============================] - 5s 687us/step - loss: 0.3767 - accuracy: 0.8382 - val_loss: 0.3773 - val_accuracy: 0.8379\n",
      "Epoch 393/500\n",
      "7500/7500 [==============================] - 6s 759us/step - loss: 0.3768 - accuracy: 0.8372 - val_loss: 0.3762 - val_accuracy: 0.8375\n",
      "Epoch 394/500\n",
      "7500/7500 [==============================] - 7s 899us/step - loss: 0.3768 - accuracy: 0.8377 - val_loss: 0.3761 - val_accuracy: 0.8380\n",
      "Epoch 395/500\n",
      "7500/7500 [==============================] - 5s 728us/step - loss: 0.3769 - accuracy: 0.8374 - val_loss: 0.3880 - val_accuracy: 0.8316\n",
      "Epoch 396/500\n",
      "7500/7500 [==============================] - 6s 770us/step - loss: 0.3769 - accuracy: 0.8376 - val_loss: 0.3789 - val_accuracy: 0.8367\n",
      "Epoch 397/500\n",
      "7500/7500 [==============================] - 6s 791us/step - loss: 0.3769 - accuracy: 0.8378 - val_loss: 0.3774 - val_accuracy: 0.8379\n",
      "Epoch 398/500\n",
      "7500/7500 [==============================] - 6s 842us/step - loss: 0.3767 - accuracy: 0.8378 - val_loss: 0.3775 - val_accuracy: 0.8371\n",
      "Epoch 399/500\n",
      "7500/7500 [==============================] - 5s 673us/step - loss: 0.3767 - accuracy: 0.8378 - val_loss: 0.3766 - val_accuracy: 0.8378\n",
      "Epoch 400/500\n",
      "7500/7500 [==============================] - 5s 729us/step - loss: 0.3768 - accuracy: 0.8378 - val_loss: 0.3772 - val_accuracy: 0.8374\n",
      "Epoch 401/500\n",
      "7500/7500 [==============================] - 5s 623us/step - loss: 0.3768 - accuracy: 0.8375 - val_loss: 0.3802 - val_accuracy: 0.8357\n",
      "Epoch 402/500\n",
      "7500/7500 [==============================] - 5s 643us/step - loss: 0.3769 - accuracy: 0.8379 - val_loss: 0.3831 - val_accuracy: 0.8365\n",
      "Epoch 403/500\n",
      "7500/7500 [==============================] - 4s 518us/step - loss: 0.3769 - accuracy: 0.8381 - val_loss: 0.3830 - val_accuracy: 0.8352\n",
      "Epoch 404/500\n",
      "7500/7500 [==============================] - 4s 527us/step - loss: 0.3768 - accuracy: 0.8382 - val_loss: 0.3761 - val_accuracy: 0.8383\n",
      "Epoch 405/500\n",
      "7500/7500 [==============================] - 4s 571us/step - loss: 0.3767 - accuracy: 0.8379 - val_loss: 0.3763 - val_accuracy: 0.8378\n",
      "Epoch 406/500\n",
      "7500/7500 [==============================] - 4s 596us/step - loss: 0.3767 - accuracy: 0.8381 - val_loss: 0.3764 - val_accuracy: 0.8381\n",
      "Epoch 407/500\n",
      "7500/7500 [==============================] - 4s 595us/step - loss: 0.3768 - accuracy: 0.8376 - val_loss: 0.3775 - val_accuracy: 0.8369\n",
      "Epoch 408/500\n",
      "7500/7500 [==============================] - 4s 586us/step - loss: 0.3768 - accuracy: 0.8379 - val_loss: 0.3822 - val_accuracy: 0.8358\n",
      "Epoch 409/500\n",
      "7500/7500 [==============================] - 4s 575us/step - loss: 0.3768 - accuracy: 0.8377 - val_loss: 0.3883 - val_accuracy: 0.8310\n",
      "Epoch 410/500\n",
      "7500/7500 [==============================] - 4s 534us/step - loss: 0.3767 - accuracy: 0.8378 - val_loss: 0.3762 - val_accuracy: 0.8381\n",
      "Epoch 411/500\n",
      "7500/7500 [==============================] - 4s 538us/step - loss: 0.3767 - accuracy: 0.8377 - val_loss: 0.3799 - val_accuracy: 0.8355\n",
      "Epoch 412/500\n",
      "7500/7500 [==============================] - 4s 538us/step - loss: 0.3768 - accuracy: 0.8378 - val_loss: 0.3809 - val_accuracy: 0.8357\n",
      "Epoch 413/500\n",
      "7500/7500 [==============================] - 4s 566us/step - loss: 0.3770 - accuracy: 0.8379 - val_loss: 0.3761 - val_accuracy: 0.8379\n",
      "Epoch 414/500\n",
      "7500/7500 [==============================] - 4s 539us/step - loss: 0.3768 - accuracy: 0.8377 - val_loss: 0.3894 - val_accuracy: 0.8308\n",
      "Epoch 415/500\n",
      "7500/7500 [==============================] - 4s 534us/step - loss: 0.3767 - accuracy: 0.8374 - val_loss: 0.3814 - val_accuracy: 0.8358\n",
      "Epoch 416/500\n",
      "7500/7500 [==============================] - 4s 532us/step - loss: 0.3767 - accuracy: 0.8376 - val_loss: 0.3763 - val_accuracy: 0.8375\n",
      "Epoch 417/500\n",
      "7500/7500 [==============================] - 4s 521us/step - loss: 0.3767 - accuracy: 0.8377 - val_loss: 0.3773 - val_accuracy: 0.8371\n",
      "Epoch 418/500\n",
      "7500/7500 [==============================] - 4s 518us/step - loss: 0.3768 - accuracy: 0.8381 - val_loss: 0.3825 - val_accuracy: 0.8338\n",
      "Epoch 419/500\n",
      "7500/7500 [==============================] - 4s 519us/step - loss: 0.3766 - accuracy: 0.8380 - val_loss: 0.3765 - val_accuracy: 0.8380\n",
      "Epoch 420/500\n",
      "7500/7500 [==============================] - 4s 554us/step - loss: 0.3768 - accuracy: 0.8377 - val_loss: 0.3786 - val_accuracy: 0.8372\n",
      "Epoch 421/500\n",
      "7500/7500 [==============================] - 4s 553us/step - loss: 0.3766 - accuracy: 0.8379 - val_loss: 0.3790 - val_accuracy: 0.8361\n",
      "Epoch 422/500\n",
      "7500/7500 [==============================] - 4s 577us/step - loss: 0.3766 - accuracy: 0.8377 - val_loss: 0.3821 - val_accuracy: 0.8342\n",
      "Epoch 423/500\n",
      "7500/7500 [==============================] - 4s 557us/step - loss: 0.3767 - accuracy: 0.8380 - val_loss: 0.3760 - val_accuracy: 0.8375\n",
      "Epoch 424/500\n",
      "7500/7500 [==============================] - 4s 557us/step - loss: 0.3767 - accuracy: 0.8379 - val_loss: 0.3769 - val_accuracy: 0.8374\n",
      "Epoch 425/500\n",
      "7500/7500 [==============================] - 4s 532us/step - loss: 0.3766 - accuracy: 0.8381 - val_loss: 0.3785 - val_accuracy: 0.8364\n",
      "Epoch 426/500\n",
      "7500/7500 [==============================] - 4s 525us/step - loss: 0.3767 - accuracy: 0.8376 - val_loss: 0.3768 - val_accuracy: 0.8372\n",
      "Epoch 427/500\n",
      "7500/7500 [==============================] - 4s 528us/step - loss: 0.3768 - accuracy: 0.8377 - val_loss: 0.3774 - val_accuracy: 0.8376\n",
      "Epoch 428/500\n",
      "7500/7500 [==============================] - 4s 513us/step - loss: 0.3767 - accuracy: 0.8376 - val_loss: 0.3770 - val_accuracy: 0.8383\n",
      "Epoch 429/500\n",
      "7500/7500 [==============================] - 4s 537us/step - loss: 0.3766 - accuracy: 0.8379 - val_loss: 0.3772 - val_accuracy: 0.8385\n",
      "Epoch 430/500\n",
      "7500/7500 [==============================] - 4s 553us/step - loss: 0.3767 - accuracy: 0.8378 - val_loss: 0.3780 - val_accuracy: 0.8374\n",
      "Epoch 431/500\n",
      "7500/7500 [==============================] - 4s 514us/step - loss: 0.3765 - accuracy: 0.8379 - val_loss: 0.3901 - val_accuracy: 0.8316\n",
      "Epoch 432/500\n",
      "7500/7500 [==============================] - 4s 515us/step - loss: 0.3768 - accuracy: 0.8380 - val_loss: 0.3777 - val_accuracy: 0.8368\n",
      "Epoch 433/500\n",
      "7500/7500 [==============================] - 4s 551us/step - loss: 0.3766 - accuracy: 0.8377 - val_loss: 0.3759 - val_accuracy: 0.8378\n",
      "Epoch 434/500\n",
      "7500/7500 [==============================] - 4s 533us/step - loss: 0.3767 - accuracy: 0.8380 - val_loss: 0.3788 - val_accuracy: 0.8371\n",
      "Epoch 435/500\n",
      "7500/7500 [==============================] - 4s 561us/step - loss: 0.3766 - accuracy: 0.8379 - val_loss: 0.3770 - val_accuracy: 0.8375\n",
      "Epoch 436/500\n",
      "7500/7500 [==============================] - 4s 546us/step - loss: 0.3767 - accuracy: 0.8379 - val_loss: 0.3772 - val_accuracy: 0.8372\n",
      "Epoch 437/500\n",
      "7500/7500 [==============================] - 4s 533us/step - loss: 0.3767 - accuracy: 0.8372 - val_loss: 0.3781 - val_accuracy: 0.8373\n",
      "Epoch 438/500\n",
      "7500/7500 [==============================] - 4s 510us/step - loss: 0.3767 - accuracy: 0.8377 - val_loss: 0.3789 - val_accuracy: 0.8366\n",
      "Epoch 439/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 4s 513us/step - loss: 0.3767 - accuracy: 0.8379 - val_loss: 0.3795 - val_accuracy: 0.8360\n",
      "Epoch 440/500\n",
      "7500/7500 [==============================] - 4s 525us/step - loss: 0.3768 - accuracy: 0.8380 - val_loss: 0.3775 - val_accuracy: 0.8370\n",
      "Epoch 441/500\n",
      "7500/7500 [==============================] - 4s 511us/step - loss: 0.3767 - accuracy: 0.8377 - val_loss: 0.3786 - val_accuracy: 0.8364\n",
      "Epoch 442/500\n",
      "7500/7500 [==============================] - 4s 512us/step - loss: 0.3765 - accuracy: 0.8377 - val_loss: 0.3780 - val_accuracy: 0.8375\n",
      "Epoch 443/500\n",
      "7500/7500 [==============================] - 4s 510us/step - loss: 0.3765 - accuracy: 0.8375 - val_loss: 0.3775 - val_accuracy: 0.8373\n",
      "Epoch 444/500\n",
      "7500/7500 [==============================] - 4s 512us/step - loss: 0.3765 - accuracy: 0.8383 - val_loss: 0.3761 - val_accuracy: 0.8377\n",
      "Epoch 445/500\n",
      "7500/7500 [==============================] - 4s 509us/step - loss: 0.3767 - accuracy: 0.8377 - val_loss: 0.3822 - val_accuracy: 0.8349\n",
      "Epoch 446/500\n",
      "7500/7500 [==============================] - 4s 547us/step - loss: 0.3767 - accuracy: 0.8376 - val_loss: 0.3780 - val_accuracy: 0.8379\n",
      "Epoch 447/500\n",
      "7500/7500 [==============================] - 4s 534us/step - loss: 0.3766 - accuracy: 0.8376 - val_loss: 0.3846 - val_accuracy: 0.8332\n",
      "Epoch 448/500\n",
      "7500/7500 [==============================] - 4s 508us/step - loss: 0.3765 - accuracy: 0.8383 - val_loss: 0.3768 - val_accuracy: 0.8372\n",
      "Epoch 449/500\n",
      "7500/7500 [==============================] - 4s 532us/step - loss: 0.3767 - accuracy: 0.8376 - val_loss: 0.3839 - val_accuracy: 0.8350\n",
      "Epoch 450/500\n",
      "7500/7500 [==============================] - 5s 608us/step - loss: 0.3766 - accuracy: 0.8382 - val_loss: 0.3766 - val_accuracy: 0.8382\n",
      "Epoch 451/500\n",
      "7500/7500 [==============================] - 4s 582us/step - loss: 0.3765 - accuracy: 0.8378 - val_loss: 0.3854 - val_accuracy: 0.8344\n",
      "Epoch 452/500\n",
      "7500/7500 [==============================] - 4s 518us/step - loss: 0.3765 - accuracy: 0.8377 - val_loss: 0.3812 - val_accuracy: 0.8363\n",
      "Epoch 453/500\n",
      "7500/7500 [==============================] - 4s 523us/step - loss: 0.3764 - accuracy: 0.8380 - val_loss: 0.3767 - val_accuracy: 0.8377\n",
      "Epoch 454/500\n",
      "7500/7500 [==============================] - 4s 533us/step - loss: 0.3764 - accuracy: 0.8380 - val_loss: 0.3812 - val_accuracy: 0.8357\n",
      "Epoch 455/500\n",
      "7500/7500 [==============================] - 4s 516us/step - loss: 0.3766 - accuracy: 0.8377 - val_loss: 0.3768 - val_accuracy: 0.8379\n",
      "Epoch 456/500\n",
      "7500/7500 [==============================] - 4s 515us/step - loss: 0.3767 - accuracy: 0.8379 - val_loss: 0.3850 - val_accuracy: 0.8337\n",
      "Epoch 457/500\n",
      "7500/7500 [==============================] - 4s 518us/step - loss: 0.3766 - accuracy: 0.8377 - val_loss: 0.3768 - val_accuracy: 0.8375\n",
      "Epoch 458/500\n",
      "7500/7500 [==============================] - 4s 515us/step - loss: 0.3765 - accuracy: 0.8380 - val_loss: 0.3852 - val_accuracy: 0.8333\n",
      "Epoch 459/500\n",
      "7500/7500 [==============================] - 4s 515us/step - loss: 0.3766 - accuracy: 0.8377 - val_loss: 0.3763 - val_accuracy: 0.8373\n",
      "Epoch 460/500\n",
      "7500/7500 [==============================] - 4s 515us/step - loss: 0.3766 - accuracy: 0.8375 - val_loss: 0.3787 - val_accuracy: 0.8373\n",
      "Epoch 461/500\n",
      "7500/7500 [==============================] - 4s 514us/step - loss: 0.3766 - accuracy: 0.8376 - val_loss: 0.3762 - val_accuracy: 0.8380\n",
      "Epoch 462/500\n",
      "7500/7500 [==============================] - 4s 516us/step - loss: 0.3766 - accuracy: 0.8379 - val_loss: 0.3783 - val_accuracy: 0.8370\n",
      "Epoch 463/500\n",
      "7500/7500 [==============================] - 4s 566us/step - loss: 0.3765 - accuracy: 0.8377 - val_loss: 0.3805 - val_accuracy: 0.8359\n",
      "Epoch 464/500\n",
      "7500/7500 [==============================] - 4s 536us/step - loss: 0.3766 - accuracy: 0.8375 - val_loss: 0.3757 - val_accuracy: 0.8383\n",
      "Epoch 465/500\n",
      "7500/7500 [==============================] - 5s 637us/step - loss: 0.3762 - accuracy: 0.8379 - val_loss: 0.3767 - val_accuracy: 0.8371\n",
      "Epoch 466/500\n",
      "7500/7500 [==============================] - 4s 539us/step - loss: 0.3764 - accuracy: 0.8382 - val_loss: 0.3774 - val_accuracy: 0.8373\n",
      "Epoch 467/500\n",
      "7500/7500 [==============================] - 4s 514us/step - loss: 0.3765 - accuracy: 0.8380 - val_loss: 0.3851 - val_accuracy: 0.8344\n",
      "Epoch 468/500\n",
      "7500/7500 [==============================] - 4s 517us/step - loss: 0.3764 - accuracy: 0.8384 - val_loss: 0.3772 - val_accuracy: 0.8377\n",
      "Epoch 469/500\n",
      "7500/7500 [==============================] - 4s 527us/step - loss: 0.3764 - accuracy: 0.8375 - val_loss: 0.3757 - val_accuracy: 0.8379\n",
      "Epoch 470/500\n",
      "7500/7500 [==============================] - 4s 584us/step - loss: 0.3765 - accuracy: 0.8379 - val_loss: 0.3773 - val_accuracy: 0.8381\n",
      "Epoch 471/500\n",
      "7500/7500 [==============================] - 4s 579us/step - loss: 0.3763 - accuracy: 0.8385 - val_loss: 0.3761 - val_accuracy: 0.8376\n",
      "Epoch 472/500\n",
      "7500/7500 [==============================] - 4s 561us/step - loss: 0.3764 - accuracy: 0.8377 - val_loss: 0.3888 - val_accuracy: 0.8321\n",
      "Epoch 473/500\n",
      "7500/7500 [==============================] - 4s 546us/step - loss: 0.3766 - accuracy: 0.8379 - val_loss: 0.3782 - val_accuracy: 0.8372\n",
      "Epoch 474/500\n",
      "7500/7500 [==============================] - 4s 562us/step - loss: 0.3764 - accuracy: 0.8381 - val_loss: 0.3764 - val_accuracy: 0.8370\n",
      "Epoch 475/500\n",
      "7500/7500 [==============================] - 4s 542us/step - loss: 0.3764 - accuracy: 0.8379 - val_loss: 0.3763 - val_accuracy: 0.8378\n",
      "Epoch 476/500\n",
      "7500/7500 [==============================] - 4s 553us/step - loss: 0.3763 - accuracy: 0.8381 - val_loss: 0.3771 - val_accuracy: 0.8375\n",
      "Epoch 477/500\n",
      "7500/7500 [==============================] - 4s 595us/step - loss: 0.3765 - accuracy: 0.8380 - val_loss: 0.3763 - val_accuracy: 0.8380\n",
      "Epoch 478/500\n",
      "7500/7500 [==============================] - 4s 528us/step - loss: 0.3767 - accuracy: 0.8375 - val_loss: 0.3764 - val_accuracy: 0.8372\n",
      "Epoch 479/500\n",
      "7500/7500 [==============================] - 4s 563us/step - loss: 0.3762 - accuracy: 0.8377 - val_loss: 0.3797 - val_accuracy: 0.8365\n",
      "Epoch 480/500\n",
      "7500/7500 [==============================] - 4s 523us/step - loss: 0.3765 - accuracy: 0.8379 - val_loss: 0.3777 - val_accuracy: 0.8368\n",
      "Epoch 481/500\n",
      "7500/7500 [==============================] - 4s 519us/step - loss: 0.3765 - accuracy: 0.8377 - val_loss: 0.3800 - val_accuracy: 0.8364\n",
      "Epoch 482/500\n",
      "7500/7500 [==============================] - 4s 516us/step - loss: 0.3764 - accuracy: 0.8381 - val_loss: 0.3770 - val_accuracy: 0.8374\n",
      "Epoch 483/500\n",
      "7500/7500 [==============================] - 4s 512us/step - loss: 0.3765 - accuracy: 0.8378 - val_loss: 0.3789 - val_accuracy: 0.8363\n",
      "Epoch 484/500\n",
      "7500/7500 [==============================] - 4s 519us/step - loss: 0.3763 - accuracy: 0.8378 - val_loss: 0.3803 - val_accuracy: 0.8370\n",
      "Epoch 485/500\n",
      "7500/7500 [==============================] - 4s 515us/step - loss: 0.3765 - accuracy: 0.8379 - val_loss: 0.3761 - val_accuracy: 0.8380\n",
      "Epoch 486/500\n",
      "7500/7500 [==============================] - 4s 512us/step - loss: 0.3765 - accuracy: 0.8381 - val_loss: 0.3839 - val_accuracy: 0.8349\n",
      "Epoch 487/500\n",
      "7500/7500 [==============================] - 4s 514us/step - loss: 0.3765 - accuracy: 0.8379 - val_loss: 0.3897 - val_accuracy: 0.8314\n",
      "Epoch 488/500\n",
      "7500/7500 [==============================] - 4s 512us/step - loss: 0.3764 - accuracy: 0.8377 - val_loss: 0.3785 - val_accuracy: 0.8364\n",
      "Epoch 489/500\n",
      "7500/7500 [==============================] - 4s 515us/step - loss: 0.3766 - accuracy: 0.8376 - val_loss: 0.3914 - val_accuracy: 0.8321\n",
      "Epoch 490/500\n",
      "7500/7500 [==============================] - 4s 513us/step - loss: 0.3766 - accuracy: 0.8378 - val_loss: 0.3836 - val_accuracy: 0.8342\n",
      "Epoch 491/500\n",
      "7500/7500 [==============================] - 4s 519us/step - loss: 0.3762 - accuracy: 0.8382 - val_loss: 0.3766 - val_accuracy: 0.8371\n",
      "Epoch 492/500\n",
      "7500/7500 [==============================] - 4s 521us/step - loss: 0.3763 - accuracy: 0.8378 - val_loss: 0.3915 - val_accuracy: 0.8319\n",
      "Epoch 493/500\n",
      "7500/7500 [==============================] - 4s 515us/step - loss: 0.3764 - accuracy: 0.8380 - val_loss: 0.3772 - val_accuracy: 0.8378\n",
      "Epoch 494/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 4s 592us/step - loss: 0.3764 - accuracy: 0.8380 - val_loss: 0.3798 - val_accuracy: 0.8364\n",
      "Epoch 495/500\n",
      "7500/7500 [==============================] - 4s 568us/step - loss: 0.3766 - accuracy: 0.8376 - val_loss: 0.3762 - val_accuracy: 0.8379\n",
      "Epoch 496/500\n",
      "7500/7500 [==============================] - 4s 569us/step - loss: 0.3763 - accuracy: 0.8376 - val_loss: 0.3813 - val_accuracy: 0.8352\n",
      "Epoch 497/500\n",
      "7500/7500 [==============================] - 4s 531us/step - loss: 0.3762 - accuracy: 0.8383 - val_loss: 0.3843 - val_accuracy: 0.8347\n",
      "Epoch 498/500\n",
      "7500/7500 [==============================] - 4s 515us/step - loss: 0.3766 - accuracy: 0.8380 - val_loss: 0.3809 - val_accuracy: 0.8359\n",
      "Epoch 499/500\n",
      "7500/7500 [==============================] - 4s 511us/step - loss: 0.3764 - accuracy: 0.8382 - val_loss: 0.3825 - val_accuracy: 0.8346\n",
      "Epoch 500/500\n",
      "7500/7500 [==============================] - 4s 510us/step - loss: 0.3764 - accuracy: 0.8381 - val_loss: 0.3769 - val_accuracy: 0.8374\n"
     ]
    }
   ],
   "source": [
    "model_1.compile(SGD(lr = .003), \"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "run_hist_1 = model_1.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is 0.837\n",
      "roc-auc is 0.871\n"
     ]
    }
   ],
   "source": [
    "y_pred_class_nn_1 = model_1.predict_classes(X_test)\n",
    "y_pred_prob_nn_1 = model_1.predict(X_test)\n",
    "\n",
    "print('accuracy is {:.3f}'.format(accuracy_score(y_test,y_pred_class_nn_1)))\n",
    "print('roc-auc is {:.3f}'.format(roc_auc_score(y_test,y_pred_prob_nn_1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_class_nn_1 = model_1.predict_classes(X)\n",
    "y_pred_prob_nn_1 = model_1.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.05876464, 0.55560994, 0.06158394, ..., 0.70166874, 0.11884412,\n",
       "       0.34610277], dtype=float32)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_prob_nn_1[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.058765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.555610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.061584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.090531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.084319</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     target\n",
       "0  0.058765\n",
       "1  0.555610\n",
       "2  0.061584\n",
       "3  0.090531\n",
       "4  0.084319"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final=pd.DataFrame({'target': y_pred_prob_nn_1[:,0]})\n",
    "final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>0.058765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>0.555610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>0.061584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>0.090531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>0.084319</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id    target\n",
       "0   5  0.058765\n",
       "1   6  0.555610\n",
       "2   8  0.061584\n",
       "3   9  0.090531\n",
       "4  11  0.084319"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final1 = pd.concat([df1['id'],final],axis=1)\n",
    "final1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "final1.to_csv('sample_submission.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
